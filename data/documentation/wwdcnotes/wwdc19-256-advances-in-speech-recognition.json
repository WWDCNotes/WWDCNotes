{"sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-256-Advances-in-Speech-Recognition"},"primaryContentSections":[{"content":[{"text":"Overview","type":"heading","anchor":"overview","level":2},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"MacOS support","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"over 50 languages supported","type":"text"}]}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-256-image","type":"image"}]},{"anchor":"Jitter-shimmer-analysis","level":2,"text":"Jitter, shimmer analysis","type":"heading"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Jitter is a measure of frequency instability, while shimmer is a measure of amplitude instability."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Basically can measure if the person is chilling or panicking."}]},{"anchor":"Pitch-voicing-analysis","level":2,"text":"Pitch, voicing analysis","type":"heading"},{"type":"paragraph","inlineContent":[{"text":"Pitch measures the frequency characteristics, voicing determine voiced regions (their wideness).","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Basically can measure if the person is active or tired, length of pauses."}]},{"anchor":"Written-By","level":2,"text":"Written By","type":"heading"},{"columns":[{"size":1,"content":[{"inlineContent":[{"type":"image","identifier":"zntfdr"}],"type":"paragraph"}]},{"size":4,"content":[{"text":"Federico Zanetello","anchor":"Federico-Zanetello","level":3,"type":"heading"},{"inlineContent":[{"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"type":"reference","isActive":true,"overridingTitle":"Contributed Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","isActive":true,"identifier":"https:\/\/github.com\/zntfdr"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","isActive":true,"identifier":"https:\/\/zntfdr.dev"}],"type":"paragraph"}]}],"numberOfColumns":5,"type":"row"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"reference","isActive":true,"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing"}]},{"anchor":"Related-Sessions","level":2,"text":"Related Sessions","type":"heading"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10101-Customize-ondevice-speech-recognition","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC16-509-Speech-Recognition-API"]},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}]},{"type":"small","inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}],"kind":"content"}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc19-256-advances-in-speech-recognition"],"traits":[{"interfaceLanguage":"swift"}]}],"schemaVersion":{"major":0,"patch":0,"minor":3},"metadata":{"roleHeading":"WWDC19","modules":[{"name":"WWDC Notes"}],"role":"sampleCode","title":"Advances in Speech Recognition"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"abstract":[{"text":"Speech Recognizer can now be used locally on iOS or macOS devices with no network connection. Learn how you can bring text-to-speech support to your app while maintaining privacy and eliminating the limitations of server-based processing. Speech recognition API has also been enhanced to provide richer analytics including speaking rate, pause duration, and voice quality.","type":"text"}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc19\/256","isActive":true,"overridingTitle":"Watch Video (6 min)"}},"kind":"article","references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC16-509-Speech-Recognition-API":{"title":"Speech Recognition API","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC16-509-Speech-Recognition-API","url":"\/documentation\/wwdcnotes\/wwdc16-509-speech-recognition-api","abstract":[{"text":"iOS 10 brings a brand new Speech Recognition API that allows you to perform rapid and contextually informed speech recognition in both file-based and realtime scenarios. In this video, you will learn all about the new API and how to bring advanced speech recognition services into your apps.","type":"text"}],"role":"sampleCode","type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-209-Whats-New-in-Machine-Learning":{"title":"What’s New in Machine Learning","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","url":"\/documentation\/wwdcnotes\/wwdc19-209-whats-new-in-machine-learning","abstract":[{"text":"Core ML 3 has been greatly expanded to enable even more amazing, on-device machine learning capabilities in your app. Learn about the new Create ML app which makes it easy to build Core ML models for many tasks. Get an overview of model personalization; exciting updates in Vision, Natural Language, Sound, and Speech; and added support for cutting-edge model types.","type":"text"}],"role":"sampleCode","type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"abstract":[{"text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Core Haptics","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","role":"collectionGroup","kind":"article","images":[{"type":"icon","identifier":"WWDC19-Icon.png"},{"type":"card","identifier":"WWDC19.jpeg"}],"title":"WWDC19","url":"\/documentation\/wwdcnotes\/wwdc19"},"WWDC19-Icon.png":{"alt":null,"identifier":"WWDC19-Icon.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-Icon.png"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"url":"\/documentation\/wwdcnotes\/zntfdr","type":"topic","kind":"article","title":"Federico Zanetello (332 notes)","abstract":[{"text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more.","type":"text"}],"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","images":[{"identifier":"zntfdr.jpeg","type":"card"},{"identifier":"zntfdr.jpeg","type":"icon"}]},"zntfdr":{"alt":"Profile image of Federico Zanetello","identifier":"zntfdr","variants":[{"traits":["1x","light"],"url":"\/images\/zntfdr.jpeg"}],"type":"image"},"https://github.com/zntfdr":{"title":"GitHub","identifier":"https:\/\/github.com\/zntfdr","url":"https:\/\/github.com\/zntfdr","type":"link","titleInlineContent":[{"text":"GitHub","type":"text"}]},"https://zntfdr.dev":{"title":"Blog","identifier":"https:\/\/zntfdr.dev","url":"https:\/\/zntfdr.dev","type":"link","titleInlineContent":[{"text":"Blog","type":"text"}]},"WWDCNotes.png":{"alt":null,"identifier":"WWDCNotes.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"type":"image"},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"title":"Contributions are welcome!","identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"link","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-704-Core-ML-3-Framework":{"title":"Core ML 3 Framework","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework","url":"\/documentation\/wwdcnotes\/wwdc19-704-core-ml-3-framework","abstract":[{"text":"Core ML 3 now enables support for advanced model types that were never before available in on-device machine learning. Learn how model personalization brings amazing personalization opportunities to your app. Gain a deeper understanding of strategies for linking models and improvements to Core ML tools used for conversion of existing models.","type":"text"}],"role":"sampleCode","type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes":{"title":"WWDC Notes","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"role":"collection","type":"topic","kind":"symbol","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","url":"\/documentation\/wwdcnotes"},"zntfdr.jpeg":{"alt":null,"identifier":"zntfdr.jpeg","variants":[{"traits":["1x","light"],"url":"\/images\/zntfdr.jpeg"}],"type":"image"},"WWDC19-256-image":{"alt":null,"identifier":"WWDC19-256-image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-256-image.png"}],"type":"image"},"https://developer.apple.com/wwdc19/256":{"identifier":"https:\/\/developer.apple.com\/wwdc19\/256","url":"https:\/\/developer.apple.com\/wwdc19\/256","checksum":null,"type":"download"},"WWDC19.jpeg":{"alt":null,"identifier":"WWDC19.jpeg","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19.jpeg"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10101-Customize-ondevice-speech-recognition":{"title":"Customize on-device speech recognition","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10101-Customize-ondevice-speech-recognition","url":"\/documentation\/wwdcnotes\/wwdc23-10101-customize-ondevice-speech-recognition","abstract":[{"text":"Find out how you can improve on-device speech recognition in your app by customizing the underlying model with additional vocabulary. We’ll share how speech recognition works on device and show you how to boost specific words and phrases for more predictable transcription. Learn how you can provide specific pronunciations for words and use template support to quickly generate a full set of custom phrases — all at runtime.","type":"text"}],"role":"sampleCode","type":"topic","kind":"article"}}}