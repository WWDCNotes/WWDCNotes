{"sections":[],"metadata":{"modules":[{"name":"WWDC Notes"}],"role":"sampleCode","title":"Introducing the Create ML App","roleHeading":"WWDC19"},"schemaVersion":{"minor":3,"patch":0,"major":0},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","interfaceLanguage":"swift"},"primaryContentSections":[{"content":[{"anchor":"Create-ML","type":"heading","level":2,"text":"Create ML"},{"inlineContent":[{"type":"image","identifier":"WWDC19-430-appIcon"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The Create ML, Apple’s app to create new Machine Learning models, has a new workflow and two new input types."}],"type":"paragraph"},{"inlineContent":[{"type":"emphasis","inlineContent":[{"type":"text","text":"“It provides a really approachable way to build custom machine learning models to add to your applications.”"}]}],"type":"paragraph"},{"name":"To open Create ML","style":"note","type":"aside","content":[{"type":"paragraph","inlineContent":[{"text":"launch Xcode 11, then go to ","type":"text"},{"type":"codeVoice","code":"Xcode > Open Developer Tool > Create ML"},{"text":". Alternatively, use spotlight and search Create ML.","type":"text"}]}]},{"anchor":"Model-Types","type":"heading","level":2,"text":"Model Types"},{"inlineContent":[{"type":"image","identifier":"WWDC19-430-models"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Activity and Sound are this year new models!"}],"type":"paragraph"},{"inlineContent":[{"text":"Each CoreML type offers different solutions.","type":"text"}],"type":"paragraph"},{"anchor":"Image-Model","type":"heading","level":2,"text":"Image Model"},{"anchor":"Image-classifier","type":"heading","level":3,"text":"Image classifier"},{"inlineContent":[{"text":"An Image Classifier can be used for categorizing images based on their contents. For example, the Art Style identifier uses a custom Image Classifier to determine the most likely movement of a piece.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC19-430-classify","type":"image"},{"text":" ","type":"text"},{"text":"￼","type":"text"}],"type":"paragraph"},{"anchor":"Object-Detector","type":"heading","level":3,"text":"Object Detector"},{"inlineContent":[{"type":"text","text":"Lets you identify multiple objects within an image."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC19-430-detect"},{"type":"text","text":" "},{"type":"text","text":"￼"}],"type":"paragraph"},{"anchor":"Sound-Model","type":"heading","level":2,"text":"Sound Model"},{"anchor":"Sound-Classifier","type":"heading","level":3,"text":"Sound Classifier"},{"inlineContent":[{"type":"text","text":"Determines the most dominant sound within an audio stream."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC19-430-sound"},{"type":"text","text":" "},{"type":"text","text":"￼"}],"type":"paragraph"},{"anchor":"Activity-Model","type":"heading","level":2,"text":"Activity Model"},{"anchor":"Activity-classifier","type":"heading","level":3,"text":"Activity classifier"},{"inlineContent":[{"text":"Puts together data from the accelerometer and gyroscope to guess the type of activity the user is doing.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC19-430-activity"},{"type":"text","text":" "},{"type":"text","text":"￼"}],"type":"paragraph"},{"anchor":"Text-Model","type":"heading","level":2,"text":"Text Model"},{"anchor":"Text-classifier","type":"heading","level":3,"text":"Text classifier"},{"inlineContent":[{"text":"Text classification can be used to label sentences, paragraphs, or even entire articles based on their contents.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We can train these for custom topic identification or categorization tasks"}],"type":"paragraph"},{"anchor":"Word-tagger","type":"heading","level":3,"text":"Word tagger"},{"inlineContent":[{"type":"text","text":"Ideal for labeling tokens or words of interest in text. General purpose examples of this are things like tagging different parts of speech or recognizing named entities."}],"type":"paragraph"},{"anchor":"Tabular-Model","type":"heading","level":2,"text":"Tabular Model"},{"inlineContent":[{"type":"text","text":"This is a generic model."}],"type":"paragraph"},{"anchor":"Tabular-Classifier","type":"heading","level":3,"text":"Tabular Classifier"},{"inlineContent":[{"text":"Classifiers are for categorizing samples based on their features of interest. And features can be a variety of different types such as integers, doubles, strings, so long as your target is a discrete value.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"What’s unique about the Tabular Classifier is it extracts away the underlying algorithm for you. And identifies the best multiple classifiers for your data.","type":"text"}],"type":"paragraph"},{"anchor":"Tabular-Regressor","type":"heading","level":3,"text":"Tabular Regressor"},{"inlineContent":[{"text":"A model that will predict a numeric value, such as a rating or a score.","type":"text"}],"type":"paragraph"},{"anchor":"Recommender","type":"heading","level":3,"text":"Recommender"},{"inlineContent":[{"text":"Allows us to recommend content based on user behavior.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The Recommender can be trained on user-item interactions with or without ratings."}],"type":"paragraph"},{"anchor":"Create-ML-Workflow","type":"heading","level":2,"text":"Create ML Workflow"},{"inlineContent":[{"text":"Five steps:","type":"text"}],"type":"paragraph"},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"choose your domain\/model (aka your input type among the five mentioned above)."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"based on the domain, the app will offer you different model types.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Input","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Training"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Output"}]}]}]},{"inlineContent":[{"type":"text","text":"Create ML will always display analytics during your progress (with user-friendly charts) to make you easily understand the accuracy of your model."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"In Apple words: "},{"type":"emphasis","inlineContent":[{"type":"text","text":"“And with additions like metrics visualization, live progress, and interactive preview, Create ML app sets the bar for a great model training experience.”"}]}],"type":"paragraph"},{"inlineContent":[{"text":"The testing can be done by simply dragging and dropping new data (of all types, based on what model we’re training .","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"The session goes on and shows how to train a flower recognizer, it skips some steps but it overall looks very promising and simple to implement. Most importantly, the ","type":"text"},{"type":"codeVoice","code":".mlmodel"},{"text":" produced by Create ML is incredibly small (way less than 100KB from a collection of several images).","type":"text"}],"type":"paragraph"},{"name":"It goes without saying, but all is mentioned here run on devices in their Neural Engine","style":"note","type":"aside","content":[{"inlineContent":[{"text":"no cpu and gpu wasted on these (only if you have a A12 or S4 device, earlier devices will run CoreML in the GPU instead).","type":"text"}],"type":"paragraph"}]},{"anchor":"Written-By","type":"heading","level":2,"text":"Written By"},{"numberOfColumns":5,"columns":[{"content":[{"type":"paragraph","inlineContent":[{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/5277837?v=4","type":"image"}]}],"size":1},{"content":[{"text":"Federico Zanetello","level":3,"anchor":"Federico-Zanetello","type":"heading"},{"type":"paragraph","inlineContent":[{"type":"reference","overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"overridingTitle":"Contributed Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"type":"reference","identifier":"https:\/\/github.com\/zntfdr","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"type":"reference","identifier":"https:\/\/zntfdr.dev","isActive":true}]}],"size":4}],"type":"row"},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","isActive":true,"type":"reference"}],"type":"paragraph"},{"anchor":"Related-Sessions","type":"heading","level":2,"text":"Related Sessions"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10156-Control-training-in-Create-ML-with-Swift","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10642-Build-Image-and-Video-Style-Transfer-models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-424-Training-Object-Detection-Models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-426-Building-Activity-Classification-Models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-427-Training-Recommendation-Models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-428-Training-Text-Classifiers-in-Create-ML"]},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}],"type":"small"},{"inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}],"kind":"content"}],"seeAlsoSections":[{"identifiers":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-718-Introducing-Accelerate-for-Swift","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-722-Introducing-Combine","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-520-Introducing-Core-Haptics","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-249-Introducing-MultiCamera-Capture-for-iOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-260-Introducing-Photo-Segmentation-Mattes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-245-Introducing-the-Indoor-Maps-Program"],"title":"New Tools & Frameworks","generated":true}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc19-430-introducing-the-create-ml-app"]}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"identifier":"https:\/\/developer.apple.com\/wwdc19\/430","overridingTitle":"Watch Video (14 min)","type":"reference","isActive":true}},"abstract":[{"type":"text","text":"Bringing the power of Core ML to your app begins with one challenge. How do you create your model? The new Create ML app provides an intuitive workflow for model creation. See how to train, evaluate, test, and preview your models quickly in this easy-to-use tool. Get started with one of the many available templates handling a number of powerful machine learning tasks. Learn more about the many features for continuous model improvement and experimentation."}],"kind":"article","references":{"WWDC19-430-appIcon":{"identifier":"WWDC19-430-appIcon","alt":null,"type":"image","variants":[{"url":"\/images\/WWDC19-430-appIcon.jpg","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-428-Training-Text-Classifiers-in-Create-ML":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-428-Training-Text-Classifiers-in-Create-ML","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-428-training-text-classifiers-in-create-ml","abstract":[{"text":"Create ML now enables you to create models for Natural Language that are built on state-of-the-art techniques. Learn how these models can be easily trained and tested with the Create ML app. Gain insight into the powerful new options for transfer learning, word embeddings, and text catalogs.","type":"text"}],"kind":"article","role":"sampleCode","title":"Training Text Classifiers in Create ML"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-427-Training-Recommendation-Models-in-Create-ML":{"type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19-427-training-recommendation-models-in-create-ml","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-427-Training-Recommendation-Models-in-Create-ML","abstract":[{"text":"Recommendation models for Core ML can enable a very personal experience for the customers using your app. They power suggestions for what music to play or what movie to see in the apps you use every day. Learn how you can easily create a custom Recommendation model from all sorts of data sources using the Create ML app. Gain a deeper understanding of how this kind of personalization is possible while maintaining user privacy. See an example of one of these recommenders in action.","type":"text"}],"role":"sampleCode","title":"Training Recommendation Models in Create ML"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-520-Introducing-Core-Haptics":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-520-Introducing-Core-Haptics","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-520-introducing-core-haptics","abstract":[{"text":"Core Haptics lets you design fully customized haptic patterns with synchronized audio. See examples of how haptics and audio enables you to create a greater sense of immersion in your app or game. Learn how to create, play back, and share content, and where Core Haptics fits in with other audio and vibration APIs.","type":"text"}],"kind":"article","role":"sampleCode","title":"Introducing Core Haptics"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"type":"image","identifier":"WWDCNotes.png","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-260-Introducing-Photo-Segmentation-Mattes":{"type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19-260-introducing-photo-segmentation-mattes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-260-Introducing-Photo-Segmentation-Mattes","abstract":[{"text":"Photos captured in Portrait Mode on iOS 12 contain an embedded person segmentation matte that made it easy to create creative visual effects like background replacement. iOS 13 leverages on-device machine learning to provide new segmentation mattes for any captured photo. Learn about the new semantic segmentation mattes available to you from both AVCapture and Core Image to isolate a person’s hair, skin, and teeth. Using any of these individual mattes or combining all of them, your app can now offer a tremendous amount of photo editing control.","type":"text"}],"role":"sampleCode","title":"Introducing Photo Segmentation Mattes"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-718-Introducing-Accelerate-for-Swift":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-718-introducing-accelerate-for-swift","abstract":[{"text":"Accelerate framework provides hundreds of computational functions that are highly optimized to the system architecture your device is running on. Learn how to access all of these powerful functions directly in Swift. Understand how the power of vector programming can deliver incredible performance to your iOS, macOS, tvOS, and watchOS apps.","type":"text"}],"title":"Introducing Accelerate for Swift","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-718-Introducing-Accelerate-for-Swift"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-426-Building-Activity-Classification-Models-in-Create-ML":{"type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19-426-building-activity-classification-models-in-create-ml","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-426-Building-Activity-Classification-Models-in-Create-ML","abstract":[{"text":"Your iPhone and Apple Watch are loaded with a number of powerful sensors including an accelerometer and gyroscope. Activity Classifiers can be trained on data from these sensors to bring some magic to your app, such as knowing when someone is running or swinging a bat. Learn how the Create ML app makes it easy to train and evaluate one of these Core ML models. Gain a deeper understanding of how to collect the raw data needed for training. See the use of these models in action.","type":"text"}],"role":"sampleCode","title":"Building Activity Classification Models in Create ML"},"WWDC19-430-detect":{"alt":null,"type":"image","variants":[{"url":"\/images\/WWDC19-430-detect.png","traits":["1x","light"]}],"identifier":"WWDC19-430-detect"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-424-Training-Object-Detection-Models-in-Create-ML":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-424-training-object-detection-models-in-create-ml","abstract":[{"type":"text","text":"Custom Core ML models for Object Detection offer you an opportunity to add some real magic to your app. Learn how the Create ML app in Xcode makes it easy to train and evaluate these models. See how you can test the model performance directly within the app by taking advantage of Continuity Camera. It’s never been easier to build and deploy great Object Detection models for Core ML."}],"title":"Training Object Detection Models in Create ML","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-424-Training-Object-Detection-Models-in-Create-ML"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10642-Build-Image-and-Video-Style-Transfer-models-in-Create-ML":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10642-Build-Image-and-Video-Style-Transfer-models-in-Create-ML","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc20-10642-build-image-and-video-style-transfer-models-in-create-ml","abstract":[{"type":"text","text":"Bring stylized effects to your photos and videos with Style Transfer in Create ML. Discover how you can train models in minutes that make it easy to bring creative visual features to your app. Learn about the training process and the options you have for controlling the results. And we’ll explore the real-time performance of these models by demonstrating three of them simultaneously in ARKit."}],"kind":"article","role":"sampleCode","title":"Build Image and Video Style Transfer models in Create ML"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"abstract":[{"type":"text","text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more."}],"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/zntfdr","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","title":"Federico Zanetello (214 notes)","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19","abstract":[{"type":"text","text":"Xcode 11, Swift 5.1, iOS 12, macOS 10.15, tvOS 13, watchOS 6."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"Combine"},{"type":"text","text":", "},{"type":"codeVoice","code":"Core Haptics"},{"type":"text","text":", "},{"type":"codeVoice","code":"Create ML"},{"text":", and more.","type":"text"}],"title":"WWDC19","role":"collectionGroup","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"},"WWDC19-430-models":{"identifier":"WWDC19-430-models","alt":null,"type":"image","variants":[{"url":"\/images\/WWDC19-430-models.png","traits":["1x","light"]}]},"WWDC19-430-sound":{"alt":null,"identifier":"WWDC19-430-sound","variants":[{"url":"\/images\/WWDC19-430-sound.png","traits":["1x","light"]}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-249-Introducing-MultiCamera-Capture-for-iOS":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-249-Introducing-MultiCamera-Capture-for-iOS","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-249-introducing-multicamera-capture-for-ios","abstract":[{"text":"In AVCapture on iOS 13 it is now possible to simultaneously capture photos and video from multiple cameras on iPhone XS, iPhone XS Max, iPhone XR, and the latest iPad Pro. It is also possible to configure the multiple microphones on the device to shape the sound that is captured. Learn how to leverage these powerful capabilities to bring creative new features like picture-in-picture and spatial audio to your camera apps. Gain a deeper understanding of the performance considerations that may influence your app design.","type":"text"}],"kind":"article","role":"sampleCode","title":"Introducing Multi-Camera Capture for iOS"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-722-Introducing-Combine":{"url":"\/documentation\/wwdcnotes\/wwdc19-722-introducing-combine","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-722-Introducing-Combine","title":"Introducing Combine","kind":"article","role":"sampleCode","abstract":[{"type":"text","text":"Combine is a unified declarative framework for processing values over time. Learn how it can simplify asynchronous code like networking, key value observing, notifications and callbacks."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-245-Introducing-the-Indoor-Maps-Program":{"url":"\/documentation\/wwdcnotes\/wwdc19-245-introducing-the-indoor-maps-program","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-245-Introducing-the-Indoor-Maps-Program","title":"Introducing the Indoor Maps Program","kind":"article","role":"sampleCode","abstract":[{"text":"The Indoor Maps Program enables organizations with large public or private spaces to deliver user experiences that leverage precise location information and present stunning indoor maps. Learn the entire enablement workflow including, creation of a standards-based map definition, map validation, testing and calibration, and details on how to use MapKit and MapKit JS to integrate it all into your app or website.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10156-Control-training-in-Create-ML-with-Swift":{"url":"\/documentation\/wwdcnotes\/wwdc20-10156-control-training-in-create-ml-with-swift","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10156-Control-training-in-Create-ML-with-Swift","title":"Control training in Create ML with Swift","kind":"article","role":"sampleCode","abstract":[{"type":"text","text":"With the Create ML framework you have more power than ever to easily develop models and automate workflows. We’ll show you how to explore and interact with your machine learning models while you train them, helping you get a better model quickly. Discover how training control in Create ML can customize your training workflow with checkpointing APIs to pause, save, resume, and extend your training process. And find out how you can monitor your progress programmatically using Combine APIs."}]},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","title":"WWDC Notes","role":"collection","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"url":"\/documentation\/wwdcnotes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes"},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"link","identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}]},"https://github.com/zntfdr":{"url":"https:\/\/github.com\/zntfdr","type":"link","identifier":"https:\/\/github.com\/zntfdr","title":"GitHub","titleInlineContent":[{"type":"text","text":"GitHub"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML":{"url":"\/documentation\/wwdcnotes\/wwdc19-425-training-sound-classification-models-in-create-ml","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","title":"Training Sound Classification Models in Create ML","kind":"article","role":"sampleCode","abstract":[{"text":"Learn how to quickly and easily create Core ML models capable of classifying the sounds heard in audio files and live audio streams. In addition to providing you the ability to train and evaluate these models, the Create ML app allows you to test the model performance in real-time using the microphone on your Mac. Leverage these on-device models in your app using the new Sound Analysis framework.","type":"text"}]},"WWDC19-430-activity":{"type":"image","identifier":"WWDC19-430-activity","variants":[{"url":"\/images\/WWDC19-430-activity.png","traits":["1x","light"]}],"alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML":{"url":"\/documentation\/wwdcnotes\/wwdc20-10043-build-an-action-classifier-with-create-ml","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","title":"Build an Action Classifier with Create ML","kind":"article","role":"sampleCode","abstract":[{"text":"Discover how to build Action Classification models in Create ML. With a custom action classifier, your app can recognize and understand body movements in real-time from videos or through a camera. We’ll show you how to use samples to easily train a Core ML model to identify human actions like jumping jacks, squats, and dance moves. Learn how this is powered by the Body Pose estimation features of the Vision Framework. Get inspired to create apps that can provide coaching for fitness routines, deliver feedback on athletic form, and more.","type":"text"}]},"https://developer.apple.com/wwdc19/430":{"url":"https:\/\/developer.apple.com\/wwdc19\/430","type":"download","identifier":"https:\/\/developer.apple.com\/wwdc19\/430","checksum":null},"https://zntfdr.dev":{"identifier":"https:\/\/zntfdr.dev","type":"link","titleInlineContent":[{"type":"text","text":"Blog"}],"url":"https:\/\/zntfdr.dev","title":"Blog"},"WWDC19-430-classify":{"identifier":"WWDC19-430-classify","alt":null,"type":"image","variants":[{"url":"\/images\/WWDC19-430-classify.png","traits":["1x","light"]}]},"https://avatars.githubusercontent.com/u/5277837?v=4":{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/5277837?v=4","alt":"Profile image of Federico Zanetello","type":"image","variants":[{"url":"https:\/\/avatars.githubusercontent.com\/u\/5277837?v=4","traits":["1x","light"]}]}}}