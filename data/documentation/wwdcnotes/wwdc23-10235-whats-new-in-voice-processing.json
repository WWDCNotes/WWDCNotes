{"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10235-Whats-new-in-voice-processing","interfaceLanguage":"swift"},"schemaVersion":{"patch":0,"minor":3,"major":0},"primaryContentSections":[{"kind":"content","content":[{"anchor":"overview","text":"Overview","type":"heading","level":2},{"inlineContent":[{"text":"Speaker: Julian Yu, Core Audio","type":"text"}],"type":"paragraph"},{"text":"Voice Processing APIs","type":"heading","anchor":"Voice-Processing-APIs","level":2},{"inlineContent":[{"type":"text","text":"Both APIs provides the same voice processing capabilities."}],"type":"paragraph"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"NEW:"}],"type":"strong"},{"text":" Voice processing now available on tvOS","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"emphasis","inlineContent":[{"text":"See more in the session “Discover Continuity Camera on tvOS”.","type":"text"}]}],"type":"paragraph"},{"type":"heading","text":"AUVoiceIO (aka AUVoiceProcessingIO)","level":3,"anchor":"AUVoiceIO-aka-AUVoiceProcessingIO"},{"type":"paragraph","inlineContent":[{"type":"text","text":"For apps that interact with I\/O audio unit directly"}]},{"type":"heading","text":"AVAudioEngine","level":3,"anchor":"AVAudioEngine"},{"type":"paragraph","inlineContent":[{"text":"Higher-level API, easier to use, less code","type":"text"}]},{"type":"heading","text":"New APIs","level":2,"anchor":"New-APIs"},{"type":"paragraph","inlineContent":[{"type":"text","text":"AUVoiceIO and AVAudioEngine get new API to provide more controls over voice processing."}]},{"type":"heading","text":"Ducking behavior","level":3,"anchor":"Ducking-behavior"},{"type":"aside","name":"Note","content":[{"inlineContent":[{"text":"There also could be other apps playing audio at the same time as your app. All the audio streams other than the voice audio stream from your app are considered as “other audio” by Apple’s voice processing, and your voice audio gets mixed in with other audio before getting played to the output device. For voice chat apps, typically the primary focus of the playback audio is the voice chat audio. That’s why we duck the volume level of other audio, in order improve the intelligibility of the voice audio. In the past, we applied a fixed amount of ducking to other audio. This has worked well for most apps, and if your app is happy with the current ducking behavior, then you don’t need to do anything.","type":"text"}],"type":"paragraph"}],"style":"note"},{"type":"paragraph","inlineContent":[{"text":"When enable advanced ducking a ducking level can be set. There is default (the current behavior), minimum, medium and maximum, where maximum lowers the other apps playing audio the most.","type":"text"}]},{"type":"heading","text":"Enabling it with AUVoiceIO","level":4,"anchor":"Enabling-it-with-AUVoiceIO"},{"type":"codeListing","syntax":"swift","code":["const AUVoiceIOOtherAudioDuckingConfiguration duckingConfig = {","\t.mEnableAdvancedDucking = true,","\t.mDuckingLevel = AUVoiceIOOtherAudioDuckingLevel::kAUVoiceIOOtherAudioDuckingLevelMin","};","\/\/ AUVoiceIO creation code omitted","OSStatus err = AudioUnitSetProperty(auVoiceIO, kAUVoiceIOProperty_OtherAudioDuckingConfiguration, kAudioUnitScope_Global, 0, &duckingConfig, sizeof(duckingConfig));"]},{"type":"heading","text":"Enabling it with AVAudioEngine","level":4,"anchor":"Enabling-it-with-AVAudioEngine"},{"type":"codeListing","syntax":"swift","code":["let engine = AVAudioEngine()","let inputNode = engine.inputNode","do {","\ttry inputNode.setVoiceProcessingEnabled(true)","} catch {","\tprint(\"Could not enable voice processing \\(error)\")","}","let duckingConfig = AVAudioVoiceProcessingOtherAudioDuckingConfiguration(mEnableAdvancedDucking: false, mDuckingLevel: .max)","inputNode.voiceProcessingOtherAudioDuckingConfiguration = duckingConfig"]},{"type":"heading","text":"Detect muted speakers","level":3,"anchor":"Detect-muted-speakers"},{"type":"paragraph","inlineContent":[{"type":"text","text":"There is an API to detect presence of a muted talker"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"It was first introduced in iOS 15, and now available in macOS 14 and tvOS 17."}]},{"type":"heading","text":"How to use the API","level":4,"anchor":"How-to-use-the-API"},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"text":"Provide a listener block to receive the notification of muted talker detection","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Implement your handling of this notification in your listener block","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Implement mute via the mute API"}],"type":"paragraph"}]}]},{"type":"heading","text":"Using it with AUVoiceIO","level":4,"anchor":"Using-it-with-AUVoiceIO"},{"type":"codeListing","syntax":"swift","code":["UVoiceIOMutedSpeechActivityEventListener listener = ","^(AUVoiceIOMutedSpeechActivityEvent event) {\t\t","    if (event == kAUVoiceIOSpeechActivityHasStarted) {","\t\t\/\/ User has started talking while muted. Prompt the user to un-mute","\t} else if (event == kAUVoiceIOSpeechActivityHasEnded) {","\t\t\/\/ User has stopped talking while muted","\t}","};","OSStatus err = AudioUnitSetProperty(auVoiceIO, kAUVoiceIOProperty_MutedSpeechActivityEventListener, kAudioUnitScope_Global, 0, &listener,  sizeof(AUVoiceIOMutedSpeechActivityEventListener));","\/\/ When user mutes","UInt32 muteUplinkOutput = 1;","result = AudioUnitSetProperty(auVoiceIO, kAUVoiceIOProperty_MuteOutput, kAudioUnitScope_Global, 0, &muteUplinkOutput, sizeof(muteUplinkOutput));"]},{"type":"heading","text":"Using it with AVAudioEngine","level":4,"anchor":"Using-it-with-AVAudioEngine"},{"type":"codeListing","syntax":"swift","code":["let listener =  { (event : AVAudioVoiceProcessingSpeechActivityEvent) in","\tif (event == AVAudioVoiceProcessingSpeechActivityEvent.started) {","\t\t\/\/ User has started talking while muted. Prompt the user to un-mute","\t} else if (event == AVAudioVoiceProcessingSpeechActivityEvent.ended) {","\t\t\/\/ User has stopped talking while muted","\t}","}","inputNode.setMutedSpeechActivityEventListener(listener)","\/\/ When user mutes","inputNode.isVoiceProcessingInputMuted = true"]},{"type":"heading","text":"Alternative macOS APIs (if not ready for the new one)","level":4,"anchor":"Alternative-macOS-APIs-if-not-ready-for-the-new-one"},{"type":"orderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Enable voice activity detection on input device via "},{"code":"kAudioDevicePropertyVoiceActivityDetectionEnable","type":"codeVoice"},{"type":"text","text":"."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Register a listener callback on HAL property "},{"type":"codeVoice","code":"kAudioDevicePropertyVoiceActivityDetectionState"},{"type":"text","text":"."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"When notified, query ","type":"text"},{"type":"codeVoice","code":"kAudioDevicePropertyVoiceActivityDetectionState"},{"text":" for its current value","type":"text"}]}]}]},{"type":"codeListing","syntax":"swift","code":["\/\/ Enable Voice Activity Detection on the input device","const AudioObjectPropertyAddress kVoiceActivityDetectionEnable{","        kAudioDevicePropertyVoiceActivityDetectionEnable,","        kAudioDevicePropertyScopeInput,","        kAudioObjectPropertyElementMain };","OSStatus status = kAudioHardwareNoError;","UInt32 shouldEnable = 1;","status = AudioObjectSetPropertyData(deviceID, &kVoiceActivityDetectionEnable, 0, NULL, sizeof(UInt32), &shouldEnable);","\/\/ Register a listener on the Voice Activity Detection State property","const AudioObjectPropertyAddress kVoiceActivityDetectionState{","        kAudioDevicePropertyVoiceActivityDetectionState,","        kAudioDevicePropertyScopeInput,","        kAudioObjectPropertyElementMain };","status = AudioObjectAddPropertyListener(deviceID, &kVoiceActivityDetectionState, (AudioObjectPropertyListenerProc)listener_callback, NULL); \/\/ “listener_callback” is the name of your listener function"]},{"type":"codeListing","syntax":"swift","code":["OSStatus listener_callback(","    AudioObjectID                 inObjectID,","    UInt32                        inNumberAddresses,","    const AudioObjectPropertyAddress*   __nullable inAddresses,","    void* __nullable              inClientData)","{","  \/\/ Assuming this is the only property we are listening for, therefore no need to go through inAddresses","       UInt32 voiceDetected = 0;","     UInt32 propertySize = sizeof(UInt32);","     OSStatus status = AudioObjectGetPropertyData(inObjectID, &kVoiceActivityState, 0, NULL, &propertySize, &voiceDetected);","  ","       if (kAudioHardwareNoError == status) {"," if (voiceDetected == 1) {","    \/\/ voice activity detected","\t} else if (voiceDetected == 0) {","\t\t    \/\/ voice activity not detected","\t}"," }"," return status;","};"]},{"type":"aside","name":"Note","content":[{"inlineContent":[{"type":"text","text":"For HAL API clients to implement mute we highly recommend using HAL’s process mute API. It suppresses the recording indicator light in the menu bar, and gives user confidence that their privacy is protected under mute."}],"type":"paragraph"}],"style":"note"},{"type":"heading","text":"Written By","level":2,"anchor":"Written-By"},{"type":"row","numberOfColumns":5,"columns":[{"size":1,"content":[{"type":"paragraph","inlineContent":[{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/759680?v=4","type":"image"}]}]},{"size":4,"content":[{"text":"Morten Bjerg Gregersen","type":"heading","level":3,"anchor":"Morten-Bjerg-Gregersen"},{"inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/MortenGregersen","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"type":"reference","overridingTitle":"Contributed Notes","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/github.com\/MortenGregersen","type":"reference","isActive":true},{"type":"text","text":" "},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"http:\/\/atterdagapps.com","isActive":true,"type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/x.com\/mortengregersen","isActive":true,"type":"reference"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"reference","isActive":true,"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing"}]},{"type":"heading","text":"Related Sessions","level":2,"anchor":"Related-Sessions"},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10256-Discover-Continuity-Camera-for-tvOS"]},{"type":"small","inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}]}],"sections":[],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"abstract":[{"type":"text","text":"Learn how to use the Apple voice processing APIs to achieve the best possible audio experience in your VoIP apps. We’ll show you how to detect when someone is talking while muted, adjust ducking behavior of other audio, and more."}],"metadata":{"roleHeading":"WWDC23","role":"sampleCode","title":"What’s new in voice processing","modules":[{"name":"WWDC Notes"}]},"sampleCodeDownload":{"action":{"overridingTitle":"Watch Video (15 min)","identifier":"https:\/\/developer.apple.com\/wwdc23\/10235","isActive":true,"type":"reference"},"kind":"sampleDownload"},"kind":"article","variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10235-whats-new-in-voice-processing"]}],"references":{"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","title":"WWDC Notes","kind":"symbol","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection"},"https://developer.apple.com/wwdc23/10235":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/10235","checksum":null,"type":"download","url":"https:\/\/developer.apple.com\/wwdc23\/10235"},"https://github.com/MortenGregersen":{"identifier":"https:\/\/github.com\/MortenGregersen","title":"GitHub","type":"link","url":"https:\/\/github.com\/MortenGregersen","titleInlineContent":[{"type":"text","text":"GitHub"}]},"WWDCNotes.png":{"identifier":"WWDCNotes.png","alt":null,"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}]},"MortenGregersen.jpeg":{"identifier":"MortenGregersen.jpeg","alt":null,"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/MortenGregersen.jpeg"}]},"http://atterdagapps.com":{"identifier":"http:\/\/atterdagapps.com","title":"Blog","type":"link","url":"http:\/\/atterdagapps.com","titleInlineContent":[{"type":"text","text":"Blog"}]},"doc://WWDCNotes/documentation/WWDCNotes/MortenGregersen":{"abstract":[{"type":"text","text":"Hi 👋 I am Morten - I live in Denmark 🇩🇰"}],"role":"sampleCode","images":[{"type":"card","identifier":"MortenGregersen.jpeg"},{"type":"icon","identifier":"MortenGregersen.jpeg"}],"url":"\/documentation\/wwdcnotes\/mortengregersen","title":"Morten Bjerg Gregersen (21 notes)","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/MortenGregersen","kind":"article","type":"topic"},"https://avatars.githubusercontent.com/u/759680?v=4":{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/759680?v=4","alt":"Profile image of Morten Bjerg Gregersen","type":"image","variants":[{"traits":["1x","light"],"url":"https:\/\/avatars.githubusercontent.com\/u\/759680?v=4"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10256-Discover-Continuity-Camera-for-tvOS":{"kind":"article","title":"Discover Continuity Camera for tvOS","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10256-Discover-Continuity-Camera-for-tvOS","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10256-discover-continuity-camera-for-tvos","role":"sampleCode","abstract":[{"text":"Discover how you can bring AVFoundation, AVFAudio, and AudioToolbox to your apps on tvOS and create camera and microphone experiences for the living room. Find out how to support tvOS in your existing iOS camera experience with the Device Discovery API, build apps that use iPhone as a webcam or FaceTime source, and explore special considerations when developing for tvOS. We’ll also show you how to enable audio recording for tvOS, and how to use echo cancellation to create great voice-driven experiences.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10233-enhance-your-apps-audio-experience-with-airpods","title":"Enhance your app’s audio experience with AirPods","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods","type":"topic","abstract":[{"type":"text","text":"Discover how you can create transformative audio experiences in your app using AirPods. Learn how to incorporate AirPods Automatic Switching, use AVAudioApplication to support Mute Control, and take advantage of Spatial Audio to create immersive soundscapes in your app or game."}]},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","type":"link","url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"title":"WWDC23","role":"collectionGroup","kind":"article","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","url":"\/documentation\/wwdcnotes\/wwdc23","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}]},"https://x.com/mortengregersen":{"identifier":"https:\/\/x.com\/mortengregersen","title":"X\/Twitter","type":"link","url":"https:\/\/x.com\/mortengregersen","titleInlineContent":[{"type":"text","text":"X\/Twitter"}]}}}