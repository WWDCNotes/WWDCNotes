{"kind":"article","sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","overridingTitle":"Watch Video (12 min)","identifier":"https:\/\/developer.apple.com\/wwdc23\/10033","isActive":true}},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10033-extend-speech-synthesis-with-personal-and-custom-voices"]}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10033-Extend-Speech-Synthesis-with-personal-and-custom-voices"},"sections":[],"metadata":{"role":"sampleCode","roleHeading":"WWDC23","title":"Extend Speech Synthesis with personal and custom voices","modules":[{"name":"WWDC Notes"}]},"abstract":[{"text":"Bring the latest advancements in Speech Synthesis to your apps. Learn how you can integrate your custom speech synthesizer and voices into iOS and macOS. We’ll show you how SSML is used to generate expressive speech synthesis, and explore how Personal Voice can enable your augmentative and assistive communication app to speak on a person’s behalf in an authentic way.","type":"text"}],"schemaVersion":{"major":0,"minor":3,"patch":0},"primaryContentSections":[{"kind":"content","content":[{"type":"heading","text":"Overview","level":2,"anchor":"overview"},{"type":"paragraph","inlineContent":[{"text":"Speaker: Grant Maloney, Accessibility Engineer","type":"text"}]},{"type":"heading","text":"Explore Speech Synthesis Markup Language (SSML)","level":2,"anchor":"Explore-Speech-Synthesis-Markup-Language-SSML"},{"type":"paragraph","inlineContent":[{"type":"text","text":"What is SSML?"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"A W3C standard for speech","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Declarative XML document"}]}]},{"content":[{"inlineContent":[{"text":"Supported on all Platforms","type":"text"}],"type":"paragraph"}]}]},{"type":"codeListing","code":["let ssml = \"\"\"","    <speak>","        Hello","        <break time=\"1s\" \/>","        <prosody rate=\"200%\">nice to meet you!<\/prosody>","    <\/speak>","\"\"\"","","guard let ssmlUtterance = AVSpeechUtterance(ssmlRepresentation: ssml) else {","    return","}","","self.synthesizer.speak(ssmlUtterance)"],"syntax":"swift"},{"type":"heading","text":"Implement a synthesis provider","level":2,"anchor":"Implement-a-synthesis-provider"},{"type":"heading","text":"What is a speech synthesizer and how does it work?","level":3,"anchor":"What-is-a-speech-synthesizer-and-how-does-it-work"},{"type":"aside","name":"Note","content":[{"inlineContent":[{"text":"A speech synthesizer receives some text and information about desired speech properties in the form of SSML and provides an audio representation of that text.","type":"text"}],"type":"paragraph"}],"style":"note"},{"type":"aside","name":"Note","content":[{"inlineContent":[{"type":"text","text":"Speech Synthesis provider audio unit extensions will be embedded in a host app and will receive speech requests in the form of SSML. The extension will be responsible for rendering audio for the SSML input and optionally returning markers indicating where words occur within those audio buffers. The system will then manage all playback for that speech request. You don’t need to handle any audio session management; it’s managed internally by the Speech Synthesis Provider framework."}],"type":"paragraph"}],"style":"note"},{"type":"heading","text":"Creating a new voice","level":3,"anchor":"Creating-a-new-voice"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Create a new Audio Unit Extension app project in Xcode, then select the “Speech Synthesizer” Audio Unit Type and provide a four character subtype identifier for your synthesizer, as well as a four character identifier for you as a manufacturer."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Use an App Group to communicate between the extension and the host app."}]},{"type":"codeListing","code":["public class WWDCSynthAudioUnit: AVSpeechSynthesisProviderAudioUnit {","    public override var speechVoices: [AVSpeechSynthesisProviderVoice] {","        get {","            let voices: [String : String] = groupDefaults.value(forKey: \"voices\") as? [String : String] ?? [:]","            return voices.map { key, value in","                return AVSpeechSynthesisProviderVoice(name: value,","                                                identifier: key,","                                          primaryLanguages: [\"en-US\"],","                                        supportedLanguages: [\"en-US\"] )","            }","        }","    }","}"],"syntax":"swift"},{"type":"heading","text":"The synthesis provider","level":4,"anchor":"The-synthesis-provider"},{"type":"paragraph","inlineContent":[{"text":"When some text should be synthesized this function will be called with the SSML.","type":"text"}]},{"type":"codeListing","code":["public class WWDCSynthAudioUnit: AVSpeechSynthesisProviderAudioUnit {","    public override func synthesizeSpeechRequest(speechRequest: AVSpeechSynthesisProviderRequest) {","        currentBuffer = getAudioBuffer(for: speechRequest.voice, with: speechRequest.ssmlRepresentation)","        framePosition = 0","    }","","    public override func cancelSpeechRequest() {","        currentBuffer = nil","    }","}"],"syntax":"swift"},{"type":"heading","text":"The render block","level":3,"anchor":"The-render-block"},{"type":"codeListing","code":["public class WWDCSynthAudioUnit: AVSpeechSynthesisProviderAudioUnit {","    public override var internalRenderBlock: AUInternalRenderBlock {","       return { [weak self]","           actionFlags, timestamp, frameCount, outputBusNumber, outputAudioBufferList, _, _ in","           guard let self else { return kAudio_ParamError }","","           \/\/ This is the audio buffer we are going to fill up","           var unsafeBuffer = UnsafeMutableAudioBufferListPointer(outputAudioBufferList)[0]","           let frames = unsafeBuffer.mData!.assumingMemoryBound(to: Float32.self)","                ","           var sourceBuffer = UnsafeMutableAudioBufferListPointer(self.currentBuffer!.mutableAudioBufferList)[0]","           let sourceFrames = sourceBuffer.mData!.assumingMemoryBound(to: Float32.self)","","           for frame in 0..<frameCount {","               if frames.count > frame && sourceFrames.count > self.framePosition {","                   frames[Int(frame)] = sourceFrames[Int(self.framePosition)]","                   self.framePosition += 1","                   if self.framePosition >= self.currentBuffer!.frameLength {","                       break","                   }","               }","           }","                ","           return noErr","       }","    }","}"],"syntax":"swift"},{"type":"heading","text":"Sample app for purchasing voices","level":3,"anchor":"Sample-app-for-purchasing-voices"},{"type":"paragraph","inlineContent":[{"text":"Take note of the ","type":"text"},{"code":"AVSpeechSynthesisProviderVoice","type":"codeVoice"},{"text":" function ","type":"text"},{"code":"updateSpeechVoices()","type":"codeVoice"},{"text":". That is how your app can signal that the set of available voices for your synthesizer has changed and the system voice list should be rebuilt.","type":"text"}]},{"type":"codeListing","code":["struct ContentView: View {","    ","    @State var purchasedVoices: [WWDCVoice] = []","    ","    var body: some View {","        List {","            MyAwesomeVoicesSection","            PurchasedVoicesSection","        }","    }","    ","    func purchase(voice: WWDCVoice) {","        \/\/ Append voice to list of purchased voices","        purchasedVoices.append(voice)","        ","        \/\/ Inform system of change in voices","        AVSpeechSynthesisProviderVoice.updateSpeechVoices()","    }","}"],"syntax":"swift"},{"type":"heading","text":"Listen for changes in available system voices","level":3,"anchor":"Listen-for-changes-in-available-system-voices"},{"type":"codeListing","code":["struct ContentView: View {","    @State var systemVoices: [AVSpeechSynthesisVoice] = AVSpeechSynthesisVoice.speechVoices()","    ","    var body: some View {","        List {","            MyAwesomeVoicesSection","            PurchasedVoicesSection","            Section(\"System Voices\") {","                ForEach(systemVoices.filter { $0.language == \"en-US\" }) { voice in","                    Text(voice.name)","                }","            }","        }","        .onReceive(NotificationCenter.default","            .publisher(for: AVSpeechSynthesizer.availableVoicesDidChangeNotification)) { _ in","                systemVoices = AVSpeechSynthesisVoice.speechVoices()","        }","    }","}"],"syntax":"swift"},{"type":"heading","text":"Use Personal Voice","level":2,"anchor":"Use-Personal-Voice"},{"type":"aside","name":"Note","content":[{"type":"paragraph","inlineContent":[{"text":"People can now record and recreate their voice on iOS and macOS using the power of their device.","type":"text"}]}],"style":"note"},{"type":"aside","name":"Note","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Your Personal Voice is generated on the device and not on a server. This voice will appear amongst the rest of the System voices and can be used with a new feature called Live Speech. Live Speech is a type-to-speak feature on iOS, iPadOS, macOS, and watchOS that lets a person synthesize speech with their own voice on the fly."}]}],"style":"note"},{"type":"aside","name":"Note","content":[{"inlineContent":[{"type":"text","text":"You can request access to synthesize speech with these voices using a new request authorization API for Personal Voice. Keep in mind that usage of Personal Voice is sensitive and should be primarily used for augmentative or alternative communication apps."}],"type":"paragraph"}],"style":"note"},{"type":"codeListing","code":["struct ContentView: View {","    @State private var personalVoices: [AVSpeechSynthesisVoice] = []","","    func fetchPersonalVoices() async {","        AVSpeechSynthesizer.requestPersonalVoiceAuthorization() { status in","            if status == .authorized {","                personalVoices = AVSpeechSynthesisVoice.speechVoices().filter { $0.voiceTraits.contains(.isPersonalVoice) }","            }","        }","    }","}"],"syntax":"swift"},{"type":"paragraph","inlineContent":[{"text":"When authorized you can use the Personal Voice:","type":"text"}]},{"type":"codeListing","code":["func speakUtterance(string: String) {","    let utterance = AVSpeechUtterance(string: string)","    if let voice = personalVoices.first {","        utterance.voice = voice","        syntheizer.speak(utterance)","    }","}"],"syntax":"swift"},{"type":"heading","text":"Written By","level":2,"anchor":"Written-By"},{"type":"row","numberOfColumns":5,"columns":[{"size":1,"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"MortenGregersen"}]}]},{"size":4,"content":[{"type":"heading","anchor":"Morten-Bjerg-Gregersen","text":"Morten Bjerg Gregersen","level":3},{"type":"paragraph","inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/MortenGregersen","type":"reference","overridingTitle":"Contributed Notes","isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/github.com\/MortenGregersen","type":"reference","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"http:\/\/atterdagapps.com","type":"reference","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/x.com\/mortengregersen","type":"reference","isActive":true}]}]}]},{"type":"thematicBreak"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true,"type":"reference"}]},{"type":"heading","text":"Related Sessions","level":2,"anchor":"Related-Sessions"},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10022-Create-a-seamless-speech-experience-in-your-apps"]},{"type":"small","inlineContent":[{"inlineContent":[{"type":"text","text":"Legal Notice"}],"type":"strong"}]},{"type":"small","inlineContent":[{"text":"All content copyright © 2012 – 2025 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}]}],"references":{"MortenGregersen":{"type":"image","alt":"Profile image of Morten Bjerg Gregersen","variants":[{"url":"\/images\/WWDCNotes\/MortenGregersen.jpeg","traits":["1x","light"]}],"identifier":"MortenGregersen"},"doc://WWDCNotes/documentation/WWDCNotes":{"url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","type":"topic","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"role":"collection","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"kind":"symbol","title":"WWDC Notes"},"doc://WWDCNotes/documentation/WWDCNotes/MortenGregersen":{"title":"Morten Bjerg Gregersen (21 notes)","kind":"article","url":"\/documentation\/wwdcnotes\/mortengregersen","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/MortenGregersen","images":[{"identifier":"MortenGregersen.jpeg","type":"card"},{"identifier":"MortenGregersen.jpeg","type":"icon"}],"abstract":[{"type":"text","text":"Hi 👋 I am Morten - I live in Denmark 🇩🇰"}],"role":"sampleCode"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Contributions are welcome!","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link"},"WWDC23.jpeg":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC23.jpeg","traits":["1x","light"]}],"type":"image","identifier":"WWDC23.jpeg","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10022-Create-a-seamless-speech-experience-in-your-apps":{"title":"Create a seamless speech experience in your apps","url":"\/documentation\/wwdcnotes\/wwdc20-10022-create-a-seamless-speech-experience-in-your-apps","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10022-Create-a-seamless-speech-experience-in-your-apps","abstract":[{"type":"text","text":"Augment your app’s accessibility experience with speech synthesis: Discover the best times and places to add speech APIs so that everyone who uses your app can benefit. Learn how to use AVSpeechSynthesizer to complement assistive technologies like VoiceOver, and when to implement alternative APIs. And we’ll show you how to route audio to the appropriate source and create apps that integrate speech seamlessly for all who need or want it."}],"role":"sampleCode","type":"topic"},"MortenGregersen.jpeg":{"type":"image","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/MortenGregersen.jpeg","traits":["1x","light"]}],"identifier":"MortenGregersen.jpeg"},"https://github.com/MortenGregersen":{"title":"GitHub","titleInlineContent":[{"text":"GitHub","type":"text"}],"url":"https:\/\/github.com\/MortenGregersen","identifier":"https:\/\/github.com\/MortenGregersen","type":"link"},"http://atterdagapps.com":{"url":"http:\/\/atterdagapps.com","titleInlineContent":[{"type":"text","text":"Blog"}],"title":"Blog","type":"link","identifier":"http:\/\/atterdagapps.com"},"WWDC23-Icon.png":{"type":"image","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC23-Icon.png","traits":["1x","light"]}],"identifier":"WWDC23-Icon.png"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"SwiftData"},{"type":"text","text":", "},{"type":"codeVoice","code":"Observation"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit"},{"type":"text","text":" views, and more."}],"images":[{"identifier":"WWDC23-Icon.png","type":"icon"},{"identifier":"WWDC23.jpeg","type":"card"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","title":"WWDC23","role":"collectionGroup","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23","type":"topic"},"https://developer.apple.com/wwdc23/10033":{"checksum":null,"url":"https:\/\/developer.apple.com\/wwdc23\/10033","identifier":"https:\/\/developer.apple.com\/wwdc23\/10033","type":"download"},"https://x.com/mortengregersen":{"url":"https:\/\/x.com\/mortengregersen","titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"title":"X\/Twitter","type":"link","identifier":"https:\/\/x.com\/mortengregersen"},"WWDCNotes.png":{"alt":null,"identifier":"WWDCNotes.png","variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"type":"image"}}}