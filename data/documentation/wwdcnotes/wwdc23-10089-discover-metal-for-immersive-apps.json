{"seeAlsoSections":[{"identifiers":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10239-Add-SharePlay-to-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10248-Analyze-hangs-with-Instruments","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10258-Animate-symbols-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10158-Animate-with-springs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10159-Beyond-scroll-views","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10170-Beyond-the-basics-of-structured-concurrency","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10028-Bring-widgets-to-life","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10027-Bring-widgets-to-new-places","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10093-Bring-your-Unity-VR-app-to-a-fully-immersive-space","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10123-Bring-your-game-to-Mac-Part-1-Make-a-game-plan","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10124-Bring-your-game-to-Mac-Part-2-Compile-your-shaders","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10125-Bring-your-game-to-Mac-Part-3-Render-with-Metal","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10023-Build-a-multidevice-workout-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10036-Build-accessible-apps-with-SwiftUI-and-UIKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10154-Build-an-app-with-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10056-Build-better-documentbased-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10016-Build-custom-workouts-with-WorkoutKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10096-Build-great-games-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10252-Build-programmatic-UI-with-Xcode-Previews","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10006-Build-robust-and-resumable-file-transfers","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10087-Build-spatial-SharePlay-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10080-Build-spatial-experiences-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10029-Build-widgets-for-the-Smart-Stack-on-Apple-Watch","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10051-Create-a-great-ShazamKit-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10070-Create-a-great-spatial-playback-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10105-Create-a-more-responsive-camera-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10034-Create-accessible-spatial-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10257-Create-animated-symbols","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10088-Create-immersive-Unity-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10278-Create-practical-workflows-in-Xcode-Cloud","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10244-Create-rich-documentation-with-SwiftDocC","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10007-Create-seamless-experiences-with-Virtualization","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10101-Customize-ondevice-speech-recognition","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10226-Debug-with-structured-logging","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10071-Deliver-video-content-for-spatial-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10160-Demystify-SwiftUI-performance","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10263-Deploy-passkeys-at-work","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10193-Design-Shortcuts-for-Spotlight","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10138-Design-and-build-apps-for-watchOS-10","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10078-Design-considerations-for-vision-and-motion","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10073-Design-for-spatial-input","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10076-Design-for-spatial-user-interfaces","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10075-Design-spatial-SharePlay-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10309-Design-widgets-for-the-Smart-Stack-on-Apple-Watch","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10115-Design-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10203-Develop-your-first-immersive-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10052-Discover-Calendar-and-EventKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10256-Discover-Continuity-Camera-for-tvOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10149-Discover-Observation-in-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10085-Discover-Quick-Look-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10155-Discover-String-Catalogs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10180-Discover-streamlined-location-updates","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10196-Dive-deeper-into-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10254-Do-more-with-Managed-Apple-IDs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10110-Elevate-your-windowed-app-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10107-Embed-the-Photos-Picker-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10094-Enhance-your-iPad-and-iPhone-apps-for-the-Shared-Space","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10091-Evolve-your-ARKit-app-for-spatial-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10167-Expand-on-Swift-macros","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10275-Explore-AirPlay-with-interstitials","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10012-Explore-App-Store-Connect-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10042-Explore-Natural-Language-multilingual-models","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10156-Explore-SwiftUI-animation","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10103-Explore-enhancements-to-App-Intents","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10192-Explore-enhancements-to-RoomPlan","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10271-Explore-immersive-sound-design","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10122-Explore-media-formats-for-the-web","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10037-Explore-pie-charts-and-interactivity-in-Swift-Charts","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10095-Explore-rendering-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10142-Explore-testing-inapp-purchases","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10086-Explore-the-USD-ecosystem","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10033-Extend-Speech-Synthesis-with-personal-and-custom-voices","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10175-Fix-failures-faster-with-Xcode-test-reports","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10168-Generalize-APIs-with-parameter-packs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10260-Get-started-with-building-apps-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10060-Get-started-with-privacy-manifests","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10111-Go-beyond-the-window-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10161-Inspectors-in-SwiftUI-Discover-the-details","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10104-Integrate-your-media-app-with-HomePod","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10281-Keep-up-with-the-keyboard","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10229-Make-features-discoverable-with-TipKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10189-Migrate-to-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10172-Mix-Swift-and-C++","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10195-Model-your-schema-with-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10150-Optimize-CarPlay-for-vehicle-systems","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10127-Optimize-GPU-renderers-with-Metal","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10100-Optimize-app-power-and-performance-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10050-Optimize-machine-learning-for-Metal-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10035-Perform-accessibility-audits-for-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10072-Principles-of-spatial-design","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10266-Protect-your-Mac-app-with-environment-constraints","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10250-Prototype-with-Xcode-Playgrounds","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10002-Ready-set-relay-Protect-app-traffic-with-network-relays","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10262-Rediscover-Safari-developer-features","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10004-Reduce-network-delays-with-L4S","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10090-Run-your-iPad-and-iPhone-apps-in-the-Shared-Space","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10241-Share-files-with-SharePlay","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10224-Simplify-distribution-in-Xcode-and-Xcode-Cloud","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10102-Spotlight-your-app-with-App-Shortcuts","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10137-Support-Cinematic-mode-videos-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10181-Support-HDR-images-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10188-Sync-to-iCloud-with-CKSyncEngine","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10113-Take-SwiftUI-to-the-next-dimension","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10162-The-SwiftUI-cookbook-for-focus","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10238-Tune-up-your-AirPlay-audio-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10057-Unleash-the-UIKit-trait-system","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10153-Unlock-the-power-of-grammatical-agreement","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10185-Update-Live-Activities-with-push-notifications","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10031-Update-your-app-for-watchOS-10","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10061-Verify-app-dependencies-with-digital-signatures","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10058-Whats-new-with-text-and-text-interactions","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10157-Wind-your-way-through-advanced-animations-in-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10273-Work-with-Reality-Composer-Pro-content-in-Xcode","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10166-Write-Swift-macros","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10128-Your-guide-to-Metal-ray-tracing"],"generated":true,"title":"Deep Dives into Topics"}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10089-Discover-Metal-for-immersive-apps","interfaceLanguage":"swift"},"primaryContentSections":[{"kind":"content","content":[{"anchor":"Chapters","text":"Chapters","level":2,"type":"heading"},{"inlineContent":[{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=6","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=110","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=358","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=689","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=780","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1048","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1222","isActive":true,"type":"reference"}],"type":"paragraph"},{"anchor":"Intro","text":"Intro","level":1,"type":"heading"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"App architecture. To write our own engine, the CompositorServices API gives access to Metal rendering on xrOS. We can combine it with ARKit, which adds world tracking and hand tracking, to create a fully immersive experience.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Render configuration. CompositorServices is the key to configure our engine to work on xrOS."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Render loop, how to setup the render loop"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"And how to render one frame."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"User input, how to use ARKit to make our experience interactive.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"text":"How to create immersive experiences with Metal on xrOS.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"RecRoom is a great example of an application that provides a fully immersive experience using CompositorServices to create a rendering session, Metal APIs to render frames, and ARKit to get world and hand tracking.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-recRoom"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"They were able to bring support to all these technologies thanks to the Unity editor."}],"type":"paragraph"},{"anchor":"App-architecture","text":"App architecture","level":1,"type":"heading"},{"inlineContent":[{"type":"text","text":"Start with the architecture of an xrOS app."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"we’ll get the most out of today’s session if we have previous experience with Metal APIs and Metal rendering techniques. If we haven’t used Metal before, check out the code samples and documentation over in developer.apple.com\/Metal."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10089-metal","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"When we create immersive experiences on xrOS with Metal, we’ll start with SwiftUI to create the application and the rendering session."}],"type":"paragraph"},{"inlineContent":[{"text":"After creating the rendering session, we can switch to a language we might be more familiar with, like C or C++, to define the inner parts of the engine.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"we start by creating a type that conforms to the SwiftUI app protocol. To conform to this protocol, we will define the list of scenes in our app."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"On xrOS, there are three main scenes types."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The window type provides an experience similar to 2D platforms like macOS. The volume type renders content within its bounds and it coexists in the Shared Space with other applications. And ImmersiveSpace allows us to render content anywhere."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal1"}],"type":"paragraph"},{"inlineContent":[{"text":"Whenever we render fully immersive experiences with Metal, we will choose the ImmersiveSpace type.","type":"text"}],"type":"paragraph"},{"anchor":"ImmersiveSpace","text":"ImmersiveSpace","level":2,"type":"heading"},{"inlineContent":[{"text":"ImmersiveSpace is a new SwiftUI Scene type available on xrOS.","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Conforms to SwiftUI Scene","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Container for immersive experiences","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"It serves as the container for fully immersive experiences.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"To learn how to use ImmersiveSpace, check out the session “Go beyond the window with SwiftUI.”"}],"type":"paragraph"},{"inlineContent":[{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111","type":"reference","isActive":true}],"type":"paragraph"},{"anchor":"Rendering-on-an-ImmersiveSpace","text":"Rendering on an ImmersiveSpace","level":2,"type":"heading"},{"inlineContent":[{"type":"text","text":"When we create an ImmersiveSpace scene, our application provides the content by using a type that conforms to the ImmersiveSpaceContent protocol. Often, when creating content for an ImmersiveSpace Scene, applications will use RealityKit."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal2"}],"type":"paragraph"},{"inlineContent":[{"text":"It uses CoreAnimation and MaterialX under the hood. But we canuse the power of Metal to render the content of our application.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10089-metal3","type":"image"}],"type":"paragraph"},{"anchor":"CompositorServices-API","text":"CompositorServices API","level":2,"type":"heading"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Metal rendering interface"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Render directly to the compositor server","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Low IPC overhead to minimize latency","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"The new CompositorServices API, introduced in xrOS, provides a Metal rendering interface to be able to render the contents of an ImmersiveSpace."}],"type":"paragraph"},{"inlineContent":[{"text":"With CompositorServices, applications can render directly into the compositor server. It has a low IPC overhead to minimize latency, and it is built from the ground up to support both C and Swift APIs.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"When using CompositorServices, the ImmersiveSpaceContent is called CompositorLayer."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To create a CompositorLayer we will need to provide two parameters. The first one is the CompositorLayerConfiguration protocol, which defines the behavior and capabilities of the rendering session. The second is the LayerRenderer."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal4"}],"type":"paragraph"},{"inlineContent":[{"text":"This is the interface to the layer rendering session. our application will use this object to schedule and render new frames.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"When writing an immersive experience with Metal, we start by defining the app type. As the scene type, use ImmersiveSpace."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"For the content type, use a CompositorLayer."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Once the CompositorLayer is ready to render content, the system will call the application with the instance of the rendering session.Here is a good place to create the instance of our custom engine.Now that we have the engine instance, we can create the render thread and run the render loop by calling start."}],"type":"paragraph"},{"syntax":"swift","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer { layerRenderer in","\t\t\t\tlet engine = my_engine_create(layerRenderer)","\t\t\t\tlet renderThread = Thread {","\t\t\t\t\tmy_engine_render_loop(engine)","\t\t\t\t}","\t\t\t\trenderThread.name = \"Render Thread\"","\t\t\t\trenderThread.start ()","\t\t\t}","\t\t}","\t}","}"],"type":"codeListing"},{"anchor":"Customize-default-app-scene","text":"Customize default app scene","level":2,"type":"heading"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Bounded window by default"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Space with Compositor Content as the initial Scene","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"One thing to take into account when defining the scene list in our application, is that by default SwiftUI creates a window scene, even if the first scene in our app is an ImmersiveSpace."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To change that default behavior, we can modify the info plist of our app.we can add the key UIApplicationPreferred DefaultSceneSessionRole to our application scene manifest to change the default scene type of our application."}],"type":"paragraph"},{"inlineContent":[{"text":"If we are using a space with a Compositor SpaceContent, we will use CPSceneSessionRole ImmersiveSpaceApplication.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10089-metal5","type":"image"}],"type":"paragraph"},{"anchor":"Render-configuration","text":"Render configuration","level":1,"type":"heading"},{"inlineContent":[{"type":"text","text":"After setting up the application, and before getting in the render loop, we’ll tell CompositorServices how to configure the LayerRenderer."}],"type":"paragraph"},{"anchor":"Configure-CompositorLayer","text":"Configure CompositorLayer","level":2,"type":"heading"},{"inlineContent":[{"type":"text","text":"To provide a configuration to the CompositorLayer, we will create a new type that conforms to the CompositorLayerConfiguration protocol. This protocol allows we to modify the setup and some of the behavior of the rendering session."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The CompositorLayerConfiguration provides we two parameters. The first one is the layer capabilities. It enables we to query what features are available on the device. Use the capabilities to create a valid configuration."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"And the second parameter is the LayerRenderer Configuration.This type defines the configuration of our rendering session. With the configuration, we can define how our engine maps its content into the layer, enable foveated rendering, and define the color management of our pipeline."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10089-metal6","type":"image"}],"type":"paragraph"},{"anchor":"Render-with-foveation","text":"Render with foveation","level":2,"type":"heading"},{"inlineContent":[{"type":"text","text":"The main goal of foveated rendering is to allow we to render content at a higher pixel-per-degree density without using a bigger texture size.In a regular display pipeline, the pixels are distributed linearly in a texture."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10089-metal7","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"xrOS optimizes this workflow by creating a map that defines what regions in a display can use a lower sampling rate."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal8"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This helps reduce the power required to render our frame while maintaining the visual fidelity of the display. Using foveation whenever possible is important, as it will result in a better visual experience."}],"type":"paragraph"},{"inlineContent":[{"text":"A great way to visualize how foveation affects our rendering pipeline is using Xcode’s Metal Debugger. With Metal Debugger, we can inspect the target textures and rasterization rate maps being used in the render pipeline.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This capture shows the contents of the texture without scaling for the rasterization rate map."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal9"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"we can notice the different sample rates by focusing in the regions of the texture that are more compressed.With the attachment viewer options in Metal Debugger, we can scale the image to visualize the final result that the display will show. Compositor provides the foveation map using an MTLRasterizationRateMap for each frame."}],"type":"paragraph"},{"inlineContent":[{"text":"It is a good practice to always check if foveation is supported.This will change depending on the platform.","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Metal rasterization rate APIs"}]}]}],"type":"unorderedList"},{"inlineContent":[{"code":"MTLRasterizationRateMap","type":"codeVoice"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Check if foveation is supported","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"codeVoice","code":"capabilities.supportsFoveation"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Set if our application supports foveation"}]}]}],"type":"unorderedList"},{"inlineContent":[{"code":"configuration.isFoveationEnabled = true","type":"codeVoice"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"For example, in the xrOS simulator, foveation is not available."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To enable foveation, we can set isFoveationEnabled on the configuration."}],"type":"paragraph"},{"anchor":"LayerRenderer","text":"LayerRenderer","level":2,"type":"heading"},{"inlineContent":[{"text":"The LayerRenderer property is one of the most important configurations for our engine. It defines how each display from the headset gets mapped into the rendered content of our application.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"Each eye first maps into a Metal texture provided by Compositor. Then Compositor provides the index of which slice to use within that texture.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal10"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"And finally, Compositor provides the viewport to use within that texture slice. The LayerRenderer lawet lets we choose different mappings between the texture slice and viewport."}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"With layered, Compositor will use one texture with two slices and two viewports."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"With dedicated, Compositor will use two textures with one slice and one viewport each."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"And finally with shared, Compositor will use one texture, one slice, and two different viewports for that slice."}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"identifier":"WWDC23-10089-metal11","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Choosing which lawet to use will depend on how we set up our rendering pipeline. For example, with layered and shared, we will be able to perform our rendering in one single pass, so we can optimize our rendering pipeline.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"With shared lawet, it might be easier to port existing code bases where foveated rendering is not an option. Layered lawet is the optimal lawet since it allows we to render our scene in a single pass while still maintaining foveated rendering."}],"type":"paragraph"},{"anchor":"Color-management","text":"Color management","level":2,"type":"heading"},{"inlineContent":[{"type":"text","text":"The last configuration property to discuss is color management."}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Color space"}]}]}],"type":"unorderedList"},{"inlineContent":[{"code":"CGColorSpace.extendedLinearDisplayP3","type":"codeVoice"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"EDR headroom of 2.0 (2x SDR)"}]}]},{"content":[{"inlineContent":[{"text":"HDR-compatible pixel format","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"codeVoice","code":"configuration.colorFormat = MTLPixelFormat.rgba16Float"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Compositor expects the content to be rendered with extended linear display P3 color space. xrOS supports an EDR headroom of 2.0."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"That is two times the SDR range. By default, Compositor does not use a pixel format that is HDR renderable, but if our application supports HDR, we can specify rgba16Float in the layer configuration."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal12"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"If we want to know more about how to render HDR with EDR, checkout the session:"}],"type":"paragraph"},{"inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To create a custom configuration in our application, start by defining a new type that conforms to the CompositorLayerConfiguration protocol. To conform to this protocol, add the makeConfiguration method."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This method provides the layer capabilities and a configuration we can modify.To enable the three properties I mentioned before, first check if foveation is supported. Then check what lawets are supported in this device."}],"type":"paragraph"},{"inlineContent":[{"text":"With this information, we can set a valid lawet in the configuration.In some devices like the simulator, where the Compositor only renders one view, layered won’t be available.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"For foveation, set it to true if the device supports it. And finally, set the colorFormat to rgba16Float to be able to render HDR content.","type":"text"}],"type":"paragraph"},{"syntax":"swift","code":["\/\/ CompositorLayer configuration","struct MyConfiguration: CompositorLayerConfiguration {","\tfunc makeConfiguration(capabilities: LayerRenderer. Capabilities,","\t\tconfiguration: inout LayerRenderer.Configuration) {","\t\t\t","\t\t\tlet supportsFoveation = capabilities.supportsFoveation","\t\t\t","\t\t\tlet supportedLawets = capabilities.supportedLawets(options: supportsFoveation ? [.foveationEnabled] : [])","\t\t","\t\t\tconfiguration.lawet = supportedLawets.contains(.layered) ? .layered : .dedicated","","\t\t\tconfiguration.isFoveationEnabled = supportsFoveation","\t\t\t","\t\t\t\/\/ HDR support","\t\t\tconfiguration.colorFormat = .rgba16Float","\t}","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"Returning to the code that created the Compositor layer, we can now add the configuration type we just created."}],"type":"paragraph"},{"syntax":"swift","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer(configuration: MyConfiguration()) { layerRenderer in","\t\t\tlet engine = my_engine_create(layerRenderer)","\t\t\tlet renderThread = Thread {","\t\t\t\tmy_engine_render_loop(engine) \/\/ <----","\t\t\t}","\t\t\trenderThread.name = \"Render Thread\"","\t\t\trenderThread.start()","\t\t\t}","\t\t}","\t}","}"],"type":"codeListing"},{"anchor":"Render-loop","text":"Render loop","level":1,"type":"heading"},{"inlineContent":[{"type":"text","text":"Now that the rendering session is configured, we can set up the render loop."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"we’ll start by using the LayerRenderer object from the CompositorLayer."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"First, we’ll load the resources and initialize any objects that our engine will need to render frames."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Then check the state of the layer.If the layer is paused, wait until the layer is running. Once the layer is unblocked from the wait, check the layer state again. If the layer is running, we’ll be able to render a frame."}],"type":"paragraph"},{"inlineContent":[{"text":"And once that frame is rendered, check the layer state again before rendering the next frame. If the layer state is invalidated, free the resources we created for the render loop.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10089-metal13","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Now, it’s time to define the main function of the render_loop."}],"type":"paragraph"},{"syntax":"swift","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer(configuration: MyConfiguration()) { layerRenderer in","\t\t\tlet engine = my_engine_create(layerRenderer)","\t\t\tlet renderThread = Thread {","\t\t\t\tmy_engine_render_loop(engine) \/\/ <-------","\t\t\t}","\t\t\trenderThread.name = \"Render Thread\"","\t\t\trenderThread.start()","\t\t\t}","\t\t}","\t}","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"Until now we’ve been using Swift since the ImmersiveSpace API is only available in Swift."}],"type":"paragraph"},{"inlineContent":[{"text":"But from here we will switch to C to write the render loop.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"As mentioned, the first step in the render loop is to allocate and initialize all the objects we’ll need to render frames. we’ll do this by calling the setup function in our custom engine."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Next, is the main section of the loop. The first step is to check the layerRenderer state. If the state is paused, the thread will sleep until the layerRenderer is running."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"If the layer state is running, the engine will render one frame. And finally, if the layer has been invalidated, the render loop will finish. The last step of the render_loop function will be to clear any used resources."}],"type":"paragraph"},{"syntax":"c","code":["void my_engine_render_loop(my_engine *engine) {","\tmy_engine_setup_render_pipeline (engine);","\t","\tbool is_rendering = true;","\twhile (is_rendering) @autoreleasepool {","\t\tswitch (cp_layer_renderer_get_state(engine->layer_renderer)) {","\t\t\tcase cp_layer_renderer_state_paused:","\t\t\t\tcp_layer_renderer_wait_until_running(engine->layer_renderer);","\t\t\t\tbreak;","\t\t\tcase cp_layer_renderer_state_running:","\t\t\t\tmy_engine_render_new_frame(engine);","\t\t\t\tbreak;","\t\t\tcase cp_layer_ renderer_ state_ invalidated:","\t\t\t\tis_rendering = false;","\t\t\tbreak;","\t\t}","\t}","\tmy_engine_invalidate(engine);","}"],"type":"codeListing"},{"anchor":"Render-one-frame","text":"Render one frame","level":1,"type":"heading"},{"inlineContent":[{"type":"text","text":"Now that the app is going through the render loop, we will see how to render one frame. Rendering content in xrOS is always from the point of view of the device. we can use ARKit to obtain the device orientation and translation."}],"type":"paragraph"},{"anchor":"World-tracking-with-ARKit","text":"World tracking with ARKit","level":2,"type":"heading"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"New xrOS API"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Tracking session for world, hands, and more"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Supports C and Swift APIs","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"ARKit is already available on iOS, and now xrOS is introducing a whole new API, which has additional features that can help we create immersive experiences. With ARKit, we can add world tracking, hand tracking, and other world sensing capabilities to our application."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The new ARKit API is also built from the ground up to support C and Swift APIs, which will allow for an easier integration with existing rendering engines."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To learn more about ARKit on xrOS, check out:"}],"type":"paragraph"},{"inlineContent":[{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082"}],"type":"paragraph"},{"inlineContent":[{"text":"Within the render loop, it’s time to render one frame. When rendering a frame, Compositor defines two main sections. The first one is the update. Here’s where we will do any work that is not input-latency critical.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"This can be things like updating the animations in our scene, updating our characters, or gathering inputs in our system like hand skeleton poses.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"The second section of the frame is the submission section. Here’s where we will perform any latency-critical work. we’ll also render any content that is headset-pose dependent here.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10089-metal14","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"In order to define the timing for each of those sections, Compositor provides a timing object. This diagram defines how the timing affects the different frame sections. The CPU and GPU tracks represent the work that is being done by our application. And the Compositor track represents the work done by the Compositor server to display our frame. The timing type from Compositor Services defines three main time values. First is the optimal input time. That is the best time to query the latency-critical input and start rendering our frame. Second is the rendering deadline. That is the time by when our CPU and GPU work to render a frame should be finished. And third is presentation time. That is the time when our frame will be presented on display.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"In the two sections of our frame, The update section should happen before the optimal input time. After the update, we will wait for the optimal input time before starting the frame submission. Then we will perform the frame submission, which will submit the render work to the GPU. It is important to notice that the CPU and GPU work needs to finish before the rendering deadline, otherwise the Compositor server won’t be able to use this frame and will use a previous one instead. Finally, on the rendering deadline, the Compositor server will composite this frame with the other layers in the system."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal15"}],"type":"paragraph"},{"inlineContent":[{"text":"Back to the render loop code, it’s time to define the render_new_frame function. In our engine’s render_new_frame function, we will first query a frame from the layerRenderer. With the frame object, we will be able to predict the timing information. Use that timing information to scope the update and submit intervals.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"Next, implement the update section. Define this section by calling the start and end update on the frame. Inside, we will gather the device inputs and update the contents of the frame.","type":"text"}],"type":"paragraph"},{"syntax":"c","code":["void my_engine_render_new_frame(my_engine *engine) {","","\tcp_frame_t frame = cp_layer_renderer_query_next_frame (engine->layer_renderer);","\tif (frame == nullptr) { return; }","\t","\tcp_frame_timing_t timing = cp_frame_predict_timing(frame);","\tif (timing == nullptr) { return; }","\t","\tcp_frame_start_update(frame);","\t","\tmy_input_state input_state = my_engine_gather_inputs (engine, timing);","\tmy_engine_update_frame(engine, timing, input_state);","\t","\tcp_frame_end_update(frame);","\t","\t\/\/ ...","}"],"type":"codeListing"},{"inlineContent":[{"text":"Once the update is finished, wait for the optimal input time before starting the submission. After the wait, define the submission section by calling start and end submission.","type":"text"}],"type":"paragraph"},{"syntax":"c","code":["void my_engine_render_new_frame(my_engine *engine) {","\t\/\/ ... query frame, predict timing, update frame contents.","","\t\/\/ Wait until the optimal time for querying the input","\tcp_time_wait_until(cp_frame_timing_get_optimal_input_time(timing));","","\t\/\/ ...","}"],"type":"codeListing"},{"inlineContent":[{"text":"Inside this section, first query the drawable object. Similar to CAMetalLayer, the drawable object contains the target texture and the information that we will need to set up the render pipeline. Now that we have our drawable, we can get the final timing information that Compositor will use to render this frame. With the final timing, we can query the ar_pose.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"It is important to set the pose in the drawable since it will be used by the Compositor to perform reprojection on the frame. Here I’m getting the pose by calling the get_ar_pose function in my engine object. But we will need to implement the contents of this function using the ARKit world tracking APIs. The last step of the function will be to encode all the GPU work and submit the frame."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Inside the submit_frame, use the drawable to render the contents of the frame as usual."}],"type":"paragraph"},{"syntax":"c","code":["void my_engine_render_new_frame(my_engine *engine) {","\t\/\/ ... query frame, predict timing, update frame contents.","\t\/\/ ... wait for optimal input time.","\t","\tcp_frame_ start_ submission(frame);","","\tcp_drawable_t drawable = cp_frame_query_drawable(frame);","\tif (drawable == nullptr) { return; }","\t","\tcp_frame_timing_t final_timing = cp_drawable_get_frame_timing(drawable);","\tar_pose_t pose = my_engine_get_ar_pose(engine, final_timing);","\tcp_drawable_set_ar_pose(drawable, pose);","","\tmy_engine_draw_and_submit_ _frame (engine, frame, drawable);","\t","\tcp_frame_end_submission(frame);","}"],"type":"codeListing"},{"anchor":"User-input","text":"User input","level":1,"type":"heading"},{"inlineContent":[{"text":"Now that the render loop is rendering frames, it’s time to make our immersive experience interactive. This video shows how RecRoom using Unity is already taking advantage of the ARKit and Compositor APIs to add interactivity to their application.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"There are two main input sources driving this interaction. ARKit’s HandTracking is providing the hand skeleton to render virtual hands. And pinch events from the LayerRenderer are driving the user interactions.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"In order to make the experience interactive, we’ll first gather the user input and then apply it to the contents of our scene. All this work will happen in the update section of the frame.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal16"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"There are two main input sources, the LayerRenderer and the ARKit HandTracking provider. With the LayerRenderer, we will get updates every time the application receives a pinch event. These updates are exposed in the form of spatial events. These events contains three main properties. The phase will tell we if the event is active, if it finished, or if it got canceled. The selection ray will allow we to determine the content of the scene that had the attention when the event began. And the last event property is the manipulator pose. This is the pose of the pinch and gets updated every frame for the duration of the event. From the HandTracking API, we will be able to get skeleton for both the left and right hands."}],"type":"paragraph"},{"inlineContent":[{"text":"Now, it’s time to add input support in the code.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Before gathering the input, we will decide if our application is rendering virtual hands or if it uses passthrough hands. Add the upperLimbVisibility scene modifier to the ImmersiveSpace to make the passthrough hands visible or hidden."}],"type":"paragraph"},{"syntax":"swift","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer (configuration: MyConfiguration ()) { layerRenderer in","\t\t\t\tlet engine = my_engine_create (layerRenderer)","\t\t\t\tlet renderThread = Thread {","\t\t\t\t\tmy_engine_render_loop(engine)","\t\t\t\t}","\t\t\t\trenderThread.name = \"Render Thread\"","\t\t\t\trenderThread.start()","\t\t\t}\t","\t\t}","\t\t.upperLimbVisibility(.hidden)","\t} ","}"],"type":"codeListing"},{"inlineContent":[{"text":"To access the spatial events, go back to where we defined the CompositorLayer render handler. Here, register a block in the layerRenderer to get updates every time there is a new spatial event.","type":"text"}],"type":"paragraph"},{"syntax":"swift","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer (configuration: MyConfiguration ()) { layerRenderer in","\t\t\t\tlet engine = my_engine_create (layerRenderer)","\t\t\t\tlet renderThread = Thread {","\t\t\t\t\tmy_engine_render_loop(engine)","\t\t\t\t}","\t\t\t\trenderThread.name = \"Render Thread\"","\t\t\t\trenderThread.start()","\t\t\t\t","\t\t\t\t\/\/ add this","\t\t\t\tlayerRenderer.onSpatialEvent = { eventCollection in","\t\t\t\t\tvar events = eventCollection.map { my_spatial_event ($0) }","\t\t\t\t\tmy_engine_push_spatial_events(engine, &events, events.count)","\t\t\t\t}","","\t\t\t}\t","\t\t}","\t\t.upperLimbVisibility(.hidden)","\t} ","}"],"type":"codeListing"},{"inlineContent":[{"text":"If we are writing our engine code in C, we’ll map the SwiftUI spatial event to a C type. Inside the C code, we can now receive the C event collection. One thing to keep in mind when handling the spatial event updates is that the updates get delivered in the main thread. This means that we will use some synchronization mechanism when reading and writing the events in our engine.","type":"text"}],"type":"paragraph"},{"syntax":"c","code":["void my_engine_push_spatial_events(my_engine *engine,","\t\t\t\t\t\t\t\tmy_spatial_event *spatial_event_collection, size_t event_count) {","\tos_unfair_lock_lock(&engine-›input_event_lock);","","\t\/\/ Copy events into an internal queue","\t","\tos_unfair_lock_unlock(&engine->input_event_lock);","}"],"type":"codeListing"},{"inlineContent":[{"text":"Now that the events are stored in the engine, it’s time to implement the gather input function. The first step is to create an object to store the current input state for this frame.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"This input state will store the events that we received from the LayerRenderer. Make sure that we are accessing our internal storage in a safe way.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"As for the hand skeleton, we can use the hand tracking provider API from ARKit to get the latest hand anchors."}],"type":"paragraph"},{"syntax":"c","code":["my_input_state my_engine_gather_inputs (my_engine *engine,cp_frame_timing_t timing) {","\t","\tmy_input_state input_state = my_input_state_create();","\t","\tos_unfair_lock_lock(&engine->input_event_lock);","\tinput_state.current_pinch_collection = my_engine_pop_spatial_events(engine);","\tos_unfair_lock_unlock(&engine->input_event_lock);","\t","\tar_hand_tracking_provider_get_latest_anchors(engine->hand_tracking_provider, input_state.left_hand, input_state.right_hand);","\t","\treturn input_state;","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"And now that our application has input support, we have all the tools at our disposal to create fully immersive experiences on xrOS."}],"type":"paragraph"},{"anchor":"Wrap-up","text":"Wrap up","level":2,"type":"heading"},{"inlineContent":[{"text":"To recap, with SwiftUI, we will define the application.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"With CompositorServices and Metal, we will set up the render loop and display 3D content."}],"type":"paragraph"},{"inlineContent":[{"text":"And finally, with ARKit, we will be able to make our experience interactive.","type":"text"}],"type":"paragraph"},{"anchor":"Resources","text":"Resources","level":2,"type":"heading"},{"inlineContent":[{"identifier":"https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing_fully_immersive_content_using_metal","isActive":true,"type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"identifier":"https:\/\/developer.apple.com\/forums\/create\/question?tag1=795030&tag2=763030&tag3=164","isActive":true,"type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"identifier":"https:\/\/developer.apple.com\/documentation\/metal","isActive":true,"type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10089","isActive":true,"type":"reference"}],"type":"paragraph"},{"anchor":"Related-Videos","text":"Related Videos","level":2,"type":"heading"},{"inlineContent":[{"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10096","isActive":true},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111","overridingTitleInlineContent":[{"text":"Go beyond the window with SwiftUI - WWDC23","type":"text"}],"isActive":true,"overridingTitle":"Go beyond the window with SwiftUI - WWDC23"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082","overridingTitleInlineContent":[{"text":"Meet ARKit for spatial computing - WWDC23","type":"text"}],"isActive":true,"overridingTitle":"Meet ARKit for spatial computing - WWDC23"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161","overridingTitleInlineContent":[{"type":"text","text":"Explore HDR rendering with EDR - WWDC21"}],"isActive":true,"overridingTitle":"Explore HDR rendering with EDR - WWDC21"}],"type":"paragraph"},{"anchor":"Written-By","text":"Written By","level":2,"type":"heading"},{"columns":[{"size":1,"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4"}]}]},{"size":4,"content":[{"anchor":"laurent-b","level":3,"type":"heading","text":"laurent b"},{"inlineContent":[{"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"overridingTitle":"Contributed Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes","type":"reference","isActive":true},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/github.com\/multitudes","type":"reference","isActive":true},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/x.com\/wrmultitudes","type":"reference","isActive":true},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/laurentbrusa.hashnode.dev\/","type":"reference","isActive":true}],"type":"paragraph"}]}],"type":"row","numberOfColumns":5},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"isActive":true,"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"reference"}],"type":"paragraph"},{"anchor":"Related-Sessions","text":"Related Sessions","level":2,"type":"heading"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10096-Build-great-games-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10111-Go-beyond-the-window-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10161-Explore-HDR-rendering-with-EDR"],"type":"links","style":"list"},{"inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}],"type":"small"},{"inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}]}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10089-discover-metal-for-immersive-apps"]}],"sections":[],"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc23\/10089","overridingTitle":"Watch Video (20 min)"}},"schemaVersion":{"major":0,"minor":3,"patch":0},"metadata":{"roleHeading":"WWDC23","modules":[{"name":"WWDC Notes"}],"title":"Discover Metal for immersive apps","role":"sampleCode"},"kind":"article","abstract":[{"type":"text","text":"Find out how you can use Metal to render fully immersive experiences for visionOS. We’ll show you how to set up a rendering session on the platform and create a basic render loop, and share how you can make your experience interactive by incorporating spatial input."}],"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10273-Work-with-Reality-Composer-Pro-content-in-Xcode":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10273-work-with-reality-composer-pro-content-in-xcode","abstract":[{"type":"text","text":"Learn how to bring content from Reality Composer Pro to life in Xcode. We’ll show you how to load 3D scenes into Xcode, integrate your content with your code, and add interactivity to your app. We’ll also share best practices and tips for using these tools together in your development workflow."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10273-Work-with-Reality-Composer-Pro-content-in-Xcode","role":"sampleCode","title":"Work with Reality Composer Pro content in Xcode","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10045-Detect-animal-poses-in-Vision":{"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10045-detect-animal-poses-in-vision","abstract":[{"text":"Go beyond detecting cats and dogs in images. We’ll show you how to use Vision to detect the individual joints and poses of these animals as well — all in real time — and share how you can enable exciting features like animal tracking for a camera app, creative embellishment on an animal photo, and more. We’ll also explore other important enhancements to Vision and share best practices.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","title":"Detect animal poses in Vision","kind":"article"},"https://x.com/wrmultitudes":{"identifier":"https:\/\/x.com\/wrmultitudes","url":"https:\/\/x.com\/wrmultitudes","title":"X\/Twitter","type":"link","titleInlineContent":[{"text":"X\/Twitter","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10102-Spotlight-your-app-with-App-Shortcuts":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how to use App Shortcuts to surface frequently used features from your app in Spotlight or through Siri. Find out how to configure search results for your app and learn best practices for creating great App Shortcuts. We’ll also show you how to build great visual and voice experiences and extend to other Apple devices like Apple Watch and HomePod."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10102-Spotlight-your-app-with-App-Shortcuts","role":"sampleCode","title":"Spotlight your app with App Shortcuts","url":"\/documentation\/wwdcnotes\/wwdc23-10102-spotlight-your-app-with-app-shortcuts"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10115-Design-with-SwiftUI":{"title":"Design with SwiftUI","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10115-Design-with-SwiftUI","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10115-design-with-swiftui","kind":"article","abstract":[{"text":"Discover how SwiftUI can help you quickly iterate and explore design ideas. Learn from Apple designers as they share how working with SwiftUI influenced the design of the Maps app in watchOS 10 and other elements of their work, and find out how you can incorporate these workflows in your own process.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10085-Discover-Quick-Look-for-spatial-computing":{"type":"topic","abstract":[{"text":"Learn how to use Quick Look on visionOS to add powerful previews for 3D content, spatial images and videos, and much more. We’ll show you the different ways that the system presents these experiences, demonstrate how someone can drag and drop Quick Look content from an app or website to create a separate window with that content, and explore how you can present Quick Look directly within an app.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10085-Discover-Quick-Look-for-spatial-computing","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10085-discover-quick-look-for-spatial-computing","title":"Discover Quick Look for spatial computing","kind":"article"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=358":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=358","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=358","title":"5:58 - Render configuration","type":"link","titleInlineContent":[{"text":"5:58 - Render configuration","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10162-The-SwiftUI-cookbook-for-focus":{"title":"The SwiftUI cookbook for focus","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10162-the-swiftui-cookbook-for-focus","role":"sampleCode","abstract":[{"text":"The SwiftUI team is back in the coding “kitchen” with powerful tools to shape your app’s focus experience. Join us and learn about the staple ingredients that support focus-driven interactions in your app. Discover focus interactions for custom views, find out about key-press handlers for keyboard input, and learn how to support movement and hierarchy with focus sections. We’ll also go through some tasty recipes for common focus patterns in your app.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10162-The-SwiftUI-cookbook-for-focus"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10101-Customize-ondevice-speech-recognition":{"kind":"article","abstract":[{"text":"Find out how you can improve on-device speech recognition in your app by customizing the underlying model with additional vocabulary. We’ll share how speech recognition works on device and show you how to boost specific words and phrases for more predictable transcription. Learn how you can provide specific pronunciations for words and use template support to quickly generate a full set of custom phrases — all at runtime.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10101-customize-ondevice-speech-recognition","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10101-Customize-ondevice-speech-recognition","title":"Customize on-device speech recognition","role":"sampleCode"},"WWDC23-10089-metal4":{"identifier":"WWDC23-10089-metal4","variants":[{"url":"\/images\/WWDC23-10089-metal4.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10226-Debug-with-structured-logging":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover the debug console in Xcode 15 and learn how you can improve your diagnostic experience through logging. Explore how you can navigate your logs easily and efficiently using advanced filtering and improved visualization. We’ll also show you how to use the dwim-print command to evaluate expressions in your code while debugging."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10226-Debug-with-structured-logging","role":"sampleCode","title":"Debug with structured logging","url":"\/documentation\/wwdcnotes\/wwdc23-10226-debug-with-structured-logging"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10096-Build-great-games-for-spatial-computing":{"abstract":[{"type":"text","text":"Find out how you can develop great gaming experiences for visionOS. We’ll share some of the key building blocks that help you create games for this platform, explore how your experiences can fluidly move between levels of immersion, and provide a roadmap for exploring ARKit, RealityKit, Reality Composer Pro, Unity, Metal, and Compositor."}],"title":"Build great games for spatial computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10096-Build-great-games-for-spatial-computing","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10096-build-great-games-for-spatial-computing","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10150-Optimize-CarPlay-for-vehicle-systems":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how you can integrate CarPlay into modern vehicle systems. We’ll show you how to adjust CarPlay for any high-resolution display — regardless of configuration or size. Learn how you can use CarPlay-supplied metadata and video streams to show information on additional displays, and find out how advances in wireless connectivity, audio, and video encoding can help prepare your vehicle system for the next generation of CarPlay."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10150-Optimize-CarPlay-for-vehicle-systems","role":"sampleCode","title":"Optimize CarPlay for vehicle systems","url":"\/documentation\/wwdcnotes\/wwdc23-10150-optimize-carplay-for-vehicle-systems"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10107-Embed-the-Photos-Picker-in-your-app":{"abstract":[{"type":"text","text":"Discover how you can simply, safely, and securely access the Photos Library in your app. Learn how to get started with the embedded picker and explore the options menu and HDR still image support. We’ll also show you how to take advantage of UI customization options to help the picker blend into your existing interface."}],"kind":"article","title":"Embed the Photos Picker in your app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10107-Embed-the-Photos-Picker-in-your-app","type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10107-embed-the-photos-picker-in-your-app"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10028-Bring-widgets-to-life":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Learn how to make animated and interactive widgets for your apps and games. We’ll show you how to tweak animations for entry transitions and add interactivity using SwiftUI Button and Toggle so that you can create powerful moments right from the Home Screen and Lock Screen."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10028-Bring-widgets-to-life","role":"sampleCode","title":"Bring widgets to life","url":"\/documentation\/wwdcnotes\/wwdc23-10028-bring-widgets-to-life"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10031-Update-your-app-for-watchOS-10":{"abstract":[{"type":"text","text":"Join us as we update an Apple Watch app to take advantage of the latest features in watchOS 10. In this code-along, we’ll show you how to use the latest SwiftUI APIs to maximize glanceability and reorient app navigation around the Digital Crown."}],"title":"Update your app for watchOS 10","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10031-Update-your-app-for-watchOS-10","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10031-update-your-app-for-watchos-10","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10254-Do-more-with-Managed-Apple-IDs":{"url":"\/documentation\/wwdcnotes\/wwdc23-10254-do-more-with-managed-apple-ids","kind":"article","role":"sampleCode","type":"topic","abstract":[{"type":"text","text":"Explore the latest updates to Managed Apple IDs and learn how you can use them in your organization. Take advantage of additional apps and services available to Managed Apple IDs, discover the Account-Driven Device Enrollment flow, and find out how to use access management controls to limit the devices and Apple services that Managed Apple IDs can access. We’ll also show you how to federate with your identity provider to automate creation and sync with your directory."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10254-Do-more-with-Managed-Apple-IDs","title":"Do more with Managed Apple IDs"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=1222":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1222","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1222","title":"20:22 - Wrap-Up","type":"link","titleInlineContent":[{"text":"20:22 - Wrap-Up","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10087-Build-spatial-SharePlay-experiences":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10087-build-spatial-shareplay-experiences","kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10087-Build-spatial-SharePlay-experiences","title":"Build spatial SharePlay experiences","abstract":[{"type":"text","text":"Discover how you can use the GroupActivities framework to build unique sharing and collaboration experiences for visionOS. We’ll introduce you to SharePlay on this platform, learn how to create experiences that make people feel present as if they were in the same space, and explore how immersive apps can respect shared context between participants."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10241-Share-files-with-SharePlay":{"kind":"article","abstract":[{"text":"Discover how to work with files and attachments in a SharePlay activity. We’ll explain how to use the GroupSessionJournal API to sync large amounts of data faster and show you how to adopt it in a demo of the sample app DrawTogether.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10241-share-files-with-shareplay","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10241-Share-files-with-SharePlay","title":"Share files with SharePlay","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10004-Reduce-network-delays-with-L4S":{"url":"\/documentation\/wwdcnotes\/wwdc23-10004-reduce-network-delays-with-l4s","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10004-Reduce-network-delays-with-L4S","role":"sampleCode","kind":"article","title":"Reduce network delays with L4S","abstract":[{"text":"Streaming video, multiplayer games, and other real-time experiences depend on responsive, low latency networking. Learn how Low Latency, Low Loss, Scalable throughput (L4S) can reduce network delays and improve the overall experience in your app. We’ll show you how to set up and test your app, network, and server with L4S.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10238-Tune-up-your-AirPlay-audio-experience":{"url":"\/documentation\/wwdcnotes\/wwdc23-10238-tune-up-your-airplay-audio-experience","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10238-Tune-up-your-AirPlay-audio-experience","role":"sampleCode","kind":"article","title":"Tune up your AirPlay audio experience","abstract":[{"text":"Learn how you can upgrade your app’s AirPlay audio experience to be more robust and responsive. We’ll show you how to adopt enhanced audio buffering with AVQueuePlayer, explore alternatives when building a custom player in your app, and share best practices.","type":"text"}]},"WWDC23-10089-metal5":{"identifier":"WWDC23-10089-metal5","variants":[{"url":"\/images\/WWDC23-10089-metal5.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","type":"link","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}]},"WWDC23-10089-metal15":{"identifier":"WWDC23-10089-metal15","variants":[{"url":"\/images\/WWDC23-10089-metal15.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10167-Expand-on-Swift-macros":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10167-expand-on-swift-macros","kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10167-Expand-on-Swift-macros","title":"Expand on Swift macros","abstract":[{"type":"text","text":"Discover how Swift macros can help you reduce boilerplate in your codebase and adopt complex features more easily. Learn how macros can analyze code, emit rich compiler errors to guide developers towards correct usage, and generate new code that is automatically incorporated back into your project. We’ll also take you through important concepts like macro roles, compiler plugins, and syntax trees."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10203-Develop-your-first-immersive-app":{"title":"Develop your first immersive app","url":"\/documentation\/wwdcnotes\/wwdc23-10203-develop-your-first-immersive-app","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10203-Develop-your-first-immersive-app","role":"sampleCode","abstract":[{"text":"Find out how you can build immersive apps for visionOS using Xcode and Reality Composer Pro. We’ll show you how to get started with a new visionOS project, use Xcode Previews for your SwiftUI development, and take advantage of RealityKit and RealityView to render 3D content.","type":"text"}]},"https://developer.apple.com/documentation/metal":{"identifier":"https:\/\/developer.apple.com\/documentation\/metal","url":"https:\/\/developer.apple.com\/documentation\/metal","title":"Metal","type":"link","titleInlineContent":[{"text":"Metal","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=689":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=689","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=689","title":"11:29 - Render Loop","type":"link","titleInlineContent":[{"text":"11:29 - Render Loop","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2023/10096":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10096","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10096","title":"Build great games for spatial computing - WWDC23","type":"link","titleInlineContent":[{"text":"Build great games for spatial computing - WWDC23","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10093-Bring-your-Unity-VR-app-to-a-fully-immersive-space":{"url":"\/documentation\/wwdcnotes\/wwdc23-10093-bring-your-unity-vr-app-to-a-fully-immersive-space","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10093-Bring-your-Unity-VR-app-to-a-fully-immersive-space","role":"sampleCode","kind":"article","title":"Bring your Unity VR app to a fully immersive space","abstract":[{"text":"Discover how you can bring your existing Unity VR apps and games to visionOS. We’ll explore workflows that can help you get started and show you how to build for eyes and hands in your apps and games with the Unity Input System. Learn about Unity’s XR Interaction Toolkit, tips for foveated rendering, and best practices.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10193-Design-Shortcuts-for-Spotlight":{"url":"\/documentation\/wwdcnotes\/wwdc23-10193-design-shortcuts-for-spotlight","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10193-Design-Shortcuts-for-Spotlight","role":"sampleCode","kind":"article","title":"Design Shortcuts for Spotlight","abstract":[{"text":"Learn about the latest updates to the visual language of App Shortcuts and find out how to design your shortcut to appear as a top hit in Spotlight. We’ll share how shortcuts can appear on iOS or iPadOS, and show you how to customize the visual appearance of a shortcut, personalize its order, select its correct behavior, and increase discoverability.","type":"text"}]},"https://developer.apple.com/forums/tags/wwdc2023-10089":{"identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10089","url":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10089","title":"Search the forums for tag wwdc2023-10089","type":"link","titleInlineContent":[{"text":"Search the forums for tag wwdc2023-10089","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10275-Explore-AirPlay-with-interstitials":{"role":"sampleCode","type":"topic","abstract":[{"text":"Learn how you can use HLS Interstitials with AirPlay to create seamless transitions for your video content between advertisements. We’ll share best practices and tips for creating a great experience when sharing content from Apple devices to popular smart TVs.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10275-explore-airplay-with-interstitials","title":"Explore AirPlay with interstitials","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10275-Explore-AirPlay-with-interstitials"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10052-Discover-Calendar-and-EventKit":{"role":"sampleCode","type":"topic","title":"Discover Calendar and EventKit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10052-Discover-Calendar-and-EventKit","url":"\/documentation\/wwdcnotes\/wwdc23-10052-discover-calendar-and-eventkit","abstract":[{"type":"text","text":"Discover how you can bring Calendar into your app and help people better manage their time. Find out how to create new events from your app, fetch events, and implement a virtual conference extension. We’ll also take you through some of the changes to calendar access levels that help your app stay connected without compromising the privacy of someone’s calendar data."}],"kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10160-Demystify-SwiftUI-performance":{"role":"sampleCode","abstract":[{"text":"Learn how you can build a mental model for performance in SwiftUI and write faster, more efficient code. We’ll share some of the common causes behind performance issues and help you triage hangs and hitches in SwiftUI to create more responsive views in your app.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10160-Demystify-SwiftUI-performance","kind":"article","title":"Demystify SwiftUI performance","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10160-demystify-swiftui-performance"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10007-Create-seamless-experiences-with-Virtualization":{"title":"Create seamless experiences with Virtualization","url":"\/documentation\/wwdcnotes\/wwdc23-10007-create-seamless-experiences-with-virtualization","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10007-Create-seamless-experiences-with-Virtualization","role":"sampleCode","abstract":[{"type":"text","text":"Discover the latest updates to the Virtualization framework. We’ll show you how to configure a virtual machine (VM) to automatically resize its display, take you through saving and restoring a running VM, and explore storage and performance options for Virtualization apps running on the desktop or in the data center."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10090-Run-your-iPad-and-iPhone-apps-in-the-Shared-Space":{"role":"sampleCode","type":"topic","title":"Run your iPad and iPhone apps in the Shared Space","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10090-Run-your-iPad-and-iPhone-apps-in-the-Shared-Space","url":"\/documentation\/wwdcnotes\/wwdc23-10090-run-your-ipad-and-iphone-apps-in-the-shared-space","abstract":[{"type":"text","text":"Discover how you can run your existing iPad and iPhone apps on Vision Pro. Learn how iPadOS and iOS apps operate on this platform, find out about the Designed for iPad experience, and explore the paths available for enhancing your app experience on visionOS."}],"kind":"article"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=6":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=6","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=6","title":"0:06 - Intro","type":"link","titleInlineContent":[{"text":"0:06 - Intro","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2023/10111":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111","title":"Go beyond the window with SwiftUI - WWDC23","type":"link","titleInlineContent":[{"text":"Go beyond the window with SwiftUI - WWDC23","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10175-Fix-failures-faster-with-Xcode-test-reports":{"url":"\/documentation\/wwdcnotes\/wwdc23-10175-fix-failures-faster-with-xcode-test-reports","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10175-Fix-failures-faster-with-Xcode-test-reports","role":"sampleCode","kind":"article","title":"Fix failures faster with Xcode test reports","abstract":[{"text":"Discover how you can find, debug, and fix test failures faster with the test report in Xcode and Xcode Cloud. Learn how Xcode identifies failure patterns to help you find the right place to start investigating. We’ll also show you how to use the UI automation explorer and video recordings to understand the events that led up to your UI test failure.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14, tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}],"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"url":"\/documentation\/wwdcnotes\/wwdc23","type":"topic","role":"collectionGroup","title":"WWDC23","kind":"article"},"https://developer.apple.com/wwdc23/10089":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/10089","checksum":null,"url":"https:\/\/developer.apple.com\/wwdc23\/10089","type":"download"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10037-Explore-pie-charts-and-interactivity-in-Swift-Charts":{"url":"\/documentation\/wwdcnotes\/wwdc23-10037-explore-pie-charts-and-interactivity-in-swift-charts","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10037-Explore-pie-charts-and-interactivity-in-Swift-Charts","role":"sampleCode","kind":"article","title":"Explore pie charts and interactivity in Swift Charts","abstract":[{"text":"Swift Charts has come full circle: Get ready to bake up pie and donut charts in your app with the latest improvements to the framework. Learn how to make your charts scrollable, explore the chart selection API for revealing additional details in your data, and find out how enabling additional interactivity can make your charts even more delightful.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML":{"abstract":[{"type":"text","text":"Find out how Create ML can help you do even more with machine learning models. Learn about the latest updates to image understanding and text-based tasks with multilingual BERT embeddings. Discover how easy it is to train models that can understand the content of images using multi-label classification. We’ll also share information about interactive model evaluation and the latest APIs for custom training data augmentations."}],"kind":"article","title":"Discover machine learning enhancements in Create ML","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML","type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10044-discover-machine-learning-enhancements-in-create-ml"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10281-Keep-up-with-the-keyboard":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10281-keep-up-with-the-keyboard","abstract":[{"text":"Each year, the keyboard evolves to support an increasing range of languages, sizes, and features. Discover how you can design your app to keep up with the keyboard, regardless of how it appears on a device. We’ll show you how to create frictionless text entry and share important architectural changes to help you understand how the keyboard works within the system.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10281-Keep-up-with-the-keyboard","role":"sampleCode","title":"Keep up with the keyboard","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10035-Perform-accessibility-audits-for-your-app":{"title":"Perform accessibility audits for your app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10035-Perform-accessibility-audits-for-your-app","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10035-perform-accessibility-audits-for-your-app","kind":"article","abstract":[{"text":"Discover how you can test your app for accessibility with every build. Learn how to perform automated audits for accessibility using XCTest and find out how to interpret the results. We’ll also share enhancements to the accessibility API that can help you improve UI test coverage.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression":{"abstract":[{"type":"text","text":"Discover how to reduce the footprint of machine learning models in your app with Core ML Tools. Learn how to use techniques like palettization, pruning, and quantization to dramatically reduce model size while still achieving great accuracy. Explore comparisons between compression during the training stages and on fully trained models, and learn how compressed models can run even faster when your app takes full advantage of the Apple Neural Engine."}],"title":"Use Core ML Tools for machine learning model compression","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10047-use-core-ml-tools-for-machine-learning-model-compression","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10122-Explore-media-formats-for-the-web":{"role":"sampleCode","type":"topic","title":"Explore media formats for the web","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10122-Explore-media-formats-for-the-web","url":"\/documentation\/wwdcnotes\/wwdc23-10122-explore-media-formats-for-the-web","abstract":[{"type":"text","text":"Learn about the latest image formats and video technologies supported in Safari 17. Discover how you can use JPEG XL, AVIF, and HEIC in your websites and experiences and learn how they differ from previous formats. We’ll also show you how the Managed Media Source API draws less power than Media Source Extensions (MSE) and explore how you can use it to more efficiently manage streaming video over 5G."}],"kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10071-Deliver-video-content-for-spatial-experiences":{"title":"Deliver video content for spatial experiences","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10071-Deliver-video-content-for-spatial-experiences","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10071-deliver-video-content-for-spatial-experiences","kind":"article","abstract":[{"text":"Learn how to prepare and deliver video content for visionOS using HTTP Live Streaming (HLS). Discover the current HLS delivery process for media and explore how you can expand your delivery pipeline to support 3D content. Get up to speed with tips and techniques for spatial media streaming and adapting your existing caption production workflows for 3D. And find out how to share audio tracks across video variants and add spatial audio to make your video content more immersive.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10180-Discover-streamlined-location-updates":{"title":"Discover streamlined location updates","url":"\/documentation\/wwdcnotes\/wwdc23-10180-discover-streamlined-location-updates","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10180-Discover-streamlined-location-updates","role":"sampleCode","abstract":[{"text":"Move into the future with Core Location! Meet the CLLocationUpdate class, designed for modern Swift concurrency, and learn how it simplifies getting location updates. We’ll show you how this class works with your apps when they run in the foreground or background and share some best practices.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10058-Whats-new-with-text-and-text-interactions":{"abstract":[{"text":"Text is an absolutely critical component of every app. Discover the latest features and enhancements for creating rich text experiences on Apple platforms. We’ll show you how to take advantage of common text elements and create entirely custom interactions for your app. Learn about updates to dictation, text loupe, and text selection, and explore improvements to text clipping, line wrapping, and hyphenation.","type":"text"}],"title":"What’s new with text and text interactions","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10058-Whats-new-with-text-and-text-interactions","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10058-whats-new-with-text-and-text-interactions","type":"topic"},"WWDC23-10089-metal10":{"identifier":"WWDC23-10089-metal10","variants":[{"url":"\/images\/WWDC23-10089-metal10.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10258-Animate-symbols-in-your-app":{"type":"topic","abstract":[{"text":"Bring delight to your app with animated symbols. Explore the new Symbols framework, which features a unified API to create and configure symbol effects. Learn how SwiftUI, AppKit, and UIKit make it easy to animate symbols in user interfaces. Discover tips and tricks to seamlessly integrate the new animations alongside other app content. To get the most from this session, we recommend first watching “What’s new in SF Symbols 5.”","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10258-Animate-symbols-in-your-app","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10258-animate-symbols-in-your-app","title":"Animate symbols in your app","kind":"article"},"https://developer.apple.com/documentation/compositorservices/drawing_fully_immersive_content_using_metal":{"identifier":"https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing_fully_immersive_content_using_metal","url":"https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing_fully_immersive_content_using_metal","title":"Drawing fully immersive content using Metal","type":"link","titleInlineContent":[{"text":"Drawing fully immersive content using Metal","type":"text"}]},"WWDC23-10089-metal1":{"identifier":"WWDC23-10089-metal1","variants":[{"url":"\/images\/WWDC23-10089-metal1.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10006-Build-robust-and-resumable-file-transfers":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10006-build-robust-and-resumable-file-transfers","abstract":[{"type":"text","text":"Find out how URLSession can help your apps transfer large files and recover from network interruptions. Learn how to pause and resume HTTP file transfers and support resumable uploads, and explore best practices for using URLSession to transfer files even when your app is suspended in the background."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10006-Build-robust-and-resumable-file-transfers","role":"sampleCode","title":"Build robust and resumable file transfers","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10082-Meet-ARKit-for-spatial-computing":{"title":"Meet ARKit for spatial computing","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10082-meet-arkit-for-spatial-computing","type":"topic","abstract":[{"type":"text","text":"Discover how you can use ARKit’s tracking and scene understanding features to develop a whole new universe of immersive apps and games. Learn how visionOS and ARKit work together to help you create apps that understand a person’s surroundings — all while preserving privacy. Explore the latest updates to the ARKit API and follow along as we demonstrate how to take advantage of hand tracking and scene geometry in your apps."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10257-Create-animated-symbols":{"type":"topic","title":"Create animated symbols","abstract":[{"type":"text","text":"Discover animation presets and learn how to use them with SF Symbols and custom symbols. We’ll show you how to experiment with different options and configurations to find the perfect animation for your app. Learn how to update custom symbols for animation using annotation features, find out how to modify your custom symbols with symbol components, and explore the redesigned export process to help keep symbols looking great on all platforms."}],"url":"\/documentation\/wwdcnotes\/wwdc23-10257-create-animated-symbols","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10257-Create-animated-symbols","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods":{"title":"Enhance your app’s audio experience with AirPods","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10233-enhance-your-apps-audio-experience-with-airpods","role":"sampleCode","abstract":[{"text":"Discover how you can create transformative audio experiences in your app using AirPods. Learn how to incorporate AirPods Automatic Switching, use AVAudioApplication to support Mute Control, and take advantage of Spatial Audio to create immersive soundscapes in your app or game.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods"},"WWDC23-10089-metal14":{"identifier":"WWDC23-10089-metal14","variants":[{"url":"\/images\/WWDC23-10089-metal14.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10172-Mix-Swift-and-C++":{"title":"Mix Swift and C++","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10172-Mix-Swift-and-C++","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10172-mix-swift-and-c++","kind":"article","abstract":[{"text":"Learn how you can use Swift in your C++ and Objective-C++ projects to make your code safer, faster, and easier to develop. We’ll show you how to use C++ and Swift APIs to incrementally incorporate Swift into your app.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10057-Unleash-the-UIKit-trait-system":{"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10057-unleash-the-uikit-trait-system","abstract":[{"text":"Discover powerful enhancements to the trait system in UIKit. Learn how you can define custom traits to add your own data to UITraitCollection, modify the data propagated to view controllers and views with trait override APIs, and adopt APIs to improve flexibility and performance. We’ll also show you how to bridge UIKit traits with SwiftUI environment keys to seamlessly access data from both UIKit and SwiftUI components in your app.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10057-Unleash-the-UIKit-trait-system","title":"Unleash the UIKit trait system","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10189-Migrate-to-SwiftData":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how you can start using SwiftData in your apps. We’ll show you how to use Xcode to generate model classes from your existing Core Data object models, use SwiftData alongside your previous implementation, or even completely replace your existing solution."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10189-Migrate-to-SwiftData","role":"sampleCode","title":"Migrate to SwiftData","url":"\/documentation\/wwdcnotes\/wwdc23-10189-migrate-to-swiftdata"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10128-Your-guide-to-Metal-ray-tracing":{"type":"topic","title":"Your guide to Metal ray tracing","abstract":[{"type":"text","text":"Discover how you can enhance the visual quality of your games and apps with Metal ray tracing. We’ll take you through the fundamentals of the Metal ray tracing API. Explore the latest enhancements and techniques that will enable you to create larger and more complex scenes, reduce memory usage and build times, and efficiently render visual content like hair and fur."}],"url":"\/documentation\/wwdcnotes\/wwdc23-10128-your-guide-to-metal-ray-tracing","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10128-Your-guide-to-Metal-ray-tracing","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10075-Design-spatial-SharePlay-experiences":{"title":"Design spatial SharePlay experiences","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10075-Design-spatial-SharePlay-experiences","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10075-design-spatial-shareplay-experiences","kind":"article","abstract":[{"text":"Explore the types of shared activities you can create in your visionOS apps and find out how your apps can use Spatial Persona templates to support meaningful interactions between people. Discover how to design your UI around a shared context, handle immersive content in a shared activity, and more.","type":"text"}]},"WWDC23-10089-recRoom":{"identifier":"WWDC23-10089-recRoom","variants":[{"url":"\/images\/WWDC23-10089-recRoom.jpg","traits":["1x","light"]}],"alt":"RecRoom","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10188-Sync-to-iCloud-with-CKSyncEngine":{"abstract":[{"type":"text","text":"Discover how CKSyncEngine can help you sync people’s CloudKit data to iCloud. Learn how you can reduce the amount of code in your app when you let the system handle scheduling for your sync operations. We’ll share how you can automatically benefit from enhanced performance as CloudKit evolves, explore testing for your sync implementation, and more."}],"title":"Sync to iCloud with CKSyncEngine","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10188-Sync-to-iCloud-with-CKSyncEngine","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10188-sync-to-icloud-with-cksyncengine","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10060-Get-started-with-privacy-manifests":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10060-get-started-with-privacy-manifests","abstract":[{"type":"text","text":"Meet privacy manifests: a new tool that helps you accurately identify the privacy practices of your app’s dependencies. Find out how third-party SDK developers can use these manifests to share privacy practices for their frameworks. We’ll also share how Xcode can produce a full privacy report to help you more easily represent the privacy practices of all the code in your app."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10060-Get-started-with-privacy-manifests","role":"sampleCode","title":"Get started with privacy manifests","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10158-Animate-with-springs":{"title":"Animate with springs","url":"\/documentation\/wwdcnotes\/wwdc23-10158-animate-with-springs","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10158-Animate-with-springs","role":"sampleCode","abstract":[{"text":"Discover how you can bring life to your app with animation! We’ll show you how to create amazing animations when you take advantage of springs and help you learn how to use them in your app.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10016-Build-custom-workouts-with-WorkoutKit":{"url":"\/documentation\/wwdcnotes\/wwdc23-10016-build-custom-workouts-with-workoutkit","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10016-Build-custom-workouts-with-WorkoutKit","role":"sampleCode","kind":"article","title":"Build custom workouts with WorkoutKit","abstract":[{"text":"WorkoutKit makes it easy to create, preview, and schedule planned workouts for the Workout app on Apple Watch. Learn how to build custom intervals, create alerts, and use the built-in preview UI to send your own workout routines to Apple Watch.","type":"text"}]},"WWDC23-10089-metal2":{"identifier":"WWDC23-10089-metal2","variants":[{"url":"\/images\/WWDC23-10089-metal2.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10176-Lift-subjects-from-images-in-your-app":{"abstract":[{"text":"Discover how you can easily pull the subject of an image from its background in your apps. Learn how to lift the primary subject or to access the subject at a given point with VisionKit. We’ll also share how you can lift subjects using Vision and combine that with lower-level frameworks like Core Image to create fun image effects and more complex compositing pipelines.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10176-lift-subjects-from-images-in-your-app","title":"Lift subjects from images in your app","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","kind":"article","role":"sampleCode"},"WWDC23-10089-metal13":{"identifier":"WWDC23-10089-metal13","variants":[{"url":"\/images\/WWDC23-10089-metal13.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10256-Discover-Continuity-Camera-for-tvOS":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how you can bring AVFoundation, AVFAudio, and AudioToolbox to your apps on tvOS and create camera and microphone experiences for the living room. Find out how to support tvOS in your existing iOS camera experience with the Device Discovery API, build apps that use iPhone as a webcam or FaceTime source, and explore special considerations when developing for tvOS. We’ll also show you how to enable audio recording for tvOS, and how to use echo cancellation to create great voice-driven experiences."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10256-Discover-Continuity-Camera-for-tvOS","role":"sampleCode","title":"Discover Continuity Camera for tvOS","url":"\/documentation\/wwdcnotes\/wwdc23-10256-discover-continuity-camera-for-tvos"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10002-Ready-set-relay-Protect-app-traffic-with-network-relays":{"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10002-ready-set-relay-protect-app-traffic-with-network-relays","abstract":[{"text":"Learn how relays can make your app’s network traffic more private and secure without the overhead of a VPN. We’ll show you how to integrate relay servers in your own app and explore how enterprise networks can use relays to securely access internal resources.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10002-Ready-set-relay-Protect-app-traffic-with-network-relays","title":"Ready, set, relay: Protect app traffic with network relays","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10248-Analyze-hangs-with-Instruments":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"User interface elements often mimic real-world interactions, including real-time responses. Apps with a noticeable delay in user interaction — a hang — can break that illusion and create frustration. We’ll show you how to use Instruments to analyze, understand, and fix hangs in your apps on all Apple platforms. Discover how you can efficiently navigate an Instruments trace document, interpret trace data, and record additional profiling data to better understand your specific hang."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10248-Analyze-hangs-with-Instruments","role":"sampleCode","title":"Analyze hangs with Instruments","url":"\/documentation\/wwdcnotes\/wwdc23-10248-analyze-hangs-with-instruments"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10050-Optimize-machine-learning-for-Metal-apps":{"role":"sampleCode","type":"topic","title":"Optimize machine learning for Metal apps","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10050-Optimize-machine-learning-for-Metal-apps","url":"\/documentation\/wwdcnotes\/wwdc23-10050-optimize-machine-learning-for-metal-apps","abstract":[{"type":"text","text":"Discover the latest enhancements to accelerated ML training in Metal. Find out about updates to PyTorch and TensorFlow, and learn about Metal acceleration for JAX. We’ll show you how MPS Graph can support faster ML inference when you use both the GPU and Apple Neural Engine, and share how the same API can rapidly integrate your Core ML and ONNX models."}],"kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10196-Dive-deeper-into-SwiftData":{"abstract":[{"text":"Learn how you can harness the power of SwiftData in your app. Find out how ModelContext and ModelContainer work together to persist your app’s data. We’ll show you how to track and make your changes manually and use SwiftData at scale with FetchDescriptor, SortDescriptor, and enumerate.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10196-Dive-deeper-into-SwiftData","url":"\/documentation\/wwdcnotes\/wwdc23-10196-dive-deeper-into-swiftdata","type":"topic","title":"Dive deeper into SwiftData","kind":"article","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10155-Discover-String-Catalogs":{"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10155-discover-string-catalogs","abstract":[{"text":"Discover how Xcode 15 makes it easy to localize your app by managing all of your strings in one place. We’ll show you how to extract, edit, export, and build strings in your project using String Catalogs. We’ll also share how you can adopt String Catalogs in existing projects at your own pace by choosing which files to migrate.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10155-Discover-String-Catalogs","title":"Discover String Catalogs","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10154-Build-an-app-with-SwiftData":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10154-build-an-app-with-swiftdata","abstract":[{"type":"text","text":"Discover how SwiftData can help you persist data in your app. Code along with us as we bring SwiftData to a multi-platform SwiftUI app. Learn how to convert existing model classes into SwiftData models, set up the environment, reflect model layer changes in UI, and build document-based applications backed by SwiftData storage."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10154-Build-an-app-with-SwiftData","role":"sampleCode","title":"Build an app with SwiftData","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10252-Build-programmatic-UI-with-Xcode-Previews":{"title":"Build programmatic UI with Xcode Previews","url":"\/documentation\/wwdcnotes\/wwdc23-10252-build-programmatic-ui-with-xcode-previews","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10252-Build-programmatic-UI-with-Xcode-Previews","role":"sampleCode","abstract":[{"type":"text","text":"Learn how you can use the #Preview macro on Xcode 15 to quickly iterate on your UI code written in SwiftUI, UIKit, or AppKit. Explore a collage of unique workflows for interacting with views right in the canvas, find out how to view multiple variations of UI simultaneously, and discover how you can travel through your widget’s timeline in seconds to test the transitions between entries. We’ll also show you how to add previews to libraries, provide sample assets, and preview your views in your physical devices to leverage their capabilities and existing data."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10229-Make-features-discoverable-with-TipKit":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10229-make-features-discoverable-with-tipkit","abstract":[{"type":"text","text":"Teach people how to use your app with TipKit! Learn how you can create effective educational moments through tips. We’ll share how you can build eligibility rules to reach the ideal audience, control tip frequency, and strategies for testing to ensure successful interactions."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10229-Make-features-discoverable-with-TipKit","role":"sampleCode","title":"Make features discoverable with TipKit","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes":{"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"title":"WWDC Notes","url":"\/documentation\/wwdcnotes","kind":"symbol","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","role":"collection","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10127-Optimize-GPU-renderers-with-Metal":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10127-optimize-gpu-renderers-with-metal","abstract":[{"type":"text","text":"Discover how to optimize your GPU renderer using the latest Metal features and best practices. We’ll show you how to use function specialization and parallel shader compilation to maintain responsive authoring workflows and the fastest rendering speeds, and help you tune your compute shaders for optimal performance."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10127-Optimize-GPU-renderers-with-Metal","role":"sampleCode","title":"Optimize GPU renderers with Metal","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10192-Explore-enhancements-to-RoomPlan":{"title":"Explore enhancements to RoomPlan","url":"\/documentation\/wwdcnotes\/wwdc23-10192-explore-enhancements-to-roomplan","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10192-Explore-enhancements-to-RoomPlan","abstract":[{"text":"Join us for an exciting update to RoomPlan as we explore MultiRoom support and enhancements to room representations. Learn how you can scan areas with more detail, capture multiple rooms, and merge individual scans into one larger structure. We’ll also share workflows and best practices when working with RoomPlan results that you want to combine into your existing 3D model library.","type":"text"}],"role":"sampleCode","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10137-Support-Cinematic-mode-videos-in-your-app":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10137-support-cinematic-mode-videos-in-your-app","abstract":[{"text":"Discover how the Cinematic Camera API helps your app work with Cinematic mode videos captured in the Camera app. We’ll share the fundamentals — including Decision layers — that make up Cinematic mode video, show you how to access and update Decisions in your app, and help you save and load those changes.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10137-Support-Cinematic-mode-videos-in-your-app","role":"sampleCode","title":"Support Cinematic mode videos in your app","kind":"article"},"WWDC23-10089-metal9":{"identifier":"WWDC23-10089-metal9","variants":[{"url":"\/images\/WWDC23-10089-metal9.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"WWDC23-10089-metal11":{"identifier":"WWDC23-10089-metal11","variants":[{"url":"\/images\/WWDC23-10089-metal11.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit":{"title":"Integrate with motorized iPhone stands using DockKit","url":"\/documentation\/wwdcnotes\/wwdc23-10304-integrate-with-motorized-iphone-stands-using-dockkit","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","role":"sampleCode","abstract":[{"text":"Discover how you can create incredible photo and video experiences in your camera app when integrating with DockKit-compatible motorized stands. We’ll show how your app can automatically track subjects in live video across a 360-degree field of view, take direct control of the stand to customize framing, directly control the motors, and provide your own inference model for tracking other objects. Finally, we’ll demonstrate how to create a sense of emotion through dynamic device animations.","type":"text"}]},"WWDCNotes.png":{"identifier":"WWDCNotes.png","variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"alt":null,"type":"image"},"https://avatars.githubusercontent.com/u/29355828?v=4":{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4","variants":[{"url":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4","traits":["1x","light"]}],"alt":"Profile image of laurent b","type":"image"},"https://github.com/multitudes":{"identifier":"https:\/\/github.com\/multitudes","url":"https:\/\/github.com\/multitudes","title":"GitHub","type":"link","titleInlineContent":[{"text":"GitHub","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10105-Create-a-more-responsive-camera-experience":{"abstract":[{"text":"Discover how AVCapture and PhotoKit can help you create more responsive and delightful apps. Learn about the camera capture process and find out how deferred photo processing can help create the best quality photo. We’ll show you how zero shutter lag uses time travel to capture the perfect action photo, dive into building a responsive capture pipeline, and share how you can adopt the Video Effects API to recognize pre-defined gestures that trigger real-time video effects.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10105-create-a-more-responsive-camera-experience","title":"Create a more responsive camera experience","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10105-Create-a-more-responsive-camera-experience","kind":"article","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10263-Deploy-passkeys-at-work":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how you can take advantage of passkeys in managed environments at work. We’ll explore how passkeys can work well in enterprise environments through Managed Apple ID support for iCloud Keychain. We’ll also share how administrators can manage passkeys for specific devices using Access Management controls in Apple Business Manager and Apple School Manager."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10263-Deploy-passkeys-at-work","role":"sampleCode","title":"Deploy passkeys at work","url":"\/documentation\/wwdcnotes\/wwdc23-10263-deploy-passkeys-at-work"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10161-Explore-HDR-rendering-with-EDR":{"title":"Explore HDR rendering with EDR","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10161-Explore-HDR-rendering-with-EDR","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc21-10161-explore-hdr-rendering-with-edr","kind":"article","abstract":[{"text":"EDR is Apple’s High Dynamic Range representation and rendering pipeline. Explore how you can render HDR content using EDR in your app and unleash the dynamic range capabilities of your HDR display including Apple’s internal displays and Pro Display XDR.","type":"text"}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro":{"url":"\/documentation\/wwdcnotes\/wwdc23-10202-explore-materials-in-reality-composer-pro","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro","role":"sampleCode","kind":"article","title":"Explore materials in Reality Composer Pro","abstract":[{"text":"Learn how Reality Composer Pro can help you alter the appearance of your 3D objects using RealityKit materials. We’ll introduce you to MaterialX and physically-based (PBR) shaders, show you how to design dynamic materials using the shader graph editor, and explore adding custom inputs to a material so that you can control it in your visionOS app.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10113-Take-SwiftUI-to-the-next-dimension":{"abstract":[{"text":"Get ready to add depth and dimension to your visionOS apps. Find out how to bring three-dimensional objects to your app using volumes, get to know the Model 3D API, and learn how to position and animate content. We’ll also show you how to use UI attachments in RealityView and support gestures in your content.","type":"text"}],"title":"Take SwiftUI to the next dimension","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10113-Take-SwiftUI-to-the-next-dimension","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10113-take-swiftui-to-the-next-dimension","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10088-Create-immersive-Unity-apps":{"url":"\/documentation\/wwdcnotes\/wwdc23-10088-create-immersive-unity-apps","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10088-Create-immersive-Unity-apps","role":"sampleCode","kind":"article","title":"Create immersive Unity apps","abstract":[{"text":"Explore how you can use Unity to create engaging and immersive experiences for visionOS. We’ll share how Unity integrates seamlessly with Apple frameworks, take you through the tools you can use to build natively for the platform, and show you how volume cameras can bring your existing scenes into visionOS windows, volumes, and spaces.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction":{"title":"Improve Core ML integration with async prediction","url":"\/documentation\/wwdcnotes\/wwdc23-10049-improve-core-ml-integration-with-async-prediction","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","role":"sampleCode","abstract":[{"type":"text","text":"Learn how to speed up machine learning features in your app with the latest Core ML execution engine improvements and find out how aggressive asset caching can help with inference and faster model loads. We’ll show you some of the latest options for async prediction and discuss considerations for balancing performance with overall memory usage to help you create a highly responsive app. Discover APIs to help you understand and maximize hardware utilization for your models."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10080-Build-spatial-experiences-with-RealityKit":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10080-build-spatial-experiences-with-realitykit","abstract":[{"type":"text","text":"Discover how RealityKit can bring your apps into a new dimension. Get started with RealityKit entities, components, and systems, and learn how you can add 3D models and effects to your app on visionOS. We’ll also take you through the RealityView API and demonstrate how to add 3D objects to windows, volumes, and spaces to make your apps more immersive. And we’ll explore combining RealityKit with spatial input, animation, and spatial audio."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10080-Build-spatial-experiences-with-RealityKit","role":"sampleCode","title":"Build spatial experiences with RealityKit","kind":"article"},"https://developer.apple.com/videos/play/wwdc2023/10082":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082","title":"Meet ARKit for spatial computing - WWDC23","type":"link","titleInlineContent":[{"text":"Meet ARKit for spatial computing - WWDC23","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10123-Bring-your-game-to-Mac-Part-1-Make-a-game-plan":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10123-bring-your-game-to-mac-part-1-make-a-game-plan","abstract":[{"type":"text","text":"Bring modern, high-end games to Mac and iPad with the powerful features of Metal and Apple silicon. Discover the game porting toolkit and learn how it can help you evaluate your existing Windows game for graphics feature compatibility and performance. We’ll share best practices and technical resources for handling audio, input, and advanced display features."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10123-Bring-your-game-to-Mac-Part-1-Make-a-game-plan","role":"sampleCode","title":"Bring your game to Mac, Part 1: Make a game plan","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10033-Extend-Speech-Synthesis-with-personal-and-custom-voices":{"title":"Extend Speech Synthesis with personal and custom voices","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10033-Extend-Speech-Synthesis-with-personal-and-custom-voices","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10033-extend-speech-synthesis-with-personal-and-custom-voices","kind":"article","abstract":[{"text":"Bring the latest advancements in Speech Synthesis to your apps. Learn how you can integrate your custom speech synthesizer and voices into iOS and macOS. We’ll show you how SSML is used to generate expressive speech synthesis, and explore how Personal Voice can enable your augmentative and assistive communication app to speak on a person’s behalf in an authentic way.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10094-Enhance-your-iPad-and-iPhone-apps-for-the-Shared-Space":{"url":"\/documentation\/wwdcnotes\/wwdc23-10094-enhance-your-ipad-and-iphone-apps-for-the-shared-space","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10094-Enhance-your-iPad-and-iPhone-apps-for-the-Shared-Space","role":"sampleCode","kind":"article","title":"Enhance your iPad and iPhone apps for the Shared Space","abstract":[{"text":"Get ready to enhance your iPad and iPhone apps for the Shared Space! We’ll show you how to optimize your experience to make it feel great on visionOS and explore Designed for iPad app interaction, visual treatments, and media.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10138-Design-and-build-apps-for-watchOS-10":{"abstract":[{"text":"Dive into the details of watchOS design principles and learn how to apply them in your app using SwiftUI. We’ll show you how to build an app for the redesigned user interface to surface timely information, communicate focused content at a glance, and make navigation consistent and predictable.","type":"text"}],"title":"Design and build apps for watchOS 10","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10138-Design-and-build-apps-for-watchOS-10","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10138-design-and-build-apps-for-watchos-10","type":"topic"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=110":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=110","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=110","title":"1:50 - App Architecture","type":"link","titleInlineContent":[{"text":"1:50 - App Architecture","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10023-Build-a-multidevice-workout-app":{"abstract":[{"type":"text","text":"Learn how you can get iPhone involved in your Apple Watch-based workout apps with HealthKit. We’ll show you how to mirror workouts between devices and take a ride with cycling data types. Plus, get to know HealthKit for iPad."}],"kind":"article","title":"Build a multi-device workout app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10023-Build-a-multidevice-workout-app","type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10023-build-a-multidevice-workout-app"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10095-Explore-rendering-for-spatial-computing":{"role":"sampleCode","abstract":[{"text":"Find out how you can take control of RealityKit rendering to improve the look and feel of your apps and games on visionOS. Discover how you can customize lighting, add grounding shadows, and control tone mapping for your content. We’ll also go over best practices for two key treatments on the platform: rasterization rate maps and dynamic content scaling.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10095-Explore-rendering-for-spatial-computing","kind":"article","title":"Explore rendering for spatial computing","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10095-explore-rendering-for-spatial-computing"},"https://laurentbrusa.hashnode.dev/":{"identifier":"https:\/\/laurentbrusa.hashnode.dev\/","url":"https:\/\/laurentbrusa.hashnode.dev\/","title":"Blog","type":"link","titleInlineContent":[{"text":"Blog","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10104-Integrate-your-media-app-with-HomePod":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10104-integrate-your-media-app-with-homepod","abstract":[{"type":"text","text":"Learn how people can interact with your media app directly from HomePod. We’ll show you how to add a media intent to your iPhone or iPad app and help people stream your content to a HomePod speaker over AirPlay simply by using their voice. Explore implementation details and get tips and best practices on how to create a great experience for music, audiobooks, podcasts, meditations, or other media types."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10104-Integrate-your-media-app-with-HomePod","role":"sampleCode","title":"Integrate your media app with HomePod","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10161-Inspectors-in-SwiftUI-Discover-the-details":{"title":"Inspectors in SwiftUI: Discover the details","url":"\/documentation\/wwdcnotes\/wwdc23-10161-inspectors-in-swiftui-discover-the-details","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10161-Inspectors-in-SwiftUI-Discover-the-details","role":"sampleCode","abstract":[{"type":"text","text":"Meet Inspectors — a structural API that can help bring a new level of detail to your apps. We’ll take you through the fundamentals of the API and show you how to adopt it. Learn about the latest updates to sheet presentation customizations and find out how you can combine the two to create perfect presentation experiences."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10244-Create-rich-documentation-with-SwiftDocC":{"abstract":[{"type":"text","text":"Learn how you can take advantage of the latest features in Swift-DocC to create rich and detailed documentation for your app or framework. We’ll show you how to use the Xcode 15 Documentation Preview editor to efficiently iterate on your existing project’s documentation, and explore expanded authoring capabilities like grid-based layouts, video support, and custom themes."}],"title":"Create rich documentation with Swift-DocC","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10244-Create-rich-documentation-with-SwiftDocC","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10244-create-rich-documentation-with-swiftdocc","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10100-Optimize-app-power-and-performance-for-spatial-computing":{"abstract":[{"type":"text","text":"Learn how you can create powerful apps and games for visionOS by optimizing for performance and efficiency. We’ll cover the unique power characteristics of the platform, explore building a performance plan, and share some of the tools and strategies to test and optimize your apps."}],"kind":"article","title":"Optimize app power and performance for spatial computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10100-Optimize-app-power-and-performance-for-spatial-computing","type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10100-optimize-app-power-and-performance-for-spatial-computing"},"WWDC23-10089-metal7":{"identifier":"WWDC23-10089-metal7","variants":[{"url":"\/images\/WWDC23-10089-metal7.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10271-Explore-immersive-sound-design":{"title":"Explore immersive sound design","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10271-explore-immersive-sound-design","role":"sampleCode","abstract":[{"text":"Discover how you can use sound to enhance the experience of your visionOS apps and games. Learn how Apple designers select sounds and build soundscapes to create textural, immersive experiences. We’ll share how you can enrich basic interactions in your app with sound when you place audio cues spatially, vary repetitive sounds, and build moments of sonic delight into your app.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10271-Explore-immersive-sound-design"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10012-Explore-App-Store-Connect-for-spatial-computing":{"abstract":[{"text":"App Store Connect provides the tools you need to test, submit, and manage your visionOS apps on the App Store. Explore basics and best practices for deploying your first spatial computing app, adding support for visionOS to an existing app, and managing compatibility. We’ll also show you how TestFlight for visionOS can help you test your apps and collect valuable feedback as you iterate.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10012-explore-app-store-connect-for-spatial-computing","title":"Explore App Store Connect for spatial computing","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10012-Explore-App-Store-Connect-for-spatial-computing","kind":"article","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10157-Wind-your-way-through-advanced-animations-in-SwiftUI":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10157-wind-your-way-through-advanced-animations-in-swiftui","abstract":[{"text":"Discover how you can take animation to the next level with the latest updates to SwiftUI. Join us as we wind our way through animation and build out multiple steps, use keyframes to add coordinated multi-track animated effects, and combine APIs in unique ways to make your app spring to life.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10157-Wind-your-way-through-advanced-animations-in-SwiftUI","role":"sampleCode","title":"Wind your way through advanced animations in SwiftUI","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10224-Simplify-distribution-in-Xcode-and-Xcode-Cloud":{"title":"Simplify distribution in Xcode and Xcode Cloud","url":"\/documentation\/wwdcnotes\/wwdc23-10224-simplify-distribution-in-xcode-and-xcode-cloud","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10224-Simplify-distribution-in-Xcode-and-Xcode-Cloud","abstract":[{"text":"Discover how to share your app using Xcode’s streamlined distribution, which allows you to submit your app to TestFlight or the App Store with one click. We’ll also show you how to use Xcode Cloud to simplify your distribution process by automatically including notes for testers in TestFlight, and use post-action to automatically notarize your Mac apps.","type":"text"}],"role":"sampleCode","kind":"article"},"WWDC23-10089-metal12":{"identifier":"WWDC23-10089-metal12","variants":[{"url":"\/images\/WWDC23-10089-metal12.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10076-Design-for-spatial-user-interfaces":{"title":"Design for spatial user interfaces","url":"\/documentation\/wwdcnotes\/wwdc23-10076-design-for-spatial-user-interfaces","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10076-Design-for-spatial-user-interfaces","role":"sampleCode","abstract":[{"type":"text","text":"Learn how to design great interfaces for spatial computing apps. We’ll share how your existing screen-based knowledge easily translates into creating great experiences for visionOS. Explore guidelines for UI components, materials, and typography and find out how you can design experiences that are familiar, legible, and easy to use."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10181-Support-HDR-images-in-your-app":{"title":"Support HDR images in your app","url":"\/documentation\/wwdcnotes\/wwdc23-10181-support-hdr-images-in-your-app","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10181-Support-HDR-images-in-your-app","role":"sampleCode","abstract":[{"text":"Learn how to identify, load, display, and create High Dynamic Range (HDR) still images in your app. Explore common HDR concepts and find out about the latest updates to the ISO specification. Learn how to identify and display HDR images with SwiftUI and UIKit, create them from ProRAW and RAW captures, and display them in CALayers. We’ll also take you through CoreGraphics support for ISO HDR and share best practices for HDR adoption.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10168-Generalize-APIs-with-parameter-packs":{"abstract":[{"type":"text","text":"Swift parameter packs are a powerful tool to expand what is possible in your generic code while also enabling you to simplify common generic patterns. We’ll show you how to abstract over types as well as the number of arguments in generic code and simplify common generic patterns to avoid overloads."}],"title":"Generalize APIs with parameter packs","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10168-Generalize-APIs-with-parameter-packs","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10168-generalize-apis-with-parameter-packs","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit":{"abstract":[{"type":"text","text":"Go beyond the window and learn how you can bring engaging and immersive 3D content to your apps with RealityKit. Discover how SwiftUI scenes work in tandem with RealityView and how you can embed your content into an entity hierarchy. We’ll also explore how you can blend virtual content and the real world using anchors, bring particle effects into your apps, add video content, and create more immersive experiences with portals."}],"title":"Enhance your spatial computing app with RealityKit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10081-enhance-your-spatial-computing-app-with-realitykit","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10159-Beyond-scroll-views":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10159-Beyond-scroll-views","abstract":[{"text":"Find out how you can take your scroll views to the next level with the latest APIs in SwiftUI. We’ll show you how to customize scroll views like never before. Explore the relationship between safe areas and a scroll view’s margins, learn how to interact with the content offset of a scroll view, and discover how you can add a bit of flair to your content with scroll transitions.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10159-beyond-scroll-views","type":"topic","role":"sampleCode","title":"Beyond scroll views","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10103-Explore-enhancements-to-App-Intents":{"abstract":[{"text":"Bring your widgets to life with App Intents! Explore the latest updates and learn how you can take advantage of dynamic options and user interactivity to build better experiences for your App Shortcuts. We’ll share how you can integrate with Apple Pay, structure your code more efficiently, and take your Shortcuts app integration to the next level.","type":"text"}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10103-explore-enhancements-to-app-intents","type":"topic","title":"Explore enhancements to App Intents","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10103-Explore-enhancements-to-App-Intents"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10036-Build-accessible-apps-with-SwiftUI-and-UIKit":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10036-build-accessible-apps-with-swiftui-and-uikit","abstract":[{"type":"text","text":"Discover how advancements in UI frameworks make it easier to build rich, accessible experiences. Find out how technologies like VoiceOver can better interact with your app’s interface through accessibility traits and actions. We’ll share the latest updates to SwiftUI that help you refine your accessibility experience and show you how to keep accessibility information up-to-date in your UIKit apps."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10036-Build-accessible-apps-with-SwiftUI-and-UIKit","role":"sampleCode","title":"Build accessible apps with SwiftUI and UIKit","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10078-Design-considerations-for-vision-and-motion":{"title":"Design considerations for vision and motion","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10078-Design-considerations-for-vision-and-motion","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10078-design-considerations-for-vision-and-motion","kind":"article","abstract":[{"text":"Learn how to design engaging immersive experiences for visionOS that respect the limitations of human vision and motion perception. We’ll show you how you can use depth cues, contrast, focus, and motion to keep people comfortable as they enjoy your apps and games.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10070-Create-a-great-spatial-playback-experience":{"abstract":[{"text":"Get ready to support video in your visionOS app! Take a tour of the frameworks and APIs that power video playback and learn how you can update your app to play 3D content. We’ll also share tips for customizing playback to create a more immersive watching experience.","type":"text"}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10070-create-a-great-spatial-playback-experience","type":"topic","title":"Create a great spatial playback experience","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10070-Create-a-great-spatial-playback-experience"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10195-Model-your-schema-with-SwiftData":{"url":"\/documentation\/wwdcnotes\/wwdc23-10195-model-your-schema-with-swiftdata","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10195-Model-your-schema-with-SwiftData","role":"sampleCode","kind":"article","title":"Model your schema with SwiftData","abstract":[{"text":"Learn how to use schema macros and migration plans with SwiftData to build more complex features for your app. We’ll show you how to fine-tune your persistence with @Attribute and @Relationship options. Learn how to exclude properties from your data model with @Transient and migrate from one version of your schema to the next with ease.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10111-Go-beyond-the-window-with-SwiftUI":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10111-go-beyond-the-window-with-swiftui","abstract":[{"text":"Get ready to launch into space — a new SwiftUI scene type that can help you make great immersive experiences for visionOS. We’ll show you how to create a new scene with ImmersiveSpace, place 3D content, and integrate RealityView. Explore how you can use the immersionStyle scene modifier to increase the level of immersion in an app and learn best practices for managing spaces, adding virtual hands with ARKit, adding support for SharePlay, and building an “out of this world” experience!","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10111-Go-beyond-the-window-with-SwiftUI","role":"sampleCode","title":"Go beyond the window with SwiftUI","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10262-Rediscover-Safari-developer-features":{"abstract":[{"text":"Get ready to explore Safari’s rich set of tools for web developers and designers. Learn how you can inspect web content, find out about Responsive Design Mode and WebDriver, and get started with simulators and devices. We’ll also show you how to pair with Vision Pro, make content inspectable in your apps, and use Open with Simulator in Responsive Design Mode to help you test your websites on any device.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10262-rediscover-safari-developer-features","title":"Rediscover Safari developer features","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10262-Rediscover-Safari-developer-features","kind":"article","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10266-Protect-your-Mac-app-with-environment-constraints":{"abstract":[{"type":"text","text":"Learn how to improve the security of your Mac app by adopting environment constraints. We’ll show you how to set limits on how processes are launched, make sure your Launch Agents and Launch Daemons aren’t tampered with, and prevent unwanted code from running in your address space."}],"kind":"article","title":"Protect your Mac app with environment constraints","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10266-Protect-your-Mac-app-with-environment-constraints","type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10266-protect-your-mac-app-with-environment-constraints"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10042-Explore-Natural-Language-multilingual-models":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10042-explore-natural-language-multilingual-models","abstract":[{"type":"text","text":"Learn how to create custom Natural Language models for text classification and word tagging using multilingual, transformer-based embeddings. We’ll show you how to train with less data and support up to 27 different languages across three scripts. Find out how to use these embeddings to fine-tune complex models trained in PyTorch and TensorFlow."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10042-Explore-Natural-Language-multilingual-models","role":"sampleCode","title":"Explore Natural Language multilingual models","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10125-Bring-your-game-to-Mac-Part-3-Render-with-Metal":{"title":"Bring your game to Mac, Part 3: Render with Metal","url":"\/documentation\/wwdcnotes\/wwdc23-10125-bring-your-game-to-mac-part-3-render-with-metal","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10125-Bring-your-game-to-Mac-Part-3-Render-with-Metal","role":"sampleCode","abstract":[{"text":"Discover how you can support Metal in your rendering code as we close out our three-part series on bringing your game to Mac. Once you’ve evaluated your existing Windows binary with the game porting toolkit and brought your HLSL shaders over to Metal, learn how you can optimally implement the features that high-end, modern games require. We’ll show you how to manage GPU resource bindings, residency, and synchronization. Find out how to optimize GPU commands submission, render rich visuals with MetalFX Upscaling, and more.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10051-Create-a-great-ShazamKit-experience":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10051-create-a-great-shazamkit-experience","abstract":[{"type":"text","text":"Discover how your app can offer a great audio matching experience with the latest updates to ShazamKit. We’ll take you through matching features, updates to audio recognition, and interactions with the Shazam library. Learn tips and best practices for using ShazamKit in your audio apps."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10051-Create-a-great-ShazamKit-experience","role":"sampleCode","title":"Create a great ShazamKit experience","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10250-Prototype-with-Xcode-Playgrounds":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10250-prototype-with-xcode-playgrounds","abstract":[{"type":"text","text":"Speed up feature development by prototyping new code with Xcode Playgrounds, eliminating the need to keep rebuilding and relaunching your project to verify your changes. We’ll show you how using a playground in your project or package can help you try out your code in various scenarios and take a close look at the returned values, including complex structures and user interface elements, so you can quickly iterate on a feature before integrating it into your project."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10250-Prototype-with-Xcode-Playgrounds","role":"sampleCode","title":"Prototype with Xcode Playgrounds","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10027-Bring-widgets-to-new-places":{"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10027-bring-widgets-to-new-places","abstract":[{"text":"The widget ecosystem is expanding: Discover how you can use the latest WidgetKit APIs to make your widget look great everywhere. We’ll show you how to identify your widget’s background, adjust layout dynamically, and prepare colors for vibrant rendering so that your widget can sit seamlessly in any environment.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10027-Bring-widgets-to-new-places","title":"Bring widgets to new places","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10029-Build-widgets-for-the-Smart-Stack-on-Apple-Watch":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10029-build-widgets-for-the-smart-stack-on-apple-watch","abstract":[{"text":"Follow along as we build a widget for the Smart Stack on watchOS 10 using the latest SwiftUI and WidgetKit APIs. Learn tips, techniques, and best practices for creating widgets that show relevant information on Apple Watch.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10029-Build-widgets-for-the-Smart-Stack-on-Apple-Watch","role":"sampleCode","title":"Build widgets for the Smart Stack on Apple Watch","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10073-Design-for-spatial-input":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10073-Design-for-spatial-input","abstract":[{"text":"Learn how to design great interactions for eyes and hands. We’ll share the design principles for spatial input, explore best practices around input methods, and help you create spatial experiences that are comfortable, intuitive, and satisfying.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10073-design-for-spatial-input","type":"topic","role":"sampleCode","title":"Design for spatial input","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision":{"url":"\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","role":"sampleCode","kind":"article","title":"Explore 3D body pose and person segmentation in Vision","abstract":[{"text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We’ll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10153-Unlock-the-power-of-grammatical-agreement":{"abstract":[{"text":"Discover how you can use automatic grammatical agreement in your apps and games to create inclusive and more natural-sounding expressions. We’ll share best practices for working with Foundation, showcase examples in multiple languages, and demonstrate how to use these APIs to enhance the user experience for your apps.","type":"text"}],"title":"Unlock the power of grammatical agreement","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10153-Unlock-the-power-of-grammatical-agreement","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10153-unlock-the-power-of-grammatical-agreement","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10110-Elevate-your-windowed-app-for-spatial-computing":{"title":"Elevate your windowed app for spatial computing","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10110-elevate-your-windowed-app-for-spatial-computing","role":"sampleCode","abstract":[{"text":"Discover how you can bring your multiplatform SwiftUI app to visionOS and the Shared Space. We’ll show you how to add the visionOS destination to an existing app and view your app in the Simulator. Explore how your SwiftUI code automatically adapts to support the unique context and presentation of the visionOS platform. Learn how you can update custom views, improve your app’s UI, and add features and controls specific to this platform.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10110-Elevate-your-windowed-app-for-spatial-computing"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10061-Verify-app-dependencies-with-digital-signatures":{"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10061-verify-app-dependencies-with-digital-signatures","abstract":[{"text":"Discover how you can help secure your app’s dependencies. We’ll show you how Xcode can automatically verify any signed XCFrameworks you include within a project. Learn how code signatures work, the benefits they provide to help protect your software supply chain, and how SDK developers can sign their XCFrameworks to help keep your apps secure.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10061-Verify-app-dependencies-with-digital-signatures","title":"Verify app dependencies with digital signatures","kind":"article"},"WWDC23-10089-metal3":{"identifier":"WWDC23-10089-metal3","variants":[{"url":"\/images\/WWDC23-10089-metal3.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"WWDC23-10089-metal6":{"identifier":"WWDC23-10089-metal6","variants":[{"url":"\/images\/WWDC23-10089-metal6.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10149-Discover-Observation-in-SwiftUI":{"abstract":[{"text":"Simplify your SwiftUI data models with Observation. We’ll share how the Observable macro can help you simplify models and improve your app’s performance. Get to know Observation, learn the fundamentals of the macro, and find out how to migrate from ObservableObject to Observable.","type":"text"}],"title":"Discover Observation in SwiftUI","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10149-Discover-Observation-in-SwiftUI","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10149-discover-observation-in-swiftui","type":"topic"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=780":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=780","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=780","title":"13:00 - Render a frame","type":"link","titleInlineContent":[{"text":"13:00 - Render a frame","type":"text"}]},"https://developer.apple.com/forums/create/question?tag1=795030&tag2=763030&tag3=164":{"identifier":"https:\/\/developer.apple.com\/forums\/create\/question?tag1=795030&tag2=763030&tag3=164","url":"https:\/\/developer.apple.com\/forums\/create\/question?tag1=795030&tag2=763030&tag3=164","title":"Have a question? Ask with tag wwdc2023-10089","type":"link","titleInlineContent":[{"text":"Have a question? Ask with tag wwdc2023-10089","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=1048":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1048","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1048","title":"17:28 - User input","type":"link","titleInlineContent":[{"text":"17:28 - User input","type":"text"}]},"WWDC23-10089-metal8":{"identifier":"WWDC23-10089-metal8","variants":[{"url":"\/images\/WWDC23-10089-metal8.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10072-Principles-of-spatial-design":{"abstract":[{"text":"Discover the fundamentals of spatial design. Learn how to design with depth, scale, windows, and immersion, and apply best practices for creating comfortable, human-centered experiences that transform reality. Find out how you can use these spatial design principles to extend your existing app or bring a new idea to life.","type":"text"}],"title":"Principles of spatial design","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10072-Principles-of-spatial-design","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10072-principles-of-spatial-design","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/multitudes":{"title":"laurent b (32 notes)","url":"\/documentation\/wwdcnotes\/multitudes","abstract":[{"text":"student at 42Berlin 🐬 | 🍎 Swift(UI) app dev  | speciality coffee ☕️ & cycling 🚴🏻‍♂️","type":"text"}],"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10239-Add-SharePlay-to-your-app":{"type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10239-add-shareplay-to-your-app","abstract":[{"text":"Discover how your app can take advantage of SharePlay to turn any activity into a shareable experience with friends! We’ll share the latest updates to SharePlay, explore the benefits of creating shared activities, dive into some exciting use cases, and take you through best practices to create engaging and fun moments of connection in your app.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10239-Add-SharePlay-to-your-app","title":"Add SharePlay to your app","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10086-Explore-the-USD-ecosystem":{"abstract":[{"text":"Discover the latest updates to Universal Scene Description (USD) on Apple platforms and learn how you can deliver great 3D content for your apps, games, and websites. Get to know USD for visionOS, explore MaterialX shaders and color management, and find out about some of the other improvements to the USD ecosystem.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10086-explore-the-usd-ecosystem","title":"Explore the USD ecosystem","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10086-Explore-the-USD-ecosystem","kind":"article","role":"sampleCode"},"WWDC23-10089-metal16":{"identifier":"WWDC23-10089-metal16","variants":[{"url":"\/images\/WWDC23-10089-metal16.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10056-Build-better-documentbased-apps":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10056-Build-better-documentbased-apps","abstract":[{"text":"Discover how you can use the latest features in iPadOS to improve your document-based apps. We’ll show you how to take advantage of UIDocument as well as existing desktop-class iPad and document-based APIs to add new features in your app. Find out how to convert data models to UIDocument, present documents with UIDocumentViewController, learn how to migrate your apps to the latest APIs, and explore best practices.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10056-build-better-documentbased-apps","type":"topic","role":"sampleCode","title":"Build better document-based apps","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10034-Create-accessible-spatial-experiences":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10034-create-accessible-spatial-experiences","abstract":[{"type":"text","text":"Learn how you can make spatial computing apps that work well for everyone. Like all Apple platforms, visionOS is designed for accessibility: We’ll share how we’ve reimagined assistive technologies like VoiceOver and Pointer Control and designed features like Dwell Control to help people interact in the way that works best for them. Learn best practices for vision, motor, cognitive, and hearing accessibility and help everyone enjoy immersive experiences for visionOS."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10034-Create-accessible-spatial-experiences","role":"sampleCode","title":"Create accessible spatial experiences","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10309-Design-widgets-for-the-Smart-Stack-on-Apple-Watch":{"abstract":[{"text":"Bring your widgets to watchOS with the new Smart Stack. We’ll show you how to use standard design layouts, color and iconography, and signal-based relevancy to ensure your app’s widgets are glanceable, distinctive and smart.","type":"text"}],"title":"Design widgets for the Smart Stack on Apple Watch","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10309-Design-widgets-for-the-Smart-Stack-on-Apple-Watch","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10309-design-widgets-for-the-smart-stack-on-apple-watch","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10166-Write-Swift-macros":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how you can use Swift macros to make your codebase more expressive and easier to read. Code along as we explore how macros can help you avoid writing repetitive code and find out how to use them in your app. We’ll share the building blocks of a macro, show you how to test it, and take you through how you can emit compilation errors from macros."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10166-Write-Swift-macros","role":"sampleCode","title":"Write Swift macros","url":"\/documentation\/wwdcnotes\/wwdc23-10166-write-swift-macros"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10124-Bring-your-game-to-Mac-Part-2-Compile-your-shaders":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how the Metal shader converter streamlines the process of bringing your HLSL shaders to Metal as we continue our three-part series on bringing your game to Mac. Find out how to build a fast, end-to-end shader pipeline from DXIL that supports all shader stages and allows you to leverage the advanced features of Apple GPUs. We’ll also show you how to reduce app launch time and stutters by generating GPU binaries with the offline compiler."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10124-Bring-your-game-to-Mac-Part-2-Compile-your-shaders","role":"sampleCode","title":"Bring your game to Mac, Part 2: Compile your shaders","url":"\/documentation\/wwdcnotes\/wwdc23-10124-bring-your-game-to-mac-part-2-compile-your-shaders"},"WWDC23-10089-metal":{"identifier":"WWDC23-10089-metal","variants":[{"url":"\/images\/WWDC23-10089-metal.jpg","traits":["1x","light"]}],"alt":"Metal","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10185-Update-Live-Activities-with-push-notifications":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10185-update-live-activities-with-push-notifications","abstract":[{"type":"text","text":"Discover how you can remotely update Live Activities in your app when you push content through Apple Push Notification service (APNs). We’ll show you how to configure your first Live Activity push locally so you can quickly iterate on your implementation. Learn best practices for determining your push priority and configuring alerting updates, and explore how to further improve your Live Activities with relevance score and stale date."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10185-Update-Live-Activities-with-push-notifications","role":"sampleCode","title":"Update Live Activities with push notifications","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10260-Get-started-with-building-apps-for-spatial-computing":{"title":"Get started with building apps for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-10260-get-started-with-building-apps-for-spatial-computing","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10260-Get-started-with-building-apps-for-spatial-computing","role":"sampleCode","abstract":[{"text":"Get ready to develop apps and games for visionOS! Discover the fundamental building blocks that make up spatial computing — windows, volumes, and spaces — and find out how you can use these elements to build engaging and immersive experiences.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10091-Evolve-your-ARKit-app-for-spatial-experiences":{"abstract":[{"type":"text","text":"Discover how you can bring your app’s AR experience to visionOS. Learn how ARKit and RealityKit have evolved for spatial computing: We’ll highlight conceptual and API changes for those coming from iPadOS and iOS and guide you to sessions with more details to help you bring your AR experience to this platform."}],"title":"Evolve your ARKit app for spatial experiences","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10091-Evolve-your-ARKit-app-for-spatial-experiences","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10091-evolve-your-arkit-app-for-spatial-experiences","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10278-Create-practical-workflows-in-Xcode-Cloud":{"type":"topic","title":"Create practical workflows in Xcode Cloud","abstract":[{"type":"text","text":"Learn how Xcode Cloud can help teams of all shapes and sizes in their development process. We’ll share different ways to configure actions to help you create simple yet powerful workflows, and show you how to extend Xcode Cloud when you integrate with additional tools."}],"url":"\/documentation\/wwdcnotes\/wwdc23-10278-create-practical-workflows-in-xcode-cloud","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10278-Create-practical-workflows-in-Xcode-Cloud","role":"sampleCode"},"https://developer.apple.com/videos/play/wwdc2021/10161":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161","title":"Explore HDR rendering with EDR - WWDC21","type":"link","titleInlineContent":[{"text":"Explore HDR rendering with EDR - WWDC21","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10156-Explore-SwiftUI-animation":{"url":"\/documentation\/wwdcnotes\/wwdc23-10156-explore-swiftui-animation","kind":"article","role":"sampleCode","type":"topic","abstract":[{"type":"text","text":"Explore SwiftUI’s powerful animation capabilities and find out how these features work together to produce impressive visual effects. Learn how SwiftUI refreshes the rendering of a view, determines what to animate, interpolates values over time, and propagates context for the current transaction."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10156-Explore-SwiftUI-animation","title":"Explore SwiftUI animation"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10170-Beyond-the-basics-of-structured-concurrency":{"title":"Beyond the basics of structured concurrency","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10170-Beyond-the-basics-of-structured-concurrency","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10170-beyond-the-basics-of-structured-concurrency","kind":"article","abstract":[{"text":"It’s all about the task tree: Find out how structured concurrency can help your apps manage automatic task cancellation, task priority propagation, and useful task-local value patterns. Learn how to manage resources in your app with useful patterns and the latest task group APIs. We’ll show you how you can leverage the power of the task tree and task-local values to gain insight into distributed systems.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10142-Explore-testing-inapp-purchases":{"url":"\/documentation\/wwdcnotes\/wwdc23-10142-explore-testing-inapp-purchases","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10142-Explore-testing-inapp-purchases","role":"sampleCode","title":"Explore testing in-app purchases","abstract":[{"text":"Learn how you can test in-app purchases throughout development with StoreKit Testing in Xcode, App Store sandbox, and TestFlight. Explore how each tool functions and how you can combine them to build the right workflow for testing your apps and games. We’ll also share a sneak preview of how you can test Family Sharing for in-app purchases & subscriptions in the App Store sandbox.","type":"text"}],"kind":"article","type":"topic"}}}