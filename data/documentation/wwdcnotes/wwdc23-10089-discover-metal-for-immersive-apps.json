{"sampleCodeDownload":{"kind":"sampleDownload","action":{"overridingTitle":"Watch Video (20 min)","identifier":"https:\/\/developer.apple.com\/wwdc23\/10089","isActive":true,"type":"reference"}},"metadata":{"title":"Discover Metal for immersive apps","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC23","role":"sampleCode"},"primaryContentSections":[{"kind":"content","content":[{"type":"heading","level":2,"anchor":"Chapters","text":"Chapters"},{"type":"paragraph","inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=6"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=110"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=358","type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=689","type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=780"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1048"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1222"}]},{"type":"heading","level":1,"anchor":"Intro","text":"Intro"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"App architecture. To write our own engine, the CompositorServices API gives access to Metal rendering on xrOS. We can combine it with ARKit, which adds world tracking and hand tracking, to create a fully immersive experience."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Render configuration. CompositorServices is the key to configure our engine to work on xrOS."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Render loop, how to setup the render loop","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"And how to render one frame.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"User input, how to use ARKit to make our experience interactive."}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"How to create immersive experiences with Metal on xrOS."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"RecRoom is a great example of an application that provides a fully immersive experience using CompositorServices to create a rendering session, Metal APIs to render frames, and ARKit to get world and hand tracking."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10089-recRoom","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"They were able to bring support to all these technologies thanks to the Unity editor."}]},{"type":"heading","level":1,"anchor":"App-architecture","text":"App architecture"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Start with the architecture of an xrOS app."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"we’ll get the most out of today’s session if we have previous experience with Metal APIs and Metal rendering techniques. If we haven’t used Metal before, check out the code samples and documentation over in developer.apple.com\/Metal."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10089-metal","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"When we create immersive experiences on xrOS with Metal, we’ll start with SwiftUI to create the application and the rendering session."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"After creating the rendering session, we can switch to a language we might be more familiar with, like C or C++, to define the inner parts of the engine."}]},{"type":"paragraph","inlineContent":[{"text":"we start by creating a type that conforms to the SwiftUI app protocol. To conform to this protocol, we will define the list of scenes in our app.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"On xrOS, there are three main scenes types."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The window type provides an experience similar to 2D platforms like macOS. The volume type renders content within its bounds and it coexists in the Shared Space with other applications. And ImmersiveSpace allows us to render content anywhere."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal1"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Whenever we render fully immersive experiences with Metal, we will choose the ImmersiveSpace type."}]},{"type":"heading","level":2,"anchor":"ImmersiveSpace","text":"ImmersiveSpace"},{"type":"paragraph","inlineContent":[{"type":"text","text":"ImmersiveSpace is a new SwiftUI Scene type available on xrOS."}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Conforms to SwiftUI Scene","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Container for immersive experiences"}]}]},{"content":[{"inlineContent":[{"text":"It serves as the container for fully immersive experiences.","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"text":"To learn how to use ImmersiveSpace, check out the session “Go beyond the window with SwiftUI.”","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111","type":"reference","isActive":true}]},{"type":"heading","level":2,"anchor":"Rendering-on-an-ImmersiveSpace","text":"Rendering on an ImmersiveSpace"},{"type":"paragraph","inlineContent":[{"text":"When we create an ImmersiveSpace scene, our application provides the content by using a type that conforms to the ImmersiveSpaceContent protocol. Often, when creating content for an ImmersiveSpace Scene, applications will use RealityKit.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10089-metal2","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"It uses CoreAnimation and MaterialX under the hood. But we canuse the power of Metal to render the content of our application."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal3"}]},{"type":"heading","level":2,"anchor":"CompositorServices-API","text":"CompositorServices API"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Metal rendering interface"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Render directly to the compositor server","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Low IPC overhead to minimize latency","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"text":"The new CompositorServices API, introduced in xrOS, provides a Metal rendering interface to be able to render the contents of an ImmersiveSpace.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"With CompositorServices, applications can render directly into the compositor server. It has a low IPC overhead to minimize latency, and it is built from the ground up to support both C and Swift APIs."}]},{"type":"paragraph","inlineContent":[{"text":"When using CompositorServices, the ImmersiveSpaceContent is called CompositorLayer.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"To create a CompositorLayer we will need to provide two parameters. The first one is the CompositorLayerConfiguration protocol, which defines the behavior and capabilities of the rendering session. The second is the LayerRenderer.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10089-metal4","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This is the interface to the layer rendering session. our application will use this object to schedule and render new frames."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"When writing an immersive experience with Metal, we start by defining the app type. As the scene type, use ImmersiveSpace."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"For the content type, use a CompositorLayer."}]},{"type":"paragraph","inlineContent":[{"text":"Once the CompositorLayer is ready to render content, the system will call the application with the instance of the rendering session.Here is a good place to create the instance of our custom engine.Now that we have the engine instance, we can create the render thread and run the render loop by calling start.","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer { layerRenderer in","\t\t\t\tlet engine = my_engine_create(layerRenderer)","\t\t\t\tlet renderThread = Thread {","\t\t\t\t\tmy_engine_render_loop(engine)","\t\t\t\t}","\t\t\t\trenderThread.name = \"Render Thread\"","\t\t\t\trenderThread.start ()","\t\t\t}","\t\t}","\t}","}"]},{"level":2,"type":"heading","anchor":"Customize-default-app-scene","text":"Customize default app scene"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Bounded window by default","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Space with Compositor Content as the initial Scene","type":"text"}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"One thing to take into account when defining the scene list in our application, is that by default SwiftUI creates a window scene, even if the first scene in our app is an ImmersiveSpace."}]},{"type":"paragraph","inlineContent":[{"text":"To change that default behavior, we can modify the info plist of our app.we can add the key UIApplicationPreferred DefaultSceneSessionRole to our application scene manifest to change the default scene type of our application.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"If we are using a space with a Compositor SpaceContent, we will use CPSceneSessionRole ImmersiveSpaceApplication.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal5"}]},{"level":1,"type":"heading","anchor":"Render-configuration","text":"Render configuration"},{"type":"paragraph","inlineContent":[{"type":"text","text":"After setting up the application, and before getting in the render loop, we’ll tell CompositorServices how to configure the LayerRenderer."}]},{"level":2,"type":"heading","anchor":"Configure-CompositorLayer","text":"Configure CompositorLayer"},{"type":"paragraph","inlineContent":[{"type":"text","text":"To provide a configuration to the CompositorLayer, we will create a new type that conforms to the CompositorLayerConfiguration protocol. This protocol allows we to modify the setup and some of the behavior of the rendering session."}]},{"type":"paragraph","inlineContent":[{"text":"The CompositorLayerConfiguration provides we two parameters. The first one is the layer capabilities. It enables we to query what features are available on the device. Use the capabilities to create a valid configuration.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"And the second parameter is the LayerRenderer Configuration.This type defines the configuration of our rendering session. With the configuration, we can define how our engine maps its content into the layer, enable foveated rendering, and define the color management of our pipeline.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal6"}]},{"level":2,"type":"heading","anchor":"Render-with-foveation","text":"Render with foveation"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The main goal of foveated rendering is to allow we to render content at a higher pixel-per-degree density without using a bigger texture size.In a regular display pipeline, the pixels are distributed linearly in a texture."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal7"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"xrOS optimizes this workflow by creating a map that defines what regions in a display can use a lower sampling rate."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10089-metal8","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This helps reduce the power required to render our frame while maintaining the visual fidelity of the display. Using foveation whenever possible is important, as it will result in a better visual experience."}]},{"type":"paragraph","inlineContent":[{"text":"A great way to visualize how foveation affects our rendering pipeline is using Xcode’s Metal Debugger. With Metal Debugger, we can inspect the target textures and rasterization rate maps being used in the render pipeline.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"This capture shows the contents of the texture without scaling for the rasterization rate map.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal9"}]},{"type":"paragraph","inlineContent":[{"text":"we can notice the different sample rates by focusing in the regions of the texture that are more compressed.With the attachment viewer options in Metal Debugger, we can scale the image to visualize the final result that the display will show. Compositor provides the foveation map using an MTLRasterizationRateMap for each frame.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"It is a good practice to always check if foveation is supported.This will change depending on the platform."}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Metal rasterization rate APIs","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"MTLRasterizationRateMap"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Check if foveation is supported","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"code":"capabilities.supportsFoveation","type":"codeVoice"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Set if our application supports foveation","type":"text"}]}]}]},{"type":"paragraph","inlineContent":[{"code":"configuration.isFoveationEnabled = true","type":"codeVoice"}]},{"type":"paragraph","inlineContent":[{"text":"For example, in the xrOS simulator, foveation is not available.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"To enable foveation, we can set isFoveationEnabled on the configuration.","type":"text"}]},{"level":2,"type":"heading","anchor":"LayerRenderer","text":"LayerRenderer"},{"type":"paragraph","inlineContent":[{"text":"The LayerRenderer property is one of the most important configurations for our engine. It defines how each display from the headset gets mapped into the rendered content of our application.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Each eye first maps into a Metal texture provided by Compositor. Then Compositor provides the index of which slice to use within that texture."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal10"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"And finally, Compositor provides the viewport to use within that texture slice. The LayerRenderer lawet lets we choose different mappings between the texture slice and viewport."}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"With layered, Compositor will use one texture with two slices and two viewports."}]}]},{"content":[{"inlineContent":[{"text":"With dedicated, Compositor will use two textures with one slice and one viewport each.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"And finally with shared, Compositor will use one texture, one slice, and two different viewports for that slice."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal11"}]},{"type":"paragraph","inlineContent":[{"text":"Choosing which lawet to use will depend on how we set up our rendering pipeline. For example, with layered and shared, we will be able to perform our rendering in one single pass, so we can optimize our rendering pipeline.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"With shared lawet, it might be easier to port existing code bases where foveated rendering is not an option. Layered lawet is the optimal lawet since it allows we to render our scene in a single pass while still maintaining foveated rendering."}]},{"level":2,"type":"heading","anchor":"Color-management","text":"Color management"},{"type":"paragraph","inlineContent":[{"text":"The last configuration property to discuss is color management.","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Color space","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"CGColorSpace.extendedLinearDisplayP3"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"EDR headroom of 2.0 (2x SDR)"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"HDR-compatible pixel format","type":"text"}]}]}]},{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"configuration.colorFormat = MTLPixelFormat.rgba16Float"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Compositor expects the content to be rendered with extended linear display P3 color space. xrOS supports an EDR headroom of 2.0."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"That is two times the SDR range. By default, Compositor does not use a pixel format that is HDR renderable, but if our application supports HDR, we can specify rgba16Float in the layer configuration."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10089-metal12","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"If we want to know more about how to render HDR with EDR, checkout the session:"}]},{"type":"paragraph","inlineContent":[{"isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161","type":"reference"}]},{"type":"paragraph","inlineContent":[{"text":"To create a custom configuration in our application, start by defining a new type that conforms to the CompositorLayerConfiguration protocol. To conform to this protocol, add the makeConfiguration method.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This method provides the layer capabilities and a configuration we can modify.To enable the three properties I mentioned before, first check if foveation is supported. Then check what lawets are supported in this device."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"With this information, we can set a valid lawet in the configuration.In some devices like the simulator, where the Compositor only renders one view, layered won’t be available."}]},{"type":"paragraph","inlineContent":[{"text":"For foveation, set it to true if the device supports it. And finally, set the colorFormat to rgba16Float to be able to render HDR content.","type":"text"}]},{"syntax":"swift","type":"codeListing","code":["\/\/ CompositorLayer configuration","struct MyConfiguration: CompositorLayerConfiguration {","\tfunc makeConfiguration(capabilities: LayerRenderer. Capabilities,","\t\tconfiguration: inout LayerRenderer.Configuration) {","\t\t\t","\t\t\tlet supportsFoveation = capabilities.supportsFoveation","\t\t\t","\t\t\tlet supportedLawets = capabilities.supportedLawets(options: supportsFoveation ? [.foveationEnabled] : [])","\t\t","\t\t\tconfiguration.lawet = supportedLawets.contains(.layered) ? .layered : .dedicated","","\t\t\tconfiguration.isFoveationEnabled = supportsFoveation","\t\t\t","\t\t\t\/\/ HDR support","\t\t\tconfiguration.colorFormat = .rgba16Float","\t}","}"]},{"type":"paragraph","inlineContent":[{"text":"Returning to the code that created the Compositor layer, we can now add the configuration type we just created.","type":"text"}]},{"syntax":"swift","type":"codeListing","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer(configuration: MyConfiguration()) { layerRenderer in","\t\t\tlet engine = my_engine_create(layerRenderer)","\t\t\tlet renderThread = Thread {","\t\t\t\tmy_engine_render_loop(engine) \/\/ <----","\t\t\t}","\t\t\trenderThread.name = \"Render Thread\"","\t\t\trenderThread.start()","\t\t\t}","\t\t}","\t}","}"]},{"level":1,"type":"heading","anchor":"Render-loop","text":"Render loop"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now that the rendering session is configured, we can set up the render loop."}]},{"type":"paragraph","inlineContent":[{"text":"we’ll start by using the LayerRenderer object from the CompositorLayer.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"First, we’ll load the resources and initialize any objects that our engine will need to render frames.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Then check the state of the layer.If the layer is paused, wait until the layer is running. Once the layer is unblocked from the wait, check the layer state again. If the layer is running, we’ll be able to render a frame.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"And once that frame is rendered, check the layer state again before rendering the next frame. If the layer state is invalidated, free the resources we created for the render loop.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal13"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now, it’s time to define the main function of the render_loop."}]},{"syntax":"swift","type":"codeListing","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer(configuration: MyConfiguration()) { layerRenderer in","\t\t\tlet engine = my_engine_create(layerRenderer)","\t\t\tlet renderThread = Thread {","\t\t\t\tmy_engine_render_loop(engine) \/\/ <-------","\t\t\t}","\t\t\trenderThread.name = \"Render Thread\"","\t\t\trenderThread.start()","\t\t\t}","\t\t}","\t}","}"]},{"type":"paragraph","inlineContent":[{"text":"Until now we’ve been using Swift since the ImmersiveSpace API is only available in Swift.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"But from here we will switch to C to write the render loop."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"As mentioned, the first step in the render loop is to allocate and initialize all the objects we’ll need to render frames. we’ll do this by calling the setup function in our custom engine."}]},{"type":"paragraph","inlineContent":[{"text":"Next, is the main section of the loop. The first step is to check the layerRenderer state. If the state is paused, the thread will sleep until the layerRenderer is running.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"If the layer state is running, the engine will render one frame. And finally, if the layer has been invalidated, the render loop will finish. The last step of the render_loop function will be to clear any used resources.","type":"text"}]},{"syntax":"c","type":"codeListing","code":["void my_engine_render_loop(my_engine *engine) {","\tmy_engine_setup_render_pipeline (engine);","\t","\tbool is_rendering = true;","\twhile (is_rendering) @autoreleasepool {","\t\tswitch (cp_layer_renderer_get_state(engine->layer_renderer)) {","\t\t\tcase cp_layer_renderer_state_paused:","\t\t\t\tcp_layer_renderer_wait_until_running(engine->layer_renderer);","\t\t\t\tbreak;","\t\t\tcase cp_layer_renderer_state_running:","\t\t\t\tmy_engine_render_new_frame(engine);","\t\t\t\tbreak;","\t\t\tcase cp_layer_ renderer_ state_ invalidated:","\t\t\t\tis_rendering = false;","\t\t\tbreak;","\t\t}","\t}","\tmy_engine_invalidate(engine);","}"]},{"level":1,"type":"heading","anchor":"Render-one-frame","text":"Render one frame"},{"type":"paragraph","inlineContent":[{"text":"Now that the app is going through the render loop, we will see how to render one frame. Rendering content in xrOS is always from the point of view of the device. we can use ARKit to obtain the device orientation and translation.","type":"text"}]},{"level":2,"type":"heading","anchor":"World-tracking-with-ARKit","text":"World tracking with ARKit"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"New xrOS API","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Tracking session for world, hands, and more"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Supports C and Swift APIs"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"text":"ARKit is already available on iOS, and now xrOS is introducing a whole new API, which has additional features that can help we create immersive experiences. With ARKit, we can add world tracking, hand tracking, and other world sensing capabilities to our application.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"The new ARKit API is also built from the ground up to support C and Swift APIs, which will allow for an easier integration with existing rendering engines.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"To learn more about ARKit on xrOS, check out:"}]},{"type":"paragraph","inlineContent":[{"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082","isActive":true}]},{"type":"paragraph","inlineContent":[{"text":"Within the render loop, it’s time to render one frame. When rendering a frame, Compositor defines two main sections. The first one is the update. Here’s where we will do any work that is not input-latency critical.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This can be things like updating the animations in our scene, updating our characters, or gathering inputs in our system like hand skeleton poses."}]},{"type":"paragraph","inlineContent":[{"text":"The second section of the frame is the submission section. Here’s where we will perform any latency-critical work. we’ll also render any content that is headset-pose dependent here.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10089-metal14","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"In order to define the timing for each of those sections, Compositor provides a timing object. This diagram defines how the timing affects the different frame sections. The CPU and GPU tracks represent the work that is being done by our application. And the Compositor track represents the work done by the Compositor server to display our frame. The timing type from Compositor Services defines three main time values. First is the optimal input time. That is the best time to query the latency-critical input and start rendering our frame. Second is the rendering deadline. That is the time by when our CPU and GPU work to render a frame should be finished. And third is presentation time. That is the time when our frame will be presented on display.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"In the two sections of our frame, The update section should happen before the optimal input time. After the update, we will wait for the optimal input time before starting the frame submission. Then we will perform the frame submission, which will submit the render work to the GPU. It is important to notice that the CPU and GPU work needs to finish before the rendering deadline, otherwise the Compositor server won’t be able to use this frame and will use a previous one instead. Finally, on the rendering deadline, the Compositor server will composite this frame with the other layers in the system.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal15"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Back to the render loop code, it’s time to define the render_new_frame function. In our engine’s render_new_frame function, we will first query a frame from the layerRenderer. With the frame object, we will be able to predict the timing information. Use that timing information to scope the update and submit intervals."}]},{"type":"paragraph","inlineContent":[{"text":"Next, implement the update section. Define this section by calling the start and end update on the frame. Inside, we will gather the device inputs and update the contents of the frame.","type":"text"}]},{"syntax":"c","type":"codeListing","code":["void my_engine_render_new_frame(my_engine *engine) {","","\tcp_frame_t frame = cp_layer_renderer_query_next_frame (engine->layer_renderer);","\tif (frame == nullptr) { return; }","\t","\tcp_frame_timing_t timing = cp_frame_predict_timing(frame);","\tif (timing == nullptr) { return; }","\t","\tcp_frame_start_update(frame);","\t","\tmy_input_state input_state = my_engine_gather_inputs (engine, timing);","\tmy_engine_update_frame(engine, timing, input_state);","\t","\tcp_frame_end_update(frame);","\t","\t\/\/ ...","}"]},{"type":"paragraph","inlineContent":[{"text":"Once the update is finished, wait for the optimal input time before starting the submission. After the wait, define the submission section by calling start and end submission.","type":"text"}]},{"syntax":"c","type":"codeListing","code":["void my_engine_render_new_frame(my_engine *engine) {","\t\/\/ ... query frame, predict timing, update frame contents.","","\t\/\/ Wait until the optimal time for querying the input","\tcp_time_wait_until(cp_frame_timing_get_optimal_input_time(timing));","","\t\/\/ ...","}"]},{"type":"paragraph","inlineContent":[{"text":"Inside this section, first query the drawable object. Similar to CAMetalLayer, the drawable object contains the target texture and the information that we will need to set up the render pipeline. Now that we have our drawable, we can get the final timing information that Compositor will use to render this frame. With the final timing, we can query the ar_pose.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"It is important to set the pose in the drawable since it will be used by the Compositor to perform reprojection on the frame. Here I’m getting the pose by calling the get_ar_pose function in my engine object. But we will need to implement the contents of this function using the ARKit world tracking APIs. The last step of the function will be to encode all the GPU work and submit the frame.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Inside the submit_frame, use the drawable to render the contents of the frame as usual."}]},{"syntax":"c","type":"codeListing","code":["void my_engine_render_new_frame(my_engine *engine) {","\t\/\/ ... query frame, predict timing, update frame contents.","\t\/\/ ... wait for optimal input time.","\t","\tcp_frame_ start_ submission(frame);","","\tcp_drawable_t drawable = cp_frame_query_drawable(frame);","\tif (drawable == nullptr) { return; }","\t","\tcp_frame_timing_t final_timing = cp_drawable_get_frame_timing(drawable);","\tar_pose_t pose = my_engine_get_ar_pose(engine, final_timing);","\tcp_drawable_set_ar_pose(drawable, pose);","","\tmy_engine_draw_and_submit_ _frame (engine, frame, drawable);","\t","\tcp_frame_end_submission(frame);","}"]},{"level":1,"type":"heading","anchor":"User-input","text":"User input"},{"type":"paragraph","inlineContent":[{"text":"Now that the render loop is rendering frames, it’s time to make our immersive experience interactive. This video shows how RecRoom using Unity is already taking advantage of the ARKit and Compositor APIs to add interactivity to their application.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"There are two main input sources driving this interaction. ARKit’s HandTracking is providing the hand skeleton to render virtual hands. And pinch events from the LayerRenderer are driving the user interactions."}]},{"type":"paragraph","inlineContent":[{"text":"In order to make the experience interactive, we’ll first gather the user input and then apply it to the contents of our scene. All this work will happen in the update section of the frame.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10089-metal16"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"There are two main input sources, the LayerRenderer and the ARKit HandTracking provider. With the LayerRenderer, we will get updates every time the application receives a pinch event. These updates are exposed in the form of spatial events. These events contains three main properties. The phase will tell we if the event is active, if it finished, or if it got canceled. The selection ray will allow we to determine the content of the scene that had the attention when the event began. And the last event property is the manipulator pose. This is the pose of the pinch and gets updated every frame for the duration of the event. From the HandTracking API, we will be able to get skeleton for both the left and right hands."}]},{"type":"paragraph","inlineContent":[{"text":"Now, it’s time to add input support in the code.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Before gathering the input, we will decide if our application is rendering virtual hands or if it uses passthrough hands. Add the upperLimbVisibility scene modifier to the ImmersiveSpace to make the passthrough hands visible or hidden."}]},{"syntax":"swift","type":"codeListing","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer (configuration: MyConfiguration ()) { layerRenderer in","\t\t\t\tlet engine = my_engine_create (layerRenderer)","\t\t\t\tlet renderThread = Thread {","\t\t\t\t\tmy_engine_render_loop(engine)","\t\t\t\t}","\t\t\t\trenderThread.name = \"Render Thread\"","\t\t\t\trenderThread.start()","\t\t\t}\t","\t\t}","\t\t.upperLimbVisibility(.hidden)","\t} ","}"]},{"type":"paragraph","inlineContent":[{"text":"To access the spatial events, go back to where we defined the CompositorLayer render handler. Here, register a block in the layerRenderer to get updates every time there is a new spatial event.","type":"text"}]},{"syntax":"swift","type":"codeListing","code":["@main","struct MyApp: App {","\tvar body: some Scene {","\t\tImmersiveSpace {","\t\t\tCompositorLayer (configuration: MyConfiguration ()) { layerRenderer in","\t\t\t\tlet engine = my_engine_create (layerRenderer)","\t\t\t\tlet renderThread = Thread {","\t\t\t\t\tmy_engine_render_loop(engine)","\t\t\t\t}","\t\t\t\trenderThread.name = \"Render Thread\"","\t\t\t\trenderThread.start()","\t\t\t\t","\t\t\t\t\/\/ add this","\t\t\t\tlayerRenderer.onSpatialEvent = { eventCollection in","\t\t\t\t\tvar events = eventCollection.map { my_spatial_event ($0) }","\t\t\t\t\tmy_engine_push_spatial_events(engine, &events, events.count)","\t\t\t\t}","","\t\t\t}\t","\t\t}","\t\t.upperLimbVisibility(.hidden)","\t} ","}"]},{"type":"paragraph","inlineContent":[{"text":"If we are writing our engine code in C, we’ll map the SwiftUI spatial event to a C type. Inside the C code, we can now receive the C event collection. One thing to keep in mind when handling the spatial event updates is that the updates get delivered in the main thread. This means that we will use some synchronization mechanism when reading and writing the events in our engine.","type":"text"}]},{"syntax":"c","type":"codeListing","code":["void my_engine_push_spatial_events(my_engine *engine,","\t\t\t\t\t\t\t\tmy_spatial_event *spatial_event_collection, size_t event_count) {","\tos_unfair_lock_lock(&engine-›input_event_lock);","","\t\/\/ Copy events into an internal queue","\t","\tos_unfair_lock_unlock(&engine->input_event_lock);","}"]},{"type":"paragraph","inlineContent":[{"text":"Now that the events are stored in the engine, it’s time to implement the gather input function. The first step is to create an object to store the current input state for this frame.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This input state will store the events that we received from the LayerRenderer. Make sure that we are accessing our internal storage in a safe way."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"As for the hand skeleton, we can use the hand tracking provider API from ARKit to get the latest hand anchors."}]},{"syntax":"c","type":"codeListing","code":["my_input_state my_engine_gather_inputs (my_engine *engine,cp_frame_timing_t timing) {","\t","\tmy_input_state input_state = my_input_state_create();","\t","\tos_unfair_lock_lock(&engine->input_event_lock);","\tinput_state.current_pinch_collection = my_engine_pop_spatial_events(engine);","\tos_unfair_lock_unlock(&engine->input_event_lock);","\t","\tar_hand_tracking_provider_get_latest_anchors(engine->hand_tracking_provider, input_state.left_hand, input_state.right_hand);","\t","\treturn input_state;","}"]},{"type":"paragraph","inlineContent":[{"text":"And now that our application has input support, we have all the tools at our disposal to create fully immersive experiences on xrOS.","type":"text"}]},{"level":2,"type":"heading","anchor":"Wrap-up","text":"Wrap up"},{"type":"paragraph","inlineContent":[{"type":"text","text":"To recap, with SwiftUI, we will define the application."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"With CompositorServices and Metal, we will set up the render loop and display 3D content."}]},{"type":"paragraph","inlineContent":[{"text":"And finally, with ARKit, we will be able to make our experience interactive.","type":"text"}]},{"level":2,"type":"heading","anchor":"Resources","text":"Resources"},{"type":"paragraph","inlineContent":[{"isActive":true,"identifier":"https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing_fully_immersive_content_using_metal","type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/forums\/create\/question?tag1=795030&tag2=763030&tag3=164","type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/documentation\/metal","type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10089","type":"reference"}]},{"level":2,"type":"heading","anchor":"Related-Videos","text":"Related Videos"},{"type":"paragraph","inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10096"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"overridingTitle":"Go beyond the window with SwiftUI - WWDC23","isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Go beyond the window with SwiftUI - WWDC23"}],"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"overridingTitle":"Meet ARKit for spatial computing - WWDC23","isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Meet ARKit for spatial computing - WWDC23"}],"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"overridingTitle":"Explore HDR rendering with EDR - WWDC21","isActive":true,"overridingTitleInlineContent":[{"text":"Explore HDR rendering with EDR - WWDC21","type":"text"}],"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161"}]},{"level":2,"type":"heading","anchor":"Written-By","text":"Written By"},{"numberOfColumns":5,"type":"row","columns":[{"content":[{"inlineContent":[{"type":"image","identifier":"multitudes"}],"type":"paragraph"}],"size":1},{"content":[{"level":3,"anchor":"laurent-b","text":"laurent b","type":"heading"},{"inlineContent":[{"isActive":true,"overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"overridingTitle":"Contributed Notes","type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/multitudes"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/x.com\/wrmultitudes"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/laurentbrusa.hashnode.dev\/"}],"type":"paragraph"}],"size":4}]},{"type":"paragraph","inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"level":2,"type":"heading","anchor":"Related-Sessions","text":"Related Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10096-Build-great-games-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10111-Go-beyond-the-window-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10161-Explore-HDR-rendering-with-EDR"],"type":"links","style":"list"},{"type":"small","inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}]},{"type":"small","inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}]}],"schemaVersion":{"patch":0,"minor":3,"major":0},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10089-discover-metal-for-immersive-apps"]}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10089-Discover-Metal-for-immersive-apps"},"sections":[],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"kind":"article","abstract":[{"type":"text","text":"Find out how you can use Metal to render fully immersive experiences for visionOS. We’ll show you how to set up a rendering session on the platform and create a basic render loop, and share how you can make your experience interactive by incorporating spatial input."}],"references":{"https://developer.apple.com/videos/play/wwdc2023/10082":{"titleInlineContent":[{"text":"Meet ARKit for spatial computing - WWDC23","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082","title":"Meet ARKit for spatial computing - WWDC23","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10082"},"WWDC23-10089-metal16":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal16.jpg"}],"identifier":"WWDC23-10089-metal16","alt":"Metal"},"WWDC23-10089-metal":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal.jpg"}],"identifier":"WWDC23-10089-metal","alt":"Metal"},"WWDC23-10089-metal14":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal14.jpg"}],"identifier":"WWDC23-10089-metal14","alt":"Metal"},"https://developer.apple.com/videos/play/wwdc2023/10111":{"titleInlineContent":[{"text":"Go beyond the window with SwiftUI - WWDC23","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111","title":"Go beyond the window with SwiftUI - WWDC23","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10111"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=1048":{"titleInlineContent":[{"text":"17:28 - User input","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1048","title":"17:28 - User input","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1048"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=110":{"titleInlineContent":[{"text":"1:50 - App Architecture","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=110","title":"1:50 - App Architecture","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=110"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10082-Meet-ARKit-for-spatial-computing":{"kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10082-meet-arkit-for-spatial-computing","title":"Meet ARKit for spatial computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","abstract":[{"text":"Discover how you can use ARKit’s tracking and scene understanding features to develop a whole new universe of immersive apps and games. Learn how visionOS and ARKit work together to help you create apps that understand a person’s surroundings — all while preserving privacy. Explore the latest updates to the ARKit API and follow along as we demonstrate how to take advantage of hand tracking and scene geometry in your apps.","type":"text"}],"type":"topic"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=780":{"titleInlineContent":[{"text":"13:00 - Render a frame","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=780","title":"13:00 - Render a frame","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=780"},"multitudes":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/multitudes.jpeg"}],"identifier":"multitudes","alt":"Profile image of laurent b"},"WWDC23-10089-metal7":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal7.jpg"}],"identifier":"WWDC23-10089-metal7","alt":"Metal"},"WWDC23-10089-metal5":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal5.jpg"}],"identifier":"WWDC23-10089-metal5","alt":"Metal"},"WWDC23-10089-metal9":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal9.jpg"}],"identifier":"WWDC23-10089-metal9","alt":"Metal"},"doc://WWDCNotes/documentation/WWDCNotes":{"title":"WWDC Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","type":"topic","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"url":"\/documentation\/wwdcnotes","role":"collection","kind":"symbol","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}]},"WWDC23-10089-metal2":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal2.jpg"}],"identifier":"WWDC23-10089-metal2","alt":"Metal"},"WWDC23-10089-recRoom":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-recRoom.jpg"}],"identifier":"WWDC23-10089-recRoom","alt":"RecRoom"},"WWDC23.jpeg":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23.jpeg"}],"identifier":"WWDC23.jpeg","alt":null},"https://developer.apple.com/documentation/compositorservices/drawing_fully_immersive_content_using_metal":{"titleInlineContent":[{"text":"Drawing fully immersive content using Metal","type":"text"}],"url":"https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing_fully_immersive_content_using_metal","title":"Drawing fully immersive content using Metal","type":"link","identifier":"https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing_fully_immersive_content_using_metal"},"https://developer.apple.com/forums/tags/wwdc2023-10089":{"titleInlineContent":[{"text":"Search the forums for tag wwdc2023-10089","type":"text"}],"url":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10089","title":"Search the forums for tag wwdc2023-10089","type":"link","identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10089"},"WWDC23-10089-metal8":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal8.jpg"}],"identifier":"WWDC23-10089-metal8","alt":"Metal"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=6":{"titleInlineContent":[{"text":"0:06 - Intro","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=6","title":"0:06 - Intro","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=6"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"title":"WWDC23","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","type":"topic","images":[{"type":"icon","identifier":"WWDC23-Icon.png"},{"type":"card","identifier":"WWDC23.jpeg"}],"url":"\/documentation\/wwdcnotes\/wwdc23","role":"collectionGroup","kind":"article","abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"code":"SwiftData","type":"codeVoice"},{"type":"text","text":", "},{"code":"Observation","type":"codeVoice"},{"type":"text","text":", "},{"code":"StoreKit","type":"codeVoice"},{"type":"text","text":" views, and more."}]},"https://x.com/wrmultitudes":{"titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"url":"https:\/\/x.com\/wrmultitudes","title":"X\/Twitter","type":"link","identifier":"https:\/\/x.com\/wrmultitudes"},"WWDC23-10089-metal12":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal12.jpg"}],"identifier":"WWDC23-10089-metal12","alt":"Metal"},"https://developer.apple.com/forums/create/question?tag1=795030&tag2=763030&tag3=164":{"titleInlineContent":[{"text":"Have a question? Ask with tag wwdc2023-10089","type":"text"}],"url":"https:\/\/developer.apple.com\/forums\/create\/question?tag1=795030&tag2=763030&tag3=164","title":"Have a question? Ask with tag wwdc2023-10089","type":"link","identifier":"https:\/\/developer.apple.com\/forums\/create\/question?tag1=795030&tag2=763030&tag3=164"},"WWDC23-10089-metal6":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal6.jpg"}],"identifier":"WWDC23-10089-metal6","alt":"Metal"},"https://github.com/multitudes":{"titleInlineContent":[{"text":"GitHub","type":"text"}],"url":"https:\/\/github.com\/multitudes","title":"GitHub","type":"link","identifier":"https:\/\/github.com\/multitudes"},"WWDC23-10089-metal15":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal15.jpg"}],"identifier":"WWDC23-10089-metal15","alt":"Metal"},"WWDCNotes.png":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"identifier":"WWDCNotes.png","alt":null},"WWDC23-10089-metal11":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal11.jpg"}],"identifier":"WWDC23-10089-metal11","alt":"Metal"},"WWDC23-Icon.png":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-Icon.png"}],"identifier":"WWDC23-Icon.png","alt":null},"WWDC23-10089-metal1":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal1.jpg"}],"identifier":"WWDC23-10089-metal1","alt":"Metal"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=689":{"titleInlineContent":[{"text":"11:29 - Render Loop","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=689","title":"11:29 - Render Loop","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=689"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=1222":{"titleInlineContent":[{"text":"20:22 - Wrap-Up","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1222","title":"20:22 - Wrap-Up","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=1222"},"https://developer.apple.com/wwdc23/10089":{"checksum":null,"url":"https:\/\/developer.apple.com\/wwdc23\/10089","type":"download","identifier":"https:\/\/developer.apple.com\/wwdc23\/10089"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10161-Explore-HDR-rendering-with-EDR":{"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc21-10161-explore-hdr-rendering-with-edr","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10161-Explore-HDR-rendering-with-EDR","title":"Explore HDR rendering with EDR","abstract":[{"type":"text","text":"EDR is Apple’s High Dynamic Range representation and rendering pipeline. Explore how you can render HDR content using EDR in your app and unleash the dynamic range capabilities of your HDR display including Apple’s internal displays and Pro Display XDR."}],"type":"topic"},"WWDC23-10089-metal4":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal4.jpg"}],"identifier":"WWDC23-10089-metal4","alt":"Metal"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10096-Build-great-games-for-spatial-computing":{"title":"Build great games for spatial computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10096-Build-great-games-for-spatial-computing","type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10096-build-great-games-for-spatial-computing","kind":"article","abstract":[{"type":"text","text":"Find out how you can develop great gaming experiences for visionOS. We’ll share some of the key building blocks that help you create games for this platform, explore how your experiences can fluidly move between levels of immersion, and provide a roadmap for exploring ARKit, RealityKit, Reality Composer Pro, Unity, Metal, and Compositor."}]},"WWDC23-10089-metal3":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal3.jpg"}],"identifier":"WWDC23-10089-metal3","alt":"Metal"},"https://laurentbrusa.hashnode.dev/":{"titleInlineContent":[{"text":"Blog","type":"text"}],"url":"https:\/\/laurentbrusa.hashnode.dev\/","title":"Blog","type":"link","identifier":"https:\/\/laurentbrusa.hashnode.dev\/"},"https://developer.apple.com/videos/play/wwdc2023/10089/?time=358":{"titleInlineContent":[{"text":"5:58 - Render configuration","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=358","title":"5:58 - Render configuration","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10089\/?time=358"},"https://developer.apple.com/documentation/metal":{"titleInlineContent":[{"text":"Metal","type":"text"}],"url":"https:\/\/developer.apple.com\/documentation\/metal","title":"Metal","type":"link","identifier":"https:\/\/developer.apple.com\/documentation\/metal"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","type":"link","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"doc://WWDCNotes/documentation/WWDCNotes/multitudes":{"type":"topic","title":"laurent b (33 notes)","url":"\/documentation\/wwdcnotes\/multitudes","role":"sampleCode","images":[{"type":"card","identifier":"multitudes.jpeg"},{"type":"icon","identifier":"multitudes.jpeg"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes","abstract":[{"type":"text","text":"student at 42Berlin 🐬 | 🍎 Swift(UI) app dev  | speciality coffee ☕️ & cycling 🚴🏻‍♂️"}]},"WWDC23-10089-metal10":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal10.jpg"}],"identifier":"WWDC23-10089-metal10","alt":"Metal"},"WWDC23-10089-metal13":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10089-metal13.jpg"}],"identifier":"WWDC23-10089-metal13","alt":"Metal"},"multitudes.jpeg":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/multitudes.jpeg"}],"identifier":"multitudes.jpeg","alt":null},"https://developer.apple.com/videos/play/wwdc2023/10096":{"titleInlineContent":[{"text":"Build great games for spatial computing - WWDC23","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10096","title":"Build great games for spatial computing - WWDC23","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10096"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10111-Go-beyond-the-window-with-SwiftUI":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10111-Go-beyond-the-window-with-SwiftUI","title":"Go beyond the window with SwiftUI","url":"\/documentation\/wwdcnotes\/wwdc23-10111-go-beyond-the-window-with-swiftui","role":"sampleCode","kind":"article","abstract":[{"text":"Get ready to launch into space — a new SwiftUI scene type that can help you make great immersive experiences for visionOS. We’ll show you how to create a new scene with ImmersiveSpace, place 3D content, and integrate RealityView. Explore how you can use the immersionStyle scene modifier to increase the level of immersion in an app and learn best practices for managing spaces, adding virtual hands with ARKit, adding support for SharePlay, and building an “out of this world” experience!","type":"text"}],"type":"topic"},"https://developer.apple.com/videos/play/wwdc2021/10161":{"titleInlineContent":[{"text":"Explore HDR rendering with EDR - WWDC21","type":"text"}],"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161","title":"Explore HDR rendering with EDR - WWDC21","type":"link","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10161"}}}