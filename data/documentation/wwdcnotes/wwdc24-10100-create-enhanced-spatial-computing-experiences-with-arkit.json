{"kind":"article","abstract":[{"type":"text","text":"Learn how to create captivating immersive experiences with ARKit’s latest features. Explore ways to use room tracking and object tracking to further engage with your surroundings. We’ll also share how your app can react to changes in your environment’s lighting on this platform. Discover improvements in hand tracking and plane detection which can make your spatial experiences more intuitive."}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc24-10100-create-enhanced-spatial-computing-experiences-with-arkit"]}],"primaryContentSections":[{"content":[{"level":2,"type":"heading","text":"Recap","anchor":"Recap"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"When app in “Full Space”, you can use ARKit to get “anchors”","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Anchor is a position and orientation in 3D space, such as "},{"code":"PlaneAnchor","type":"codeVoice"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Anchors provided through data providers, such as "},{"code":"PlaneDetectionProvider","type":"codeVoice"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Providers are run in an "},{"type":"codeVoice","code":"ARKitSession"}],"type":"paragraph"}]}]},{"inlineContent":[{"identifier":"WWDC24-10100-ARKitSession","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Learn more at least year’s session "},{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","type":"reference","isActive":true},{"type":"text","text":"."}],"type":"paragraph"},{"level":2,"type":"heading","text":"Room tracking","anchor":"Room-tracking"},{"inlineContent":[{"text":"ARKit detects the boundaries of the room:","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC24-10100-Room-Boundaries","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"It also recognizes transitions between rooms (when walking throught the door). Apps can provide different experiences per room.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Use the new "},{"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/roomtrackingprovider","isActive":true},{"type":"text","text":" which requires world sensing authorization. The available properties are "},{"code":"currentRoomAnchor","type":"codeVoice"},{"type":"text","text":" and "},{"code":"anchorUpdates","type":"codeVoice"},{"type":"text","text":" to get a stream of "},{"code":"RoomAnchor","type":"codeVoice"},{"type":"text","text":" changes."}],"type":"paragraph"},{"inlineContent":[{"text":"A ","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/roomanchor","type":"reference"},{"text":" has the properties ","type":"text"},{"code":"isCurrentRoom","type":"codeVoice"},{"text":", ","type":"text"},{"code":"geometry","type":"codeVoice"},{"text":"\/","type":"text"},{"code":"geometries(of classification:)","type":"codeVoice"},{"text":", ","type":"text"},{"code":"contains(_ point:)","type":"codeVoice"},{"text":", ","type":"text"},{"code":"planeAnchorIDs","type":"codeVoice"},{"text":" and ","type":"text"},{"code":"meshAnchorIDs","type":"codeVoice"},{"text":".","type":"text"}],"type":"paragraph"},{"level":2,"type":"heading","text":"Plane detection","anchor":"Plane-detection"},{"inlineContent":[{"text":"ARKit detects planes and surfaces as ","type":"text"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/planeanchor","isActive":true},{"text":"s.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"These are useful for placing virtual content around surfaces (like a board game on a table) or walls (like "},{"type":"reference","isActive":true,"identifier":"https:\/\/apps.apple.com\/us\/app\/posters-discover-movies-home\/id6478062053"},{"type":"text","text":" on the wall)."}],"type":"paragraph"},{"inlineContent":[{"text":"The detected surface types are:","type":"text"}],"type":"paragraph"},{"type":"tabNavigator","tabs":[{"title":"Horizontal surfaces","content":[{"inlineContent":[{"type":"image","identifier":"WWDC24-10100-Horizontal-Surfaces"}],"type":"paragraph"}]},{"title":"Vertical surfaces","content":[{"inlineContent":[{"type":"image","identifier":"WWDC24-10100-Vertical-Surfaces"}],"type":"paragraph"}]},{"title":"Angled surfaces (New)","content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC24-10100-Angled-Surfaces"}]}]}]},{"inlineContent":[{"text":"To get the new slanted (angled) plane anchors, include ","type":"text"},{"type":"codeVoice","code":".slanted"},{"text":" in the ","type":"text"},{"type":"codeVoice","code":"alignments"},{"text":" parameters like so:","type":"text"}],"type":"paragraph"},{"syntax":"swift","code":["let planeDetection = PlaneDetectionProvider(alignments: [.horizontal, .vertical, .slanted])"],"type":"codeListing"},{"level":2,"type":"heading","text":"Object tracking","anchor":"Object-tracking"},{"inlineContent":[{"text":"ARKit can now give you the position and orientation of objects in your environment to anchor virtual content to them.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"A USDZ-formatted 3D object needs to be converted to a reference object using "},{"type":"reference","identifier":"https:\/\/developer.apple.com\/machine-learning\/create-ml\/","isActive":true},{"type":"text","text":" which then gets passed to ARKit to detect the objects you want. Learn more in the session "},{"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10101-Explore-object-tracking-for-visionOS","isActive":true},{"type":"text","text":"."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Once this is done, you can pass the reference object to ARKit like this:"}],"type":"paragraph"},{"syntax":"swift","code":["Task {","    do {","        let url = URL(fileURLWithPath: \"\/path\/to\/globe.referenceobject\")","        let referenceObject = try await ReferenceObject(from: url)","        let objectTracking = ObjectTrackingProvider(referenceObjects: [referenceObject])","    } catch {","        \/\/ Handle reference object loading error.","    }","    ...","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"You’ll receive detected objects as "},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/objectanchor"},{"type":"text","text":" instances which have a "},{"type":"codeVoice","code":"boundingBox"},{"type":"text","text":" property of type "},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/objectanchor\/axisalignedboundingbox"},{"type":"text","text":" containing the coordinates."}],"type":"paragraph"},{"level":2,"type":"heading","text":"World tracking","anchor":"World-tracking"},{"inlineContent":[{"type":"text","text":"The sensors of the device detect world conditions such as low lighting and provides warnings about limited capabilities. Now, apps can hook into these world conditions via the new "},{"type":"codeVoice","code":"worldTrackingLimitations"},{"type":"text","text":" SwiftUI Environment value like so:"}],"type":"paragraph"},{"syntax":"swift","code":["struct WellPreparedView: View {","    @Environment(\\.worldTrackingLimitations) var worldTrackingLimitations","    ","    var body: some View {","        ..."," ","        .onChange(of: worldTrackingLimitations) {","            if worldTrackingLimitations.contains(.translation) {","                \/\/ Rearrange content when anchored positions are unavailable.","            }","        }","    }","}"],"type":"codeListing"},{"inlineContent":[{"text":"In ARKit, the ","type":"text"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/deviceanchor","isActive":true},{"text":" now holds a ","type":"text"},{"type":"codeVoice","code":"trackingState"},{"text":" property.","type":"text"}],"type":"paragraph"},{"level":2,"type":"heading","text":"Hand tracking","anchor":"Hand-tracking"},{"inlineContent":[{"type":"text","text":"New this year: The "},{"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/handanchor","isActive":true},{"type":"text","text":" now updates with a higher rate."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Because there’s still some delay, ARKit will now try to predict future hand anchor positions and provide them in real-time, which is also available in RealityKit."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"There’s a new "},{"identifier":"https:\/\/developer.apple.com\/documentation\/compositorservices\/layerrenderer\/frame\/timing\/4439315-trackableanchortime","type":"reference","isActive":true},{"type":"text","text":" propertly on the "},{"type":"codeVoice","code":"LayerRenderer"},{"type":"text","text":" which you can use to get ARKit to provide forward prediction like so:"}],"type":"paragraph"},{"syntax":"swift","code":["func submitFrame(_ frame: LayerRenderer.Frame) {","    ...","","    guard let drawable = frame.queryDrawable() else { return }","","    \/\/ Get the trackable anchor time to target.","    let trackableAnchorTime = drawable.frameTiming.trackableAnchorTime","","    \/\/ Convert the timestamp into units of seconds.","    let anchorPredictionTime = LayerRenderer.Clock.Instant.epoch","      .duration(to: trackableAnchorTime).timeInterval  ","","    \/\/ Predict hand anchors for the time that provides best content registration.","    let (leftHand, rightHand) = handTracking.handAnchors(at: anchorPredictionTime)","    ","    ...","}"],"type":"codeListing"},{"type":"video","identifier":"WWDC24-10100-Hand-tracking"},{"name":"Warning","style":"warning","type":"aside","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"This is a low-latency prediction, so it may be somewhat inaccurate."}]}]},{"inlineContent":[{"type":"text","text":"To learn more about rendering with compositor services, watch "},{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10092-Render-Metal-with-passthrough-in-visionOS","isActive":true,"type":"reference"},{"type":"text","text":" and "},{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","isActive":true,"type":"reference"},{"type":"text","text":"."}],"type":"paragraph"},{"inlineContent":[{"text":"In RealityKit, pass the new ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/trackingmode-swift.struct\/predicted"},{"text":" tracking mode instead of the slower but more accurate ","type":"text"},{"type":"codeVoice","code":"continuuous"},{"text":" mode to an ","type":"text"},{"type":"codeVoice","code":"AnchorEntity"},{"text":" for a lower-latency prediction.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"A detailed RalityKit example is provided in "},{"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit","isActive":true},{"type":"text","text":"."}],"type":"paragraph"},{"level":2,"type":"heading","text":"Written By","anchor":"Written-By"},{"type":"row","numberOfColumns":5,"columns":[{"content":[{"inlineContent":[{"type":"image","identifier":"Jeehut"}],"type":"paragraph"}],"size":1},{"content":[{"level":3,"text":"Cihat Gündüz","type":"heading","anchor":"Cihat-G%C3%BCnd%C3%BCz"},{"inlineContent":[{"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"isActive":true,"overridingTitle":"Contributed Notes","type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/Jeehut"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/Jeehut"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/fline.dev"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/x.com\/Jeehut"}],"type":"paragraph"}],"size":4}]},{"type":"thematicBreak"},{"inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}],"type":"paragraph"},{"level":2,"type":"heading","text":"Related Sessions","anchor":"Related-Sessions"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10092-Render-Metal-with-passthrough-in-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10101-Explore-object-tracking-for-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing"]},{"inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2025 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}],"kind":"content"}],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10100-Create-enhanced-spatial-computing-experiences-with-ARKit","interfaceLanguage":"swift"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24"]]},"schemaVersion":{"major":0,"patch":0,"minor":3},"sections":[],"metadata":{"roleHeading":"WWDC24","role":"sampleCode","modules":[{"name":"WWDC Notes"}],"title":"Create enhanced spatial computing experiences with ARKit"},"sampleCodeDownload":{"action":{"isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc24\/10100","type":"reference","overridingTitle":"Watch Video (15 min)"},"kind":"sampleDownload"},"references":{"https://developer.apple.com/documentation/compositorservices/layerrenderer/frame/timing/4439315-trackableanchortime":{"title":"trackableAnchorTime","url":"https:\/\/developer.apple.com\/documentation\/compositorservices\/layerrenderer\/frame\/timing\/4439315-trackableanchortime","identifier":"https:\/\/developer.apple.com\/documentation\/compositorservices\/layerrenderer\/frame\/timing\/4439315-trackableanchortime","titleInlineContent":[{"type":"codeVoice","code":"trackableAnchorTime"}],"type":"link"},"WWDC24-10100-Angled-Surfaces":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC24-10100-Angled-Surfaces.jpeg","traits":["1x","light"]}],"type":"image","identifier":"WWDC24-10100-Angled-Surfaces","alt":null},"https://developer.apple.com/machine-learning/create-ml/":{"url":"https:\/\/developer.apple.com\/machine-learning\/create-ml\/","type":"link","title":"Create ML","identifier":"https:\/\/developer.apple.com\/machine-learning\/create-ml\/","titleInlineContent":[{"text":"Create ML","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10101-Explore-object-tracking-for-visionOS":{"kind":"article","title":"Explore object tracking for visionOS","url":"\/documentation\/wwdcnotes\/wwdc24-10101-explore-object-tracking-for-visionos","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10101-Explore-object-tracking-for-visionOS","type":"topic","role":"sampleCode","abstract":[{"type":"text","text":"Find out how you can use object tracking to turn real-world objects into virtual anchors in your visionOS app. Learn how you can build spatial experiences with object tracking from start to finish. Find out how to create a reference object using machine learning in Create ML and attach content relative to your target object in Reality Composer Pro, RealityKit or ARKit APIs."}]},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Contributions are welcome!","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"type":"link"},"Jeehut.jpeg":{"variants":[{"url":"\/images\/WWDCNotes\/Jeehut.jpeg","traits":["1x","light"]}],"type":"image","identifier":"Jeehut.jpeg","alt":null},"https://developer.apple.com/documentation/arkit/planeanchor":{"url":"https:\/\/developer.apple.com\/documentation\/arkit\/planeanchor","type":"link","title":"PlaneAnchor","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/planeanchor","titleInlineContent":[{"code":"PlaneAnchor","type":"codeVoice"}]},"https://apps.apple.com/us/app/posters-discover-movies-home/id6478062053":{"title":"virtual posters","url":"https:\/\/apps.apple.com\/us\/app\/posters-discover-movies-home\/id6478062053","identifier":"https:\/\/apps.apple.com\/us\/app\/posters-discover-movies-home\/id6478062053","titleInlineContent":[{"type":"text","text":"virtual posters"}],"type":"link"},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","role":"collection","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","kind":"symbol","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC Notes","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"url":"\/documentation\/wwdcnotes"},"doc://WWDCNotes/documentation/WWDCNotes/Jeehut":{"title":"Cihat Gündüz (67 notes)","abstract":[{"type":"text","text":"Spatial-first Indie Developer for  Platforms. Actively contributing to Open Source since 2011!"}],"kind":"article","role":"sampleCode","type":"topic","images":[{"identifier":"Jeehut.jpeg","type":"card"},{"identifier":"Jeehut.jpeg","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/Jeehut","url":"\/documentation\/wwdcnotes\/jeehut"},"WWDC24-10100-Vertical-Surfaces":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC24-10100-Vertical-Surfaces.jpeg","traits":["1x","light"]}],"type":"image","identifier":"WWDC24-10100-Vertical-Surfaces","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10082-Meet-ARKit-for-spatial-computing":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10082-meet-arkit-for-spatial-computing","abstract":[{"type":"text","text":"Discover how you can use ARKit’s tracking and scene understanding features to develop a whole new universe of immersive apps and games. Learn how visionOS and ARKit work together to help you create apps that understand a person’s surroundings — all while preserving privacy. Explore the latest updates to the ARKit API and follow along as we demonstrate how to take advantage of hand tracking and scene geometry in your apps."}],"title":"Meet ARKit for spatial computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","role":"sampleCode"},"WWDC24-10100-ARKitSession":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC24-10100-ARKitSession.jpeg","traits":["1x","light"]}],"identifier":"WWDC24-10100-ARKitSession","alt":null},"Jeehut":{"alt":"Profile image of Cihat Gündüz","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/Jeehut.jpeg"}],"identifier":"Jeehut","type":"image"},"https://x.com/Jeehut":{"type":"link","titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"identifier":"https:\/\/x.com\/Jeehut","title":"X\/Twitter","url":"https:\/\/x.com\/Jeehut"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24":{"images":[{"identifier":"WWDC24-Icon.png","type":"icon"},{"identifier":"WWDC24.jpeg","type":"card"}],"role":"collectionGroup","url":"\/documentation\/wwdcnotes\/wwdc24","type":"topic","abstract":[{"text":"Xcode 16, Swift 6, iOS 18, macOS 15 (Sequoia), tvOS 18, visionOS 2, watchOS 11.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: Swift Testing, ","type":"text"},{"code":"FinanceKit","type":"codeVoice"},{"text":", ","type":"text"},{"code":"TabletopKit","type":"codeVoice"},{"text":", and more.","type":"text"}],"title":"WWDC24","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24"},"WWDCNotes.png":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"identifier":"WWDCNotes.png","alt":null},"WWDC24-10100-Hand-tracking":{"alt":null,"poster":null,"variants":[{"traits":["1x","light"],"url":"\/videos\/WWDCNotes\/WWDC24-10100-Hand-tracking.mp4"}],"identifier":"WWDC24-10100-Hand-tracking","type":"video"},"https://developer.apple.com/documentation/arkit/objectanchor/axisalignedboundingbox":{"type":"link","titleInlineContent":[{"code":"AxesAlignedBoundingBox","type":"codeVoice"}],"identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/objectanchor\/axisalignedboundingbox","title":"AxesAlignedBoundingBox","url":"https:\/\/developer.apple.com\/documentation\/arkit\/objectanchor\/axisalignedboundingbox"},"https://developer.apple.com/documentation/arkit/roomtrackingprovider":{"url":"https:\/\/developer.apple.com\/documentation\/arkit\/roomtrackingprovider","type":"link","title":"RoomTrackingDataProvider","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/roomtrackingprovider","titleInlineContent":[{"code":"RoomTrackingDataProvider","type":"codeVoice"}]},"https://developer.apple.com/documentation/arkit/objectanchor":{"title":"ObjectAnchor","url":"https:\/\/developer.apple.com\/documentation\/arkit\/objectanchor","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/objectanchor","titleInlineContent":[{"type":"codeVoice","code":"ObjectAnchor"}],"type":"link"},"https://developer.apple.com/documentation/arkit/handanchor":{"type":"link","titleInlineContent":[{"code":"HandAnchor","type":"codeVoice"}],"identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/handanchor","title":"HandAnchor","url":"https:\/\/developer.apple.com\/documentation\/arkit\/handanchor"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit":{"type":"topic","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit","kind":"article","title":"Build a spatial drawing app with RealityKit","abstract":[{"type":"text","text":"Harness the power of RealityKit through the process of building a spatial drawing app. As you create an eye-catching spatial experience that integrates RealityKit with ARKit and SwiftUI, you’ll explore how resources work in RealityKit and how to use features like low-level mesh and texture APIs to achieve fast updates of the users’ brush strokes."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10104-build-a-spatial-drawing-app-with-realitykit"},"https://github.com/Jeehut":{"url":"https:\/\/github.com\/Jeehut","type":"link","title":"GitHub","identifier":"https:\/\/github.com\/Jeehut","titleInlineContent":[{"text":"GitHub","type":"text"}]},"WWDC24.jpeg":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC24.jpeg"}],"identifier":"WWDC24.jpeg","type":"image"},"WWDC24-10100-Horizontal-Surfaces":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC24-10100-Horizontal-Surfaces.jpeg","traits":["1x","light"]}],"type":"image","identifier":"WWDC24-10100-Horizontal-Surfaces","alt":null},"WWDC24-Icon.png":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC24-Icon.png","traits":["1x","light"]}],"identifier":"WWDC24-Icon.png","alt":null},"https://developer.apple.com/documentation/arkit/deviceanchor":{"title":"DeviceAnchor","url":"https:\/\/developer.apple.com\/documentation\/arkit\/deviceanchor","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/deviceanchor","titleInlineContent":[{"type":"codeVoice","code":"DeviceAnchor"}],"type":"link"},"https://fline.dev":{"type":"link","titleInlineContent":[{"text":"Blog","type":"text"}],"identifier":"https:\/\/fline.dev","title":"Blog","url":"https:\/\/fline.dev"},"https://developer.apple.com/documentation/realitykit/anchoringcomponent/trackingmode-swift.struct/predicted":{"url":"https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/trackingmode-swift.struct\/predicted","type":"link","title":"predicted","identifier":"https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/trackingmode-swift.struct\/predicted","titleInlineContent":[{"code":"predicted","type":"codeVoice"}]},"https://developer.apple.com/documentation/arkit/roomanchor":{"title":"RoomAnchor","url":"https:\/\/developer.apple.com\/documentation\/arkit\/roomanchor","identifier":"https:\/\/developer.apple.com\/documentation\/arkit\/roomanchor","titleInlineContent":[{"type":"codeVoice","code":"RoomAnchor"}],"type":"link"},"https://developer.apple.com/wwdc24/10100":{"type":"download","checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc24\/10100","url":"https:\/\/developer.apple.com\/wwdc24\/10100"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10092-Render-Metal-with-passthrough-in-visionOS":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10092-Render-Metal-with-passthrough-in-visionOS","title":"Render Metal with passthrough in visionOS","abstract":[{"type":"text","text":"Get ready to extend your Metal experiences for visionOS. Learn best practices for integrating your rendered content with people’s physical environments with passthrough. Find out how to position rendered content to match the physical world, reduce latency with trackable anchor prediction, and more."}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc24-10092-render-metal-with-passthrough-in-visionos","type":"topic"},"WWDC24-10100-Room-Boundaries":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC24-10100-Room-Boundaries.jpeg"}],"identifier":"WWDC24-10100-Room-Boundaries","alt":null}}}