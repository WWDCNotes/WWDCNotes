{"kind":"article","hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc23-10045-detect-animal-poses-in-vision"],"traits":[{"interfaceLanguage":"swift"}]}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision"},"sampleCodeDownload":{"kind":"sampleDownload","action":{"overridingTitle":"Watch Video (13 min)","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10045","type":"reference","isActive":true}},"sections":[],"schemaVersion":{"major":0,"minor":3,"patch":0},"metadata":{"title":"Detect animal poses in Vision","role":"sampleCode","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC23"},"abstract":[{"type":"text","text":"Go beyond detecting cats and dogs in images. We‚Äôll show you how to use Vision to detect the individual joints and poses of these animals as well ‚Äî all in real time ‚Äî and share how you can enable exciting features like animal tracking for a camera app, creative embellishment on an animal photo, and more. We‚Äôll also explore other important enhancements to Vision and share best practices."}],"primaryContentSections":[{"content":[{"anchor":"overview","type":"heading","level":2,"text":"Overview"},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"anchor":"Related-Sessions","type":"heading","level":2,"text":"Related Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision"],"style":"list"}],"kind":"content"}],"references":{"doc://WWDCNotes/documentation/WWDCNotes":{"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"type":"topic","title":"WWDC Notes","kind":"symbol","url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"role":"collection"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶","type":"link","titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision":{"title":"Explore 3D body pose and person segmentation in Vision","url":"\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision","abstract":[{"text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We‚Äôll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","role":"sampleCode","kind":"article","type":"topic"},"WWDCNotes.png":{"identifier":"WWDCNotes.png","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDCNotes.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"kind":"article","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}],"images":[{"type":"icon","identifier":"WWDC23-Icon.png"},{"type":"card","identifier":"WWDC23.jpeg"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","role":"collectionGroup","title":"WWDC23","url":"\/documentation\/wwdcnotes\/wwdc23"},"WWDC23.jpeg":{"identifier":"WWDC23.jpeg","variants":[{"url":"\/images\/WWDCNotes\/WWDC23.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"https://developer.apple.com/videos/play/wwdc2023/10045":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10045","checksum":null,"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10045","type":"download"},"WWDC23-Icon.png":{"identifier":"WWDC23-Icon.png","type":"image","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC23-Icon.png","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","url":"\/documentation\/wwdcnotes\/wwdc23-10304-integrate-with-motorized-iphone-stands-using-dockkit","role":"sampleCode","abstract":[{"type":"text","text":"Discover how you can create incredible photo and video experiences in your camera app when integrating with DockKit-compatible motorized stands. We‚Äôll show how your app can automatically track subjects in live video across a 360-degree field of view, take direct control of the stand to customize framing, directly control the motors, and provide your own inference model for tracking other objects. Finally, we‚Äôll demonstrate how to create a sense of emotion through dynamic device animations."}],"type":"topic","kind":"article","title":"Integrate with motorized iPhone stands using DockKit"}}}