{"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc25-297-learn-about-the-apple-projected-media-profile"],"traits":[{"interfaceLanguage":"swift"}]}],"abstract":[{"text":"Dive into the Apple Projected Media Profile (APMP) and see how APMP enables 180º\/360º and Wide FoV projections in QuickTime and MP4 files using Video Extended Usage signaling. We’ll provide guidance on using OS-provided frameworks and tools to convert, read\/write, edit, and encode media containing APMP. And we’ll review Apple Positional Audio Codec’s (APAC) capabilities for creating and delivering spatial audio content for the most immersive experiences.","type":"text"}],"primaryContentSections":[{"kind":"content","content":[{"level":2,"anchor":"Prerequisites","type":"heading","text":"Prerequisites"},{"inlineContent":[{"type":"text","text":"Watch "},{"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-304-Explore-video-experiences-for-visionOS","isActive":true},{"type":"text","text":" first."}],"type":"paragraph"},{"level":2,"anchor":"Fundamentals","type":"heading","text":"Fundamentals"},{"inlineContent":[{"type":"image","identifier":"WWDC25-297-New-Video-Profiles"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Equirectangular projection supported by many tools including Final Cut Pro","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Pixel coordinates expressed as angles of lat\/long, projected into rows & columns","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"image","identifier":"WWDC25-297-Equirectangular-Projection"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"ParametricImmersive projection for fisheye with focal length, offset, and skew","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Additional radial distortion parameters may be needed (for wide angle lenses), also:"}]}]},{"content":[{"inlineContent":[{"text":"Tangential distortion, projection offset, radial angle limit, lens frame adjustment","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"image","identifier":"WWDC25-297-Radial-Distortion"}],"type":"paragraph"},{"level":2,"anchor":"Apple-Projected-Media-Profile","type":"heading","text":"Apple Projected Media Profile"},{"level":3,"anchor":"Specification","type":"heading","text":"Specification"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"APMP allows signaling 180, 360, and Wide FOV content in MOV\/MP4 files"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"VisionOS 1 had introduced “Video Extended Usage” box to MP4 (vexu)","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"VisionOS 26 adds projection signaling information to the vexu box","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"New lens collection box for intrinsics, extrinsics, and distortions (for parametric)","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"New view packing box defines eye frame packing (side-by-side, over-under)","type":"text"}]}]}],"type":"unorderedList"},{"tabs":[{"title":"Monoscopic 360º","content":[{"inlineContent":[{"identifier":"WWDC25-297-Video-Extended-Usage-Monoscopic-360","type":"image"}],"type":"paragraph"}]},{"title":"Stereosopic 180º","content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC25-297-Video-Extended-Usage-Stereoscopic-180"}]}]}],"type":"tabNavigator"},{"inlineContent":[{"type":"text","text":"Learn more in "},{"type":"reference","identifier":"https:\/\/developer.apple.com\/av-foundation\/Stereo-Video-ISOBMFF-Extensions-1-0.pdf","isActive":true},{"type":"text","text":"."}],"type":"paragraph"},{"level":3,"anchor":"Workflow","type":"heading","text":"Workflow"},{"items":[{"content":[{"inlineContent":[{"text":"Variety of cameras can capture APMP content, e.g. Canon, GoPro, and Insta 360 cameras","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Their editing software will receive updates later this year to export in APMP"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Use camera software for stitching, stabilization, etc. - export with spherical metadata"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Then use "},{"type":"codeVoice","code":"avconvert"},{"type":"text","text":" CLI tool \/ Finder action to convert from spherical video to APMP"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"identifier":"WWDC25-297-APMP-Production-Pipeline","type":"image"}],"type":"paragraph"},{"level":3,"anchor":"Convert","type":"heading","text":"Convert"},{"inlineContent":[{"type":"text","text":"Make a compatible VR video asset treated like an APMP file until video software has adopted it like this:"}],"type":"paragraph"},{"syntax":"swift","code":["import AVFoundation","","func wasConvertedFromSpherical(url: URL) -> Bool {","   let assetOptions = [AVURLAssetShouldParseExternalSphericalTagsKey: true]","   let urlAsset = AVURLAsset(url: url, options: assetOptions)","","   \/\/ simplified for sample, assume first video track","   let track = try await urlAsset.loadTracks(withMediaType: .video).first!","","   \/\/ Retrieve formatDescription from video track, simplified for sample assume first format description","   let formatDescription = try await videoTrack.load(.formatDescriptions).first","","   \/\/ Detect if formatDescription includes extensions synthesized from spherical","   let wasConvertedFromSpherical = formatDescription.extensions[.convertedFromExternalSphericalTags]","","   return wasConvertedFromSpherical","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"For a compatible Wide FOV video, convert like this:"}],"type":"paragraph"},{"syntax":"swift","code":["\/\/ Convert wide-FOV content from recognized camera models","import ImmersiveMediaSupport","","func upliftIntoParametricImmersiveIfPossible(url: URL) -> AVMutableMovie {","   let movie = AVMutableMovie(url: url)","","   let assetInfo = try await ParametricImmersiveAssetInfo(asset: movie)","   if (assetInfo.isConvertible) {","       guard let newDescription = assetInfo.requiredFormatDescription else {","               fatalError(\"no format description for convertible asset\")","       }","       let videoTracks = try await movie.loadTracks(withMediaType: .video)","       guard let videoTrack = videoTracks.first,","                 let currentDescription = try await videoTrack.load(.formatDescriptions).first","       else {","         fatalError(\"missing format description for video track\")","       }","       \/\/ presumes that format already compatible for intended use case (delivery or production)","       \/\/ for delivery then if not already HEVC should transcode for example","       videoTrack.replaceFormatDescription(currentDescription, with: newDescription)","   }","   return movie","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"With these conversions, system-wide APIs will recognize video as APMP content."}],"type":"paragraph"},{"inlineContent":[{"text":"Learn more in ","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/documentation\/avfoundation\/converting-projected-video-to-apple-projected-media-profile","type":"reference"},{"text":".","type":"text"}],"type":"paragraph"},{"level":3,"anchor":"Read","type":"heading","text":"Read"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"CoreMedia"},{"type":"text","text":" and "},{"type":"codeVoice","code":"AVFoundation"},{"type":"text","text":" now support projected media"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"To identify if an asset conforms to APMP profile, check options like this:","type":"text"}]}]}],"type":"unorderedList"},{"syntax":"swift","code":["\/\/ Determine if an asset contains any tracks with nonRectilinearVideo and if so, whether any are AIV","import AVFoundation","","func classifyProjectedMedia(movieURL: URL) async -> (containsNonRectilinearVideo: Bool, containsAppleImmersiveVideo: Bool) {","   let asset = AVMovie(url: movieURL)","   let assistant = AVAssetPlaybackAssistant(asset: asset)","   let options = await assistant.playbackConfigurationOptions","","   \/\/ Note contains(.nonRectilinearProjection) is true for both APMP & AIV, while contains(.appleImmersiveVideo) is true only for AIV","   return (options.contains(.nonRectilinearProjection), options.contains(.appleImmersiveVideo))","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"Learn more: "},{"type":"reference","isActive":true,"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-296-Support-immersive-video-playback-in-visionOS-apps"}],"type":"paragraph"},{"level":3,"anchor":"Edit","type":"heading","text":"Edit"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Use "},{"type":"codeVoice","code":"AVVideoComposition"},{"type":"text","text":" from "},{"type":"codeVoice","code":"AVFoundation"},{"type":"text","text":" and "},{"type":"codeVoice","code":"CMTaggedDynamicBuffer"}]}]},{"content":[{"inlineContent":[{"text":"These buffers are used to handle steoscopic content across different APIs","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"image","identifier":"WWDC25-297-CMTaggedDynamicBuffer"}],"type":"paragraph"},{"inlineContent":[{"text":"Author note: More code provided for new buffer-related APIs (watch video to learn more).","type":"text"}],"type":"paragraph"},{"level":3,"anchor":"Write","type":"heading","text":"Write"},{"inlineContent":[{"text":"Use ","type":"text"},{"code":"AVAssetWriterInput","type":"codeVoice"},{"text":" to pass compression properties, full example:","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC25-297-Write-APMP-File"}],"type":"paragraph"},{"level":3,"anchor":"Publish","type":"heading","text":"Publish"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"APMP delivery should be HEVC Main\/Main10 with color sampling 4:2:0 in Rec. 709 or P3-D65"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Monoscopic: 8K@30FPS \/ 5.7K@60FPS \/ 4K@90FPS, 10-bit: 120 Mbit, 8-bit: 100 Mbit"}]}]},{"content":[{"inlineContent":[{"text":"Stereoscopic: 4320x4320 per eye, 4K@30FPS \/ 2K@90FPS, 10-bit: 84 Mbit, 8-bit: 70 Mbit","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Bitrate depends on content, but recommended to stay under 150 Mbit\/sec peak","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Learn more about ","type":"text"},{"identifier":"https:\/\/developer.apple.com\/av-foundation\/HEVC-Stereo-Video-Profile.pdf","isActive":true,"type":"reference"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"identifier":"WWDC25-297-Recomended-Playback-Limits","type":"image"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"AVQT (Advanced Video Quality Tool) was updated to support spatial\/VR content"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Use for assessing quality of content and fine-tuning video encoding parameters","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"New features include quality assessment considering (half-)requirectangular projection"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"Learn more in "},{"isActive":true,"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10145-Evaluate-videos-with-the-Advanced-Video-Quality-Tool"},{"type":"text","text":" and "},{"isActive":true,"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10149-Whats-new-in-AVQT"},{"type":"text","text":"."}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/http-live-streaming"},{"text":" documentation updated for streaming APMP content","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/documentation\/http-live-streaming\/using-apple-s-http-live-streaming-hls-tools"},{"type":"text","text":" have also been updated to publish APMP media"}]}]},{"content":[{"inlineContent":[{"text":"Use ","type":"text"},{"type":"codeVoice","code":"#EXT-X-STREAM-INF"},{"text":" in HLS manifest with ","type":"text"},{"type":"codeVoice","code":"REQ-VIDEO-LAYOUT=\"CH-STEREO\/PROJ-HEQU\""}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Refer to ","type":"text"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/http-live-streaming\/hls-authoring-specification-for-apple-devices","isActive":true},{"text":" for the latest information and guidelines","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"level":2,"anchor":"Apple-Positional-Audio-Codec","type":"heading","text":"Apple Positional Audio Codec"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"APAC can encode ambisonic audio – technique for record\/mix\/play spatial audio","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Not tied to a specific speaker layout, encoded mathematically","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Array of microphones needed, signals transformed to spherical harmonic components","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"1st-order ambisonics uses 3 components: Front\/Back, Left\/Right, Up\/Down"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"2nd-order ambisonics uses 9, 3rd-order ambisonics use 16 for more more spatial resolution"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"APAC is recommended for APMP media files, plays on all platforms (except watchOS)"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"System encoder supports 1st, 2nd, and 3rd-order ambisonics, minimal code sample:"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"image","identifier":"WWDC25-297-Minimal-Ambisonics-Code"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Recommended bitrates range for 384 Mbit for 1st order, to 768 Mbit for 3rd order","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"APAC can be segmented and streamed via HLS","type":"text"}]}]}],"type":"unorderedList"},{"level":2,"anchor":"Written-By","type":"heading","text":"Written By"},{"numberOfColumns":5,"type":"row","columns":[{"size":1,"content":[{"inlineContent":[{"type":"image","identifier":"Jeehut"}],"type":"paragraph"}]},{"size":4,"content":[{"type":"heading","level":3,"anchor":"Cihat-G%C3%BCnd%C3%BCz","text":"Cihat Gündüz"},{"type":"paragraph","inlineContent":[{"isActive":true,"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/Jeehut","type":"reference","overridingTitle":"Contributed Notes","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"identifier":"https:\/\/github.com\/Jeehut","type":"reference"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"identifier":"https:\/\/fline.dev","type":"reference"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"identifier":"https:\/\/x.com\/Jeehut","type":"reference"}]}]}]},{"type":"thematicBreak"},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"}],"type":"paragraph"},{"level":2,"anchor":"Related-Sessions","type":"heading","text":"Related Sessions"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-304-Explore-video-experiences-for-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-296-Support-immersive-video-playback-in-visionOS-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10145-Evaluate-videos-with-the-Advanced-Video-Quality-Tool","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10149-Whats-new-in-AVQT","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-403-Learn-about-Apple-Immersive-Video-technologies","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-237-Whats-new-for-the-spatial-web"]},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2025 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}]}],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-297-Learn-about-the-Apple-Projected-Media-Profile","interfaceLanguage":"swift"},"schemaVersion":{"minor":3,"major":0,"patch":0},"sections":[],"metadata":{"modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC25","role":"sampleCode","title":"Learn about the Apple Projected Media Profile"},"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","overridingTitle":"Watch Video (19 min)","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2025\/297","isActive":true}},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25"]]},"kind":"article","references":{"https://x.com/Jeehut":{"identifier":"https:\/\/x.com\/Jeehut","titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"url":"https:\/\/x.com\/Jeehut","type":"link","title":"X\/Twitter"},"https://developer.apple.com/documentation/avfoundation/converting-projected-video-to-apple-projected-media-profile":{"identifier":"https:\/\/developer.apple.com\/documentation\/avfoundation\/converting-projected-video-to-apple-projected-media-profile","title":"this sample application","type":"link","url":"https:\/\/developer.apple.com\/documentation\/avfoundation\/converting-projected-video-to-apple-projected-media-profile","titleInlineContent":[{"text":"this sample application","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2025/297":{"type":"download","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2025\/297","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2025\/297","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10145-Evaluate-videos-with-the-Advanced-Video-Quality-Tool":{"abstract":[{"type":"text","text":"Learn how the Advanced Video Quality Tool (AVQT) can help you accurately assess the perceptual quality of your compressed video files. Utilizing the AVFoundation framework, AVQT supports a wide range of video formats, codecs, resolutions and frame-rates in both the SDR and HDR domains, which results in easy and efficient workflows — for example, no requirement to decode to a raw pixel format."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10145-Evaluate-videos-with-the-Advanced-Video-Quality-Tool","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc21-10145-evaluate-videos-with-the-advanced-video-quality-tool","role":"sampleCode","title":"Evaluate videos with the Advanced Video Quality Tool"},"WWDC25-297-Radial-Distortion":{"identifier":"WWDC25-297-Radial-Distortion","variants":[{"url":"\/images\/WWDCNotes\/WWDC25-297-Radial-Distortion.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"WWDC25-Icon.png":{"alt":null,"type":"image","identifier":"WWDC25-Icon.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC25-Icon.png"}]},"https://developer.apple.com/documentation/http-live-streaming/hls-authoring-specification-for-apple-devices":{"identifier":"https:\/\/developer.apple.com\/documentation\/http-live-streaming\/hls-authoring-specification-for-apple-devices","title":"this document","type":"link","url":"https:\/\/developer.apple.com\/documentation\/http-live-streaming\/hls-authoring-specification-for-apple-devices","titleInlineContent":[{"type":"text","text":"this document"}]},"Jeehut.jpeg":{"identifier":"Jeehut.jpeg","variants":[{"url":"\/images\/WWDCNotes\/Jeehut.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"WWDC25-297-Video-Extended-Usage-Monoscopic-360":{"alt":null,"type":"image","identifier":"WWDC25-297-Video-Extended-Usage-Monoscopic-360","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC25-297-Video-Extended-Usage-Monoscopic-360.jpeg"}]},"Jeehut":{"variants":[{"url":"\/images\/WWDCNotes\/Jeehut.jpeg","traits":["1x","light"]}],"identifier":"Jeehut","type":"image","alt":"Profile image of Cihat Gündüz"},"https://fline.dev":{"identifier":"https:\/\/fline.dev","titleInlineContent":[{"text":"Blog","type":"text"}],"url":"https:\/\/fline.dev","type":"link","title":"Blog"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC25-237-Whats-new-for-the-spatial-web":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc25-237-whats-new-for-the-spatial-web","role":"sampleCode","title":"What’s new for the spatial web","abstract":[{"text":"Discover the latest spatial features for the web on visionOS 26. We’ll cover how to display inline 3D models with the brand new HTML model element. And we’ll share powerful features, including model lighting, interactions, and animations. Learn how to embed newly supported immersive media on your web site, such as 360-degree video and Apple Immersive Video. And get a sneak peek at adding a custom environment to your web pages.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-237-Whats-new-for-the-spatial-web"},"WWDC25-297-Video-Extended-Usage-Stereoscopic-180":{"alt":null,"type":"image","identifier":"WWDC25-297-Video-Extended-Usage-Stereoscopic-180","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC25-297-Video-Extended-Usage-Stereoscopic-180.jpeg"}]},"WWDC25-297-Equirectangular-Projection":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC25-297-Equirectangular-Projection.jpeg","traits":["1x","light"]}],"identifier":"WWDC25-297-Equirectangular-Projection","type":"image","alt":null},"WWDC25-297-CMTaggedDynamicBuffer":{"identifier":"WWDC25-297-CMTaggedDynamicBuffer","variants":[{"url":"\/images\/WWDCNotes\/WWDC25-297-CMTaggedDynamicBuffer.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"WWDC25.jpg":{"alt":null,"type":"image","identifier":"WWDC25.jpg","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC25.jpg"}]},"https://github.com/Jeehut":{"identifier":"https:\/\/github.com\/Jeehut","title":"GitHub","type":"link","url":"https:\/\/github.com\/Jeehut","titleInlineContent":[{"type":"text","text":"GitHub"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC25-403-Learn-about-Apple-Immersive-Video-technologies":{"kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-403-Learn-about-Apple-Immersive-Video-technologies","abstract":[{"type":"text","text":"Explore the capabilities of Apple Immersive Video and Apple Spatial Audio Format technologies to create truly immersive experiences. Meet the new ImmersiveMediaSupport framework, which offers functionality to read and write the necessary metadata for enabling Apple Immersive Video. Learn guidelines for encoding and publishing Apple Immersive Video content in standalone files for playback or streaming via HLS."}],"url":"\/documentation\/wwdcnotes\/wwdc25-403-learn-about-apple-immersive-video-technologies","type":"topic","title":"Learn about Apple Immersive Video technologies"},"WWDC25-297-Write-APMP-File":{"identifier":"WWDC25-297-Write-APMP-File","variants":[{"url":"\/images\/WWDCNotes\/WWDC25-297-Write-APMP-File.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"https://developer.apple.com/documentation/http-live-streaming/using-apple-s-http-live-streaming-hls-tools":{"type":"link","identifier":"https:\/\/developer.apple.com\/documentation\/http-live-streaming\/using-apple-s-http-live-streaming-hls-tools","titleInlineContent":[{"type":"text","text":"HLS Tools"}],"url":"https:\/\/developer.apple.com\/documentation\/http-live-streaming\/using-apple-s-http-live-streaming-hls-tools","title":"HLS Tools"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC25":{"abstract":[{"type":"text","text":"Xcode 26, Swift 6.2, iOS\/macOS\/tvOS\/visionOS 26."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"code":"Foundation Models","type":"codeVoice"},{"type":"text","text":", "},{"code":"AlarmKit","type":"codeVoice"},{"type":"text","text":", "},{"code":"PermissionKit","type":"codeVoice"},{"type":"text","text":", and more."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25","url":"\/documentation\/wwdcnotes\/wwdc25","kind":"article","role":"collectionGroup","type":"topic","title":"WWDC25","images":[{"identifier":"WWDC25-Icon.png","type":"icon"},{"identifier":"WWDC25.jpg","type":"card"}]},"https://developer.apple.com/documentation/http-live-streaming":{"identifier":"https:\/\/developer.apple.com\/documentation\/http-live-streaming","title":"HTTP Live Streaming","type":"link","url":"https:\/\/developer.apple.com\/documentation\/http-live-streaming","titleInlineContent":[{"type":"text","text":"HTTP Live Streaming"}]},"WWDC25-297-Recomended-Playback-Limits":{"identifier":"WWDC25-297-Recomended-Playback-Limits","variants":[{"url":"\/images\/WWDCNotes\/WWDC25-297-Recomended-Playback-Limits.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"WWDC25-297-New-Video-Profiles":{"alt":"Table showcasing all supported media profiles, highlighting the new 180, 360, and Wide FOV columns","type":"image","identifier":"WWDC25-297-New-Video-Profiles","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC25-297-New-Video-Profiles.jpeg"}]},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"identifier":"WWDCNotes.png","type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10149-Whats-new-in-AVQT":{"url":"\/documentation\/wwdcnotes\/wwdc22-10149-whats-new-in-avqt","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10149-Whats-new-in-AVQT","role":"sampleCode","type":"topic","title":"What’s new in AVQT","abstract":[{"type":"text","text":"Discover the latest updates and improvements to the Advanced Video Quality Tool (AVQT). We’ll take you through the interactive reports feature and help you learn how to identify video quality-related issues. We’ll also explore extended support for raw formats, show you how to evaluate specific scenes within a video, and explore how you can use AVQT for Linux to analyze videos on Linux servers and online in the cloud."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC25-304-Explore-video-experiences-for-visionOS":{"abstract":[{"text":"Learn about the different ways you can create and present immersive video experiences within your app. We’ll explore the diverse media types available in visionOS 26, including profiles for 180°, 360°, and wide FOV video; options for creating and playing Apple Immersive Video; and expanded capabilities for 2D, 3D, and spatial video. Discover which profiles are best for your app and its content.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-304-Explore-video-experiences-for-visionOS","url":"\/documentation\/wwdcnotes\/wwdc25-304-explore-video-experiences-for-visionos","title":"Explore video experiences for visionOS","kind":"article","type":"topic","role":"sampleCode"},"WWDC25-297-APMP-Production-Pipeline":{"identifier":"WWDC25-297-APMP-Production-Pipeline","variants":[{"url":"\/images\/WWDCNotes\/WWDC25-297-APMP-Production-Pipeline.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/Jeehut":{"title":"Cihat Gündüz (66 notes)","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/jeehut","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/Jeehut","abstract":[{"text":"Spatial-first Indie Developer for  Platforms. Actively contributing to Open Source since 2011!","type":"text"}],"images":[{"type":"card","identifier":"Jeehut.jpeg"},{"type":"icon","identifier":"Jeehut.jpeg"}]},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","title":"WWDC Notes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","type":"topic","role":"collection","images":[{"identifier":"WWDCNotes.png","type":"icon"}]},"WWDC25-297-Minimal-Ambisonics-Code":{"alt":null,"type":"image","identifier":"WWDC25-297-Minimal-Ambisonics-Code","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC25-297-Minimal-Ambisonics-Code.jpeg"}]},"https://developer.apple.com/av-foundation/HEVC-Stereo-Video-Profile.pdf":{"identifier":"https:\/\/developer.apple.com\/av-foundation\/HEVC-Stereo-Video-Profile.pdf","title":"MV-HEVC profile in this document","type":"link","url":"https:\/\/developer.apple.com\/av-foundation\/HEVC-Stereo-Video-Profile.pdf","titleInlineContent":[{"type":"text","text":"MV-HEVC profile in this document"}]},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link","title":"Contributions are welcome!"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC25-296-Support-immersive-video-playback-in-visionOS-apps":{"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc25-296-support-immersive-video-playback-in-visionos-apps","abstract":[{"type":"text","text":"Discover how to play immersive videos in visionOS apps. We’ll cover various immersive rendering modes, review the frameworks that support them, and walk through how to render immersive video in your app. To get the most out of this video, we recommend first watching “Explore video experiences for visionOS” from WWDC25."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-296-Support-immersive-video-playback-in-visionOS-apps","kind":"article","type":"topic","title":"Support immersive video playback in visionOS apps"},"https://developer.apple.com/av-foundation/Stereo-Video-ISOBMFF-Extensions-1-0.pdf":{"type":"link","identifier":"https:\/\/developer.apple.com\/av-foundation\/Stereo-Video-ISOBMFF-Extensions-1-0.pdf","titleInlineContent":[{"type":"text","text":"this spatial and immersive format specification document"}],"url":"https:\/\/developer.apple.com\/av-foundation\/Stereo-Video-ISOBMFF-Extensions-1-0.pdf","title":"this spatial and immersive format specification document"}}}