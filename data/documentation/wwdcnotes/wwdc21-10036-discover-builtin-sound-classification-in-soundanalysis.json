{"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21"]]},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc21-10036-discover-builtin-sound-classification-in-soundanalysis"],"traits":[{"interfaceLanguage":"swift"}]}],"sections":[],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10036-Discover-builtin-sound-classification-in-SoundAnalysis","interfaceLanguage":"swift"},"metadata":{"title":"Discover built-in sound classification in SoundAnalysis","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC21","role":"sampleCode"},"abstract":[{"text":"Explore how you can use the Sound Analysis framework in your app to detect and classify discrete sounds from any audio source ‚Äî including live sounds from a microphone or from a video or audio file ‚Äî and identify precisely in a moment where that sound occurs. Learn how the built-in sound classifier makes it easy for you to identify over 300 different types of sounds without the need for a custom trained model. This includes a variety of noises, ranging from human sounds, musical instruments, animals, and various items.","type":"text"}],"schemaVersion":{"patch":0,"major":0,"minor":3},"primaryContentSections":[{"kind":"content","content":[{"anchor":"overview","type":"heading","text":"Overview","level":2},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference"}]},{"anchor":"Related-Sessions","type":"heading","text":"Related Sessions","level":2},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10020-Compose-advanced-models-with-Create-ML-Components","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10044-Explore-ShazamKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML"]},{"type":"small","inlineContent":[{"inlineContent":[{"type":"text","text":"Legal Notice"}],"type":"strong"}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright ¬© 2012 ‚Äì 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}]}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"overridingTitle":"Watch Video (19 min)","type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc21\/10036"}},"kind":"article","references":{"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","type":"topic","url":"\/documentation\/wwdcnotes","title":"WWDC Notes"},"WWDCNotes.png":{"type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"identifier":"WWDCNotes.png"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21":{"title":"WWDC21","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc21","role":"collectionGroup","images":[{"identifier":"WWDC21-Icon.png","type":"icon"},{"identifier":"WWDC21.jpeg","type":"card"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21","abstract":[{"type":"text","text":"Xcode 13, Swift 5.5, iOS 15, macOS 12 (Monterey), tvOS 15, watchOS 8."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"MusicKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"DocC"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit 2"},{"type":"text","text":", and more."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10044-Explore-ShazamKit":{"url":"\/documentation\/wwdcnotes\/wwdc21-10044-explore-shazamkit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10044-Explore-ShazamKit","type":"topic","abstract":[{"type":"text","text":"Take advantage of Shazam‚Äôs exact audio matching capabilities within your app when you use ShazamKit. Learn how you can harness the immense Shazam catalog to create all sorts of experiences, including quickly recognizing the exact song playing in the background of a video captured by your app, offering dynamic visual effects based on the music playing in a room, or even syncing with external audio to provide companion app experiences. We‚Äôll also show you how you can build custom catalogs within ShazamKit to match with any audio source ‚Äî all on device."}],"kind":"article","title":"Explore ShazamKit","role":"sampleCode"},"WWDC21.jpeg":{"type":"image","alt":null,"variants":[{"url":"\/images\/WWDC21.jpeg","traits":["1x","light"]}],"identifier":"WWDC21.jpeg"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"type":"link","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Learn More‚Ä¶","type":"text"}],"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶"},"https://developer.apple.com/wwdc21/10036":{"type":"download","checksum":null,"url":"https:\/\/developer.apple.com\/wwdc21\/10036","identifier":"https:\/\/developer.apple.com\/wwdc21\/10036"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML":{"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc19-425-training-sound-classification-models-in-create-ml","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","kind":"article","title":"Training Sound Classification Models in Create ML","type":"topic","abstract":[{"text":"Learn how to quickly and easily create Core ML models capable of classifying the sounds heard in audio files and live audio streams. In addition to providing you the ability to train and evaluate these models, the Create ML app allows you to test the model performance in real-time using the microphone on your Mac. Leverage these on-device models in your app using the new Sound Analysis framework.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10020-Compose-advanced-models-with-Create-ML-Components":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10020-Compose-advanced-models-with-Create-ML-Components","title":"Compose advanced models with Create ML Components","abstract":[{"text":"Take your custom machine learning models to the next level with Create ML Components. We‚Äôll show you how to work with temporal data like video or audio and compose models that can count repetitive human actions or provide advanced sound classification. We‚Äôll also share best practices on using incremental fitting to speed up model training with new data.","type":"text"}],"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc22-10020-compose-advanced-models-with-create-ml-components","role":"sampleCode","kind":"article"},"WWDC21-Icon.png":{"identifier":"WWDC21-Icon.png","variants":[{"url":"\/images\/WWDC21-Icon.png","traits":["1x","light"]}],"type":"image","alt":null}}}