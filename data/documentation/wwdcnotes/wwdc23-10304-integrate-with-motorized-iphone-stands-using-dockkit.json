{"schemaVersion":{"major":0,"patch":0,"minor":3},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes"]]},"abstract":[{"type":"text","text":"Discover how you can create incredible photo and video experiences in your camera app when integrating with DockKit-compatible motorized stands. We‚Äôll show how your app can automatically track subjects in live video across a 360-degree field of view, take direct control of the stand to customize framing, directly control the motors, and provide your own inference model for tracking other objects. Finally, we‚Äôll demonstrate how to create a sense of emotion through dynamic device animations."}],"primaryContentSections":[{"kind":"content","content":[{"type":"heading","text":"Overview","level":2,"anchor":"overview"},{"type":"paragraph","inlineContent":[{"text":"To learn more techniques for image tracking, check out ‚ÄúDetect animal poses in Vision‚Äù from WWDC23 and ‚ÄúClassify hand poses and actions with Create ML‚Äù from WWDC21.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing"}]},{"type":"heading","text":"Written By","level":2,"anchor":"Written-By"},{"type":"row","columns":[{"content":[{"type":"paragraph","inlineContent":[{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/123?v=4","type":"image"}]}],"size":1},{"content":[{"type":"heading","anchor":"To-Do<doc<replace-this-with-your-GitHub-handle>>","text":"[To Do](<doc:<replace this with your GitHub handle>>)","level":3},{"type":"paragraph","inlineContent":[{"text":"An amazing developer.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"[Contributed Notes](<doc:","type":"text"},{"text":">)","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/x.com\/Jeehut","type":"reference","isActive":true}]}],"size":4}],"numberOfColumns":5},{"type":"heading","text":"Related Sessions","level":2,"anchor":"Related-Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision"],"style":"list"},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright ¬© 2012 ‚Äì 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}]}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10304-integrate-with-motorized-iphone-stands-using-dockkit"]}],"metadata":{"role":"sampleCode","roleHeading":"WWDC23","title":"Integrate with motorized iPhone stands using DockKit","modules":[{"name":"WWDC Notes"}]},"sections":[],"kind":"article","identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","interfaceLanguage":"swift"},"sampleCodeDownload":{"action":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/10304","overridingTitle":"Watch Video","type":"reference","isActive":true},"kind":"sampleDownload"},"references":{"doc://WWDCNotes/documentation/WWDCNotes":{"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"type":"topic","role":"collection","url":"\/documentation\/wwdcnotes","title":"WWDC Notes","kind":"symbol","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","images":[{"type":"icon","identifier":"WWDCNotes.png"}]},"https://avatars.githubusercontent.com/u/123?v=4":{"alt":"Profile image of To Do","variants":[{"traits":["1x","light"],"url":"https:\/\/avatars.githubusercontent.com\/u\/123?v=4"}],"type":"image","identifier":"https:\/\/avatars.githubusercontent.com\/u\/123?v=4"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10045-Detect-animal-poses-in-Vision":{"role":"sampleCode","title":"Detect animal poses in Vision","abstract":[{"type":"text","text":"Go beyond detecting cats and dogs in images. We‚Äôll show you how to use Vision to detect the individual joints and poses of these animals as well ‚Äî all in real time ‚Äî and share how you can enable exciting features like animal tracking for a camera app, creative embellishment on an animal photo, and more. We‚Äôll also explore other important enhancements to Vision and share best practices."}],"url":"\/documentation\/wwdcnotes\/wwdc23-10045-detect-animal-poses-in-vision","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision"},"https://x.com/Jeehut":{"titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"title":"X\/Twitter","url":"https:\/\/x.com\/Jeehut","type":"link","identifier":"https:\/\/x.com\/Jeehut"},"https://developer.apple.com/wwdc23/10304":{"checksum":null,"url":"https:\/\/developer.apple.com\/wwdc23\/10304","type":"download","identifier":"https:\/\/developer.apple.com\/wwdc23\/10304"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML":{"role":"sampleCode","title":"Classify hand poses and actions with Create ML","abstract":[{"type":"text","text":"With Create ML, your app‚Äôs ability to understand the expressiveness of the human hand has never been easier. Discover how you can build off the support for Hand Pose Detection in Vision and train custom Hand Pose and Hand Action classifiers using the Create ML app and framework. Learn how simple it is to collect data, train a model, and integrate it with Vision, Camera, and ARKit to create a fun, entertaining app experience."}],"url":"\/documentation\/wwdcnotes\/wwdc21-10039-classify-hand-poses-and-actions-with-create-ml","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision":{"abstract":[{"type":"text","text":"Explore how the Vision framework can help your app detect body and hand poses in photos and video. With pose detection, your app can analyze the poses, movements, and gestures of people to offer new video editing possibilities, or to perform action classification when paired with an action classifier built in Create ML. And we‚Äôll show you how you can bring gesture recognition into your app through hand pose, delivering a whole new form of interaction."}],"url":"\/documentation\/wwdcnotes\/wwdc20-10653-detect-body-and-hand-pose-with-vision","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision","title":"Detect Body and Hand Pose with Vision","kind":"article","type":"topic"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"identifier":"WWDCNotes.png","type":"image","alt":null},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶","type":"link","titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}]}}}