{"metadata":{"role":"sampleCode","title":"Detect people, faces, and poses using Vision","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC21"},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","interfaceLanguage":"swift"},"sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc21\/10040","overridingTitle":"Watch Video (17 min)","type":"reference"}},"sections":[],"schemaVersion":{"patch":0,"major":0,"minor":3},"primaryContentSections":[{"kind":"content","content":[{"anchor":"overview","level":2,"type":"heading","text":"Overview"},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference"}]},{"anchor":"Related-Sessions","level":2,"type":"heading","text":"Related Sessions"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10024-Whats-new-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-222-Understanding-Images-in-Vision-Framework"]}]}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc21-10040-detect-people-faces-and-poses-using-vision"],"traits":[{"interfaceLanguage":"swift"}]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21"]]},"kind":"article","abstract":[{"type":"text","text":"Discover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app."}],"references":{"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML":{"abstract":[{"type":"text","text":"With Create ML, your app‚Äôs ability to understand the expressiveness of the human hand has never been easier. Discover how you can build off the support for Hand Pose Detection in Vision and train custom Hand Pose and Hand Action classifiers using the Create ML app and framework. Learn how simple it is to collect data, train a model, and integrate it with Vision, Camera, and ARKit to create a fun, entertaining app experience."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML","url":"\/documentation\/wwdcnotes\/wwdc21-10039-classify-hand-poses-and-actions-with-create-ml","kind":"article","role":"sampleCode","type":"topic","title":"Classify hand poses and actions with Create ML"},"https://developer.apple.com/wwdc21/10040":{"identifier":"https:\/\/developer.apple.com\/wwdc21\/10040","url":"https:\/\/developer.apple.com\/wwdc21\/10040","checksum":null,"type":"download"},"WWDC21.jpeg":{"type":"image","identifier":"WWDC21.jpeg","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC21.jpeg"}],"alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-222-Understanding-Images-in-Vision-Framework":{"title":"Understanding Images in Vision Framework","url":"\/documentation\/wwdcnotes\/wwdc19-222-understanding-images-in-vision-framework","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-222-Understanding-Images-in-Vision-Framework","kind":"article","role":"sampleCode","abstract":[{"text":"Learn all about the many advances in the Vision Framework including effortless image classification, image saliency, determining image similarity, and improvements in facial feature detection, and face capture quality scoring. This packed session will show you how easy it is to bring powerful computer vision techniques to your apps.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision","title":"Detect Body and Hand Pose with Vision","kind":"article","type":"topic","abstract":[{"text":"Explore how the Vision framework can help your app detect body and hand poses in photos and video. With pose detection, your app can analyze the poses, movements, and gestures of people to offer new video editing possibilities, or to perform action classification when paired with an action classifier built in Create ML. And we‚Äôll show you how you can bring gesture recognition into your app through hand pose, delivering a whole new form of interaction.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc20-10653-detect-body-and-hand-pose-with-vision","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10024-Whats-new-in-Vision":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10024-Whats-new-in-Vision","kind":"article","abstract":[{"type":"text","text":"Learn about the latest updates to Vision APIs that help your apps recognize text, detect faces and face landmarks, and implement optical flow. We‚Äôll take you through the capabilities of optical flow for video-based apps, show you how to update your apps with revisions to the machine learning models that drive these APIs, and explore how you can visualize your Vision tasks with Quick Look Preview support in Xcode."}],"title":"What‚Äôs new in Vision","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc22-10024-whats-new-in-vision"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21":{"kind":"article","title":"WWDC21","url":"\/documentation\/wwdcnotes\/wwdc21","type":"topic","images":[{"identifier":"WWDC21-Icon.png","type":"icon"},{"identifier":"WWDC21.jpeg","type":"card"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21","role":"collectionGroup","abstract":[{"type":"text","text":"Xcode 13, Swift 5.5, iOS 15, macOS 12 (Monterey), tvOS 15, watchOS 8."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"MusicKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"DocC"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit 2"},{"type":"text","text":", and more."}]},"WWDCNotes.png":{"identifier":"WWDCNotes.png","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDCNotes.png"}]},"WWDC21-Icon.png":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC21-Icon.png"}],"identifier":"WWDC21-Icon.png","alt":null,"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes":{"url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection","kind":"symbol","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC Notes","type":"topic"}}}