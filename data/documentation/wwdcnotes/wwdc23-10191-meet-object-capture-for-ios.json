{"schemaVersion":{"major":0,"patch":0,"minor":3},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10191-Meet-Object-Capture-for-iOS","interfaceLanguage":"swift"},"kind":"article","variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc23-10191-meet-object-capture-for-ios"],"traits":[{"interfaceLanguage":"swift"}]}],"primaryContentSections":[{"content":[{"type":"heading","level":2,"text":"Overview","anchor":"overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Object Capture utilizes cutting-edge computer vision technologies to generate realistic 3D models from a series of images captured from different angles. Initially introduced for macOS in WWDC21, it is now also available for iOS 17 and above."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-recap","type":"image"}]},{"type":"aside","content":[{"type":"paragraph","inlineContent":[{"text":"For a recap, watch ","type":"text"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10076","isActive":true},{"text":".","type":"text"}]}],"name":"Note","style":"note"},{"type":"paragraph","inlineContent":[{"type":"text","text":"To showcase this new capability for iOS, Apple has provided a sample app that allows you to learn and adapt it to your own apps. Follow this "},{"identifier":"https:\/\/developer.apple.com\/documentation\/realitykit\/guided-capture-sample","type":"reference","isActive":true},{"text":" to download the project and read the explanation below to understand how to capture objects and how this sample app works internally.","type":"text"}]},{"type":"heading","level":1,"text":"System Requirements","anchor":"System-Requirements"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The sample cannot run on a simulator, if you want to try it out, you will meet the following requirements:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"iOS or iPadOS 17 or later."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"iPhone or iPad with a LiDAR Scanner and an A14 Bionic chip or later."}]}]}]},{"type":"aside","content":[{"inlineContent":[{"text":"You can even compile the sample app on a device that doesn’t meet those requirements, but it will crash when attempting to start capturing","type":"text"}],"type":"paragraph"}],"name":"Note","style":"note"},{"type":"heading","level":1,"text":"Steps to scan objects using the demo app","anchor":"Steps-to-scan-objects-using-the-demo-app"},{"type":"orderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Open the app and point it at the object."}]}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-1","type":"image"}]},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"An automatic bounding box will be generated before starting the capture. You can adjust the edges to define the scanning area.","type":"text"}]}]}],"type":"orderedList","start":2},{"inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-2"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"The app will guide you through the regions that require more image scanning.","type":"text"}],"type":"paragraph"}]}],"type":"orderedList","start":3},{"inlineContent":[{"identifier":"WWDC23-10191-capture-object-3","type":"image"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"The app provides feedback to help you capture high-quality shots.","type":"text"}],"type":"paragraph"}]}],"type":"orderedList","start":4},{"inlineContent":[{"identifier":"WWDC23-10191-capture-object-4","type":"image"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"After completing the first scanning, you can flip the object to capture the bottom (if necessary)."}]}]}],"type":"orderedList","start":5},{"inlineContent":[{"identifier":"WWDC23-10191-capture-object-5","type":"image"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"The app requires two more scan rounds. Once all scans are completed, the app will start generating the 3D object model. This process may take a few minutes.","type":"text"}]}]}],"type":"orderedList","start":6},{"inlineContent":[{"identifier":"WWDC23-10191-capture-object-6","type":"image"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Once the processing is complete, the app will provide a visualization of your new 3D model and you will be able to save it as a usdz file."}],"type":"paragraph"}]}],"type":"orderedList","start":7},{"inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-7"}],"type":"paragraph"},{"text":"Support for more objects with LiDAR","anchor":"Support-for-more-objects-with-LiDAR","level":1,"type":"heading"},{"inlineContent":[{"text":"The LiDAR Scanner has received improvements to scan and reconstruct low-texture objects, as demonstrated with the chair image.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-8"}],"type":"paragraph"},{"inlineContent":[{"text":"Although low-texture objects are supported, scanning certain objects still presents challenges. It is best to avoid objects that are reflective, transparent, or have thin structures.","type":"text"}],"type":"paragraph"},{"text":"Flippable Objects","anchor":"Flippable-Objects","level":1,"type":"heading"},{"inlineContent":[{"text":"Textured and rigid objects are ideal for capturing the bottom side. However, objects with repetitive textures, deformable objects, and textureless objects are not recommended for flipping.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10191-capture-object-9","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"When flipping an object, it is advisable to use diffuse lights to minimize shadow and reflections on the object’s surface."}],"type":"paragraph"},{"text":"Non-flippable Objects","anchor":"Non-flippable-Objects","level":1,"type":"heading"},{"inlineContent":[{"text":"For non-flippable objects, it is recommended to scan them from three different heights and place them against a textured background to enhance visibility.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-10"}],"type":"paragraph"},{"text":"Object Capture API","anchor":"Object-Capture-API","level":1,"type":"heading"},{"inlineContent":[{"type":"text","text":"The Object Capture API consists of two steps:"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Image Capture","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Model Reconstruction"}]}]}],"type":"orderedList"},{"text":"Image Capture API","anchor":"Image-Capture-API","level":1,"type":"heading"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"The Image Capture API comprises two parts: the session from "},{"type":"codeVoice","code":"ObjectCaptureSession"},{"type":"text","text":", which allows you to control the flow of a state machine during image capture, and the SwiftUI View "},{"type":"codeVoice","code":"ObjectCaptureView"},{"type":"text","text":", which displays the camera feed and adapts UI elements based on the session’s state."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The session progresses through various states, including initializing, ready, detecting, capturing, finishing, completed, and failed (for errors)."}]}]}],"type":"unorderedList"},{"text":"Adding Capture Object API to your app","anchor":"Adding-Capture-Object-API-to-your-app","level":2,"type":"heading"},{"inlineContent":[{"text":"To get started, you need to first import RealityKit and SwiftUI, and then create a session object:","type":"text"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["import RealityKit","import SwiftUI ","","var session = ObjectCaptureSession()"]},{"style":"note","name":"Note","content":[{"inlineContent":[{"type":"text","text":"Since "},{"code":"ObjectCaptureSession","type":"codeVoice"},{"type":"text","text":" is a reference type, it is important to keep it alive through "},{"code":"@StateObject","type":"codeVoice"},{"type":"text","text":" until the capture process is completed."}],"type":"paragraph"}],"type":"aside"},{"inlineContent":[{"text":"These are the different session states:","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10191-session-states","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Continuing, call the "},{"type":"codeVoice","code":"start"},{"type":"text","text":" method with a directory indicating where to store the captured images:"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var configuration = ObjectCaptureSession.Configuration()","configuration.checkpointDirectory = getDocumentsDir().appendingPathComponent(\"Snapshots\/\")","","session.start(imagesDirectory: getDocumentsDir().appendingPathComponent(\"Images\/\"),","            configuration: configuration)"]},{"inlineContent":[{"text":"Once this call is done, it will move to ","type":"text"},{"code":"ready","type":"codeVoice"},{"text":" state.","type":"text"}],"type":"paragraph"},{"style":"note","name":"Note","content":[{"inlineContent":[{"text":"You can also provide a checkpoint directory to speed up the reconstruction process.","type":"text"}],"type":"paragraph"}],"type":"aside"},{"inlineContent":[{"type":"text","text":"Next, create an "},{"type":"codeVoice","code":"ObjectCaptureView"},{"type":"text","text":" and provide the session object. This view should be wrapped within another SwiftUI view:"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["import RealityKit","import SwiftUI","","struct CapturePrimaryView: View {","    var body: some View {","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"inlineContent":[{"type":"codeVoice","code":"ObjectCaptureView"},{"text":" displays the UI corresponding to the current session state.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To start detecting the object, call "},{"code":"session.startDetecting()","type":"codeVoice"},{"type":"text","text":" from the UI:"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var body: some View {","    ZStack {","        ObjectCaptureView(session: session)","        if case .ready = session.state {","            CreateButton(label: \"Continue\") { ","                session.startDetecting() ","            }","        }","    }","}"]},{"inlineContent":[{"text":"In this state, the bounding box is displayed, allowing you to adjust the capture edges.","type":"text"},{"text":" ","type":"text"},{"text":"- To capture a different object, call ","type":"text"},{"type":"codeVoice","code":"session.resetDetection()"},{"text":" to return to the ","type":"text"},{"type":"codeVoice","code":"ready"},{"text":" state:","type":"text"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["Button {","    session.resetDetection()","} label: {","    Text(\"Reset\")","}"]},{"inlineContent":[{"type":"text","text":"Once the bounding box is adjusted, call "},{"type":"codeVoice","code":"session.startCapturing()"},{"type":"text","text":" from the UI to progress to the next state:"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var body: some View {","    ZStack {","        ObjectCaptureView(session: session)","        if case .ready = session.state {","            CreateButton(label: \"Continue\") { ","                session.startDetecting()","            }","        } else if case .detecting = session.state {","            CreateButton(label: \"Start Capture\") { ","                session.startCapturing()","            }","        }","    }","}"]},{"inlineContent":[{"type":"text","text":"The session will automatically capture images while you slowly move around the object. "},{"type":"codeVoice","code":"ObjectCaptureView"},{"type":"text","text":" provides a dial indicating areas that require scanning."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Once the scanning is complete, "},{"type":"codeVoice","code":"session.userCompletedScanPass"},{"type":"text","text":" is set to "},{"type":"codeVoice","code":"true"},{"type":"text","text":":"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var body: some View {","    if session.userCompletedScanPass {","        VStack {","        }","    } else {","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"inlineContent":[{"type":"text","text":"The session can detect if the scanned object needs to be flipped to capture parts not yet visible to the camera, such as the bottom. This can be determined using "},{"code":"ObjectCaptureSession.Feedback","type":"codeVoice"},{"type":"text","text":" and checking if "},{"code":"session.feedback.contains(.objectNotFlippable)","type":"codeVoice"},{"type":"text","text":":"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["if !session.feedback.contains(.objectNotFlippable) && !hasIndicatedFlipObjectAnyway {","    session.beginNewScanPass()","} else {","    session.beginNewScanPassAfterFlip()","}"]},{"inlineContent":[{"text":"If the user decides to flip the object, ","type":"text"},{"type":"codeVoice","code":"beginNewScanPassAfterFlip()"},{"text":" sets the session back to the ready state to adjust the bounding box for the new orientation. Otherwise, ","type":"text"},{"type":"codeVoice","code":"beginNewScanPass()"},{"text":" keeps the session in the capturing state, allowing capturing from different heights while maintaining the same bounding box.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Once the three scans are completed, the sample app provides a button to end the session by calling "},{"type":"codeVoice","code":"session.finish()"},{"type":"text","text":". This transitions the session to the finishing state:"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var body: some View {","    if session.userCompletedScanPass {","        VStack {","            CreateButton(label: \"Finish\") {","                session.finish() ","            }","        }","    } else {","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"inlineContent":[{"text":"Once the process is finished, the session automatically moves to the ","type":"text"},{"type":"codeVoice","code":"completed"},{"text":" state, triggering on-device reconstruction.","type":"text"}],"type":"paragraph"},{"style":"note","name":"Note","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"If an error occurs during the process, the session enters into failed state, requiring the creation of a new session."}]}],"type":"aside"},{"inlineContent":[{"text":"Optionally, you can use ","type":"text"},{"type":"codeVoice","code":"ObjectCapturePointCloudView"},{"text":" to preview the scanned parts of the object since its initial placement or last flip:","type":"text"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var body: some View {","    if session.userCompletedScanPass {","        VStack {","            ObjectCapturePointCloudView(session: session)","            CreateButton(label: \"Finish\") {","                session.finish() ","            }","        }","    } else {    ","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-11"}],"type":"paragraph"},{"text":"Model Reconstruction API","anchor":"Model-Reconstruction-API","level":2,"type":"heading"},{"inlineContent":[{"type":"text","text":"The following code from the sample app initiates the reconstruction process, applicable for both iOS and macOS:"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var body: some View {","  ReconstructionProgressView()","      .task { \/\/ 1","          var configuration = PhotogrammetrySession.Configuration() ","          configuration.checkpointDirectory = getDocumentsDir()","              .appendingPathComponent(\"Snapshots\/\") \/\/ 3","          let session = try PhotogrammetrySession( \/\/ 2","              input: getDocumentsDir().appendingPathComponent(\"Images\/\"),","              configuration: configuration)","          try session.process(requests: [ \/\/ 4","              .modelFile(url: getDocumentsDir().appendingPathComponent(\"model.usdz\")) ","          ])","          for try await output in session.outputs { \/\/ 5","              switch output {","                  case .processingComplete:","                      handleComplete()","                      \/\/ Handle other Output messages here.","              }","          }","      }","}"]},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"The ","type":"text"},{"code":"task","type":"codeVoice"},{"text":" modifier is attached to ","type":"text"},{"code":"ReconstructionProgressView","type":"codeVoice"},{"text":".","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"A new ","type":"text"},{"code":"PhotogrammetrySession","type":"codeVoice"},{"text":" is created, pointing to the images.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Optionally, the same checkpoint directory used during image capture can be provided to expedite the reconstruction process.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The ","type":"text"},{"code":"process","type":"codeVoice"},{"text":" function is called to request a model file.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Finally, the message stream is awaited in an asynchronous loop, and output messages are handled as they arrive.","type":"text"}]}]}],"type":"orderedList"},{"text":"Detail level on iOS","anchor":"Detail-level-on-iOS","level":1,"type":"heading"},{"inlineContent":[{"text":"To optimize model generation and viewing on mobile devices, iOS supports a reduced detail level. The reconstruction model includes the following texture maps: ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/docs.cryengine.com\/display\/SDKDOC2\/Diffuse+Maps"},{"text":", ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/en.wikipedia.org\/wiki\/Ambient_occlusion"},{"text":", and ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/en.wikipedia.org\/wiki\/Normal_mapping"},{"text":". If you require a higher level of detail, you must transfer your images to macOS for reconstruction.","type":"text"}],"type":"paragraph"},{"text":"Capturing for Mac","anchor":"Capturing-for-Mac","level":1,"type":"heading"},{"inlineContent":[{"text":"Starting from 2023, Mac reconstruction also utilizes the saved LiDAR data in the images. By default, ","type":"text"},{"type":"codeVoice","code":"ObjectCaptureSession"},{"text":" stops capturing images when the reconstruction limit for the iOS device is reached. However, for macOS, you can allow the session to capture more images than on-device reconstruction can use by setting ","type":"text"},{"type":"codeVoice","code":"isOverCaptureEnabled"},{"text":" to ","type":"text"},{"type":"codeVoice","code":"true"},{"text":" in the session’s configurations:","type":"text"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["var configuration = ObjectCaptureSession.Configuration()","configuration.isOverCaptureEnabled = true","","session.start(imagesDirectory: getDocumentsDir().appendingPathComponent(\"Images\/\"),","            configuration: configuration)"]},{"inlineContent":[{"type":"text","text":"These additional shots are not used for on-device reconstruction but are stored in the Images folder."}],"type":"paragraph"},{"text":"Model Reconstruction in Mac","anchor":"Model-Reconstruction-in-Mac","level":1,"type":"heading"},{"inlineContent":[{"type":"text","text":"No code is required. Simply import the images into Reality Composer Pro, choose a detail level, and obtain your model!"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10191-composer-pro","type":"image"}],"type":"paragraph"},{"style":"note","name":"Note","content":[{"inlineContent":[{"type":"text","text":"For more details, check out "},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083\/","isActive":true,"type":"reference"}],"type":"paragraph"}],"type":"aside"},{"text":"Reconstruction Enhancements","anchor":"Reconstruction-Enhancements","level":1,"type":"heading"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Improved performance and image quality on Mac."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"The API now provides an estimated processing time in addition to progress percentage.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"High-quality poses can now be requested for each image, including the estimated position and orientation of the camera for that image.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"To obtain the poses, add "},{"type":"codeVoice","code":".poses"},{"type":"text","text":" to the "},{"type":"codeVoice","code":"process()"},{"type":"text","text":" function call and handle them when they arrive in the message stream:"}]}]}],"type":"unorderedList"},{"syntax":"swift","type":"codeListing","code":["try session.process(requests: [ ","    .poses ","    .modelFile(url: modelURL),","])","for try await output in session.outputs {","    switch output {","    case .poses(let poses):","        handlePoses(poses)","    case .processingComplete:","        handleComplete()","    }","}"]},{"items":[{"content":[{"inlineContent":[{"text":"The API now provides a new custom detail level to control the amount of mesh decimation, texture map resolution, format, and which texture maps should be included.","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10191-custom-detail-level"}],"type":"paragraph"},{"text":"Written By","anchor":"Written-By","level":2,"type":"heading"},{"columns":[{"size":1,"content":[{"inlineContent":[{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/17436242?v=4","type":"image"}],"type":"paragraph"}]},{"size":4,"content":[{"type":"heading","anchor":"Pedro-Rojas","level":3,"text":"Pedro Rojas"},{"type":"paragraph","inlineContent":[{"isActive":true,"overridingTitle":"Contributed Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/pitt500","overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/github.com\/pitt500","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/x.com\/swiftandtips","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/www.youtube.com\/@swiftandtips","type":"reference"}]}]}],"type":"row","numberOfColumns":5},{"inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}],"type":"paragraph"},{"text":"Related Sessions","anchor":"Related-Sessions","level":2,"type":"heading"},{"style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro"],"type":"links"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}],"kind":"content"}],"metadata":{"modules":[{"name":"WWDC Notes"}],"title":"Meet Object Capture for iOS","role":"sampleCode","roleHeading":"WWDC23"},"sections":[],"abstract":[{"type":"text","text":"Discover how you can offer an end-to-end Object Capture experience directly in your iOS apps to help people turn their objects into ready-to-use 3D models. Learn how you can create a fully automated Object Capture scan flow with our sample app and how you can assist people in automatically capturing the best content for their model. We’ll also discuss LiDAR data and provide best practices for scanning objects."}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"overridingTitle":"Watch Video (20 min)","identifier":"https:\/\/developer.apple.com\/wwdc23\/10191","type":"reference"}},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"references":{"WWDC23-10191-capture-object-10":{"alt":"Chair with low textures","identifier":"WWDC23-10191-capture-object-10","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-10.png"}],"type":"image"},"https://developer.apple.com/videos/play/wwdc2023/10083/":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083\/","titleInlineContent":[{"text":"Meet Reality Composer Pro","type":"text"}],"type":"link","title":"Meet Reality Composer Pro","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083\/"},"https://github.com/pitt500":{"identifier":"https:\/\/github.com\/pitt500","titleInlineContent":[{"text":"GitHub","type":"text"}],"type":"link","title":"GitHub","url":"https:\/\/github.com\/pitt500"},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"type":"link","title":"Contributions are welcome!","url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing"},"https://developer.apple.com/videos/play/wwdc2021/10076":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10076","titleInlineContent":[{"text":"Create 3D models with Object Capture","type":"text"}],"type":"link","title":"Create 3D models with Object Capture","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10076"},"WWDC23-10191-capture-object-4":{"alt":"Feedback requesting more lights, moving slower and arrow to center camera","identifier":"WWDC23-10191-capture-object-4","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-4.png"}],"type":"image"},"WWDC23-10191-capture-object-2":{"alt":"Bounding box","identifier":"WWDC23-10191-capture-object-2","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-2.png"}],"type":"image"},"WWDC23-10191-capture-object-3":{"alt":"Scanning Regions","identifier":"WWDC23-10191-capture-object-3","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-3.png"}],"type":"image"},"https://en.wikipedia.org/wiki/Normal_mapping":{"identifier":"https:\/\/en.wikipedia.org\/wiki\/Normal_mapping","titleInlineContent":[{"text":"normal","type":"text"}],"type":"link","title":"normal","url":"https:\/\/en.wikipedia.org\/wiki\/Normal_mapping"},"WWDC23-10191-capture-object-8":{"alt":"Chair with low textures","identifier":"WWDC23-10191-capture-object-8","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-8.png"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"SwiftData"},{"type":"text","text":", "},{"type":"codeVoice","code":"Observation"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit"},{"type":"text","text":" views, and more."}],"url":"\/documentation\/wwdcnotes\/wwdc23","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","role":"collectionGroup","title":"WWDC23","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"kind":"article","type":"topic"},"https://developer.apple.com/wwdc23/10191":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/10191","checksum":null,"type":"download","url":"https:\/\/developer.apple.com\/wwdc23\/10191"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10083-Meet-Reality-Composer-Pro":{"abstract":[{"type":"text","text":"Discover how to easily compose, edit, and preview 3D content with Reality Composer Pro. Follow along as we explore this developer tool by setting up a new project, composing scenes, adding particle emitters and audio, and even previewing content on device."}],"url":"\/documentation\/wwdcnotes\/wwdc23-10083-meet-reality-composer-pro","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro","role":"sampleCode","title":"Meet Reality Composer Pro","kind":"article","type":"topic"},"WWDC23-10191-capture-object-5":{"alt":"Flipping Object","identifier":"WWDC23-10191-capture-object-5","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-5.png"}],"type":"image"},"WWDC23-10191-capture-object-11":{"alt":"Point Cloud View","identifier":"WWDC23-10191-capture-object-11","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-11.png"}],"type":"image"},"https://avatars.githubusercontent.com/u/17436242?v=4":{"alt":"Profile image of Pedro Rojas","identifier":"https:\/\/avatars.githubusercontent.com\/u\/17436242?v=4","variants":[{"traits":["1x","light"],"url":"https:\/\/avatars.githubusercontent.com\/u\/17436242?v=4"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/pitt500":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/pitt500","kind":"article","type":"topic","role":"sampleCode","title":"Pedro Rojas (1 note)","abstract":[{"text":"iOS & Swift Specialist | Content Creator | Engineering Manager at Insulet | Formerly Meta and HP","type":"text"}],"url":"\/documentation\/wwdcnotes\/pitt500"},"WWDC23-10191-custom-detail-level":{"alt":"Custom Detail Level","identifier":"WWDC23-10191-custom-detail-level","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-custom-detail-level.png"}],"type":"image"},"https://docs.cryengine.com/display/SDKDOC2/Diffuse+Maps":{"identifier":"https:\/\/docs.cryengine.com\/display\/SDKDOC2\/Diffuse+Maps","titleInlineContent":[{"text":"Diffuse","type":"text"}],"type":"link","title":"Diffuse","url":"https:\/\/docs.cryengine.com\/display\/SDKDOC2\/Diffuse+Maps"},"https://developer.apple.com/documentation/realitykit/guided-capture-sample":{"identifier":"https:\/\/developer.apple.com\/documentation\/realitykit\/guided-capture-sample","titleInlineContent":[{"text":"link","type":"text"}],"type":"link","title":"link","url":"https:\/\/developer.apple.com\/documentation\/realitykit\/guided-capture-sample"},"WWDC23-10191-capture-object-1":{"alt":"Point to the object","identifier":"WWDC23-10191-capture-object-1","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-1.png"}],"type":"image"},"https://en.wikipedia.org/wiki/Ambient_occlusion":{"identifier":"https:\/\/en.wikipedia.org\/wiki\/Ambient_occlusion","titleInlineContent":[{"text":"ambient occlusion","type":"text"}],"type":"link","title":"ambient occlusion","url":"https:\/\/en.wikipedia.org\/wiki\/Ambient_occlusion"},"WWDC23-10191-capture-object-9":{"alt":"Flippable vs Non-flipabble objects","identifier":"WWDC23-10191-capture-object-9","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-9.png"}],"type":"image"},"https://x.com/swiftandtips":{"identifier":"https:\/\/x.com\/swiftandtips","titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"type":"link","title":"X\/Twitter","url":"https:\/\/x.com\/swiftandtips"},"WWDC23-10191-session-states":{"alt":"Session States","identifier":"WWDC23-10191-session-states","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-session-states.png"}],"type":"image"},"WWDC23-10191-capture-object-7":{"alt":"Final 3D Model","identifier":"WWDC23-10191-capture-object-7","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-7.png"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","title":"WWDC Notes","url":"\/documentation\/wwdcnotes","type":"topic","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","role":"collection"},"WWDCNotes.png":{"alt":null,"identifier":"WWDCNotes.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"type":"image"},"https://www.youtube.com/@swiftandtips":{"identifier":"https:\/\/www.youtube.com\/@swiftandtips","titleInlineContent":[{"text":"Blog","type":"text"}],"type":"link","title":"Blog","url":"https:\/\/www.youtube.com\/@swiftandtips"},"WWDC23-10191-capture-object-6":{"alt":"Scans complete","identifier":"WWDC23-10191-capture-object-6","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-6.png"}],"type":"image"},"WWDC23-10191-composer-pro":{"alt":"Reality Composer Pro","identifier":"WWDC23-10191-composer-pro","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-composer-pro.png"}],"type":"image"},"WWDC23-10191-capture-object-recap":{"alt":"Recap of Object Capture for macOS","identifier":"WWDC23-10191-capture-object-recap","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-recap.png"}],"type":"image"}}}