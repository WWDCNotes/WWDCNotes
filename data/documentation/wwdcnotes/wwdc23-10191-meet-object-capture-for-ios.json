{"abstract":[{"type":"text","text":"Discover how you can offer an end-to-end Object Capture experience directly in your iOS apps to help people turn their objects into ready-to-use 3D models. Learn how you can create a fully automated Object Capture scan flow with our sample app and how you can assist people in automatically capturing the best content for their model. We’ll also discuss LiDAR data and provide best practices for scanning objects."}],"primaryContentSections":[{"content":[{"anchor":"overview","type":"heading","text":"Overview","level":2},{"type":"paragraph","inlineContent":[{"type":"text","text":"Object Capture utilizes cutting-edge computer vision technologies to generate realistic 3D models from a series of images captured from different angles. Initially introduced for macOS in WWDC21, it is now also available for iOS 17 and above."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-recap"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"For a recap, watch "},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10076","type":"reference"},{"type":"text","text":"."}]}],"type":"aside","name":"Note","style":"note"},{"type":"paragraph","inlineContent":[{"text":"To showcase this new capability for iOS, Apple has provided a sample app that allows you to learn and adapt it to your own apps. Follow this ","type":"text"},{"identifier":"https:\/\/developer.apple.com\/documentation\/realitykit\/guided-capture-sample","isActive":true,"type":"reference"},{"text":" to download the project and read the explanation below to understand how to capture objects and how this sample app works internally.","type":"text"}]},{"anchor":"System-Requirements","type":"heading","text":"System Requirements","level":1},{"type":"paragraph","inlineContent":[{"type":"text","text":"The sample cannot run on a simulator, if you want to try it out, you will meet the following requirements:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"iOS or iPadOS 17 or later.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"iPhone or iPad with a LiDAR Scanner and an A14 Bionic chip or later.","type":"text"}],"type":"paragraph"}]}]},{"content":[{"inlineContent":[{"text":"You can even compile the sample app on a device that doesn’t meet those requirements, but it will crash when attempting to start capturing","type":"text"}],"type":"paragraph"}],"type":"aside","name":"Note","style":"note"},{"anchor":"Steps-to-scan-objects-using-the-demo-app","type":"heading","text":"Steps to scan objects using the demo app","level":1},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Open the app and point it at the object."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-1","type":"image"}]},{"type":"orderedList","start":2,"items":[{"content":[{"inlineContent":[{"type":"text","text":"An automatic bounding box will be generated before starting the capture. You can adjust the edges to define the scanning area."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-2","type":"image"}]},{"type":"orderedList","start":3,"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The app will guide you through the regions that require more image scanning."}]}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-3","type":"image"}]},{"type":"orderedList","start":4,"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The app provides feedback to help you capture high-quality shots."}]}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-4","type":"image"}]},{"type":"orderedList","start":5,"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"After completing the first scanning, you can flip the object to capture the bottom (if necessary).","type":"text"}]}]}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-5"}]},{"type":"orderedList","start":6,"items":[{"content":[{"inlineContent":[{"type":"text","text":"The app requires two more scan rounds. Once all scans are completed, the app will start generating the 3D object model. This process may take a few minutes."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-6","type":"image"}]},{"type":"orderedList","start":7,"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Once the processing is complete, the app will provide a visualization of your new 3D model and you will be able to save it as a usdz file."}]}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-7","type":"image"}]},{"anchor":"Support-for-more-objects-with-LiDAR","type":"heading","text":"Support for more objects with LiDAR","level":1},{"type":"paragraph","inlineContent":[{"type":"text","text":"The LiDAR Scanner has received improvements to scan and reconstruct low-texture objects, as demonstrated with the chair image."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-8"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Although low-texture objects are supported, scanning certain objects still presents challenges. It is best to avoid objects that are reflective, transparent, or have thin structures."}]},{"anchor":"Flippable-Objects","type":"heading","text":"Flippable Objects","level":1},{"type":"paragraph","inlineContent":[{"text":"Textured and rigid objects are ideal for capturing the bottom side. However, objects with repetitive textures, deformable objects, and textureless objects are not recommended for flipping.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-9","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"When flipping an object, it is advisable to use diffuse lights to minimize shadow and reflections on the object’s surface."}]},{"anchor":"Non-flippable-Objects","type":"heading","text":"Non-flippable Objects","level":1},{"type":"paragraph","inlineContent":[{"type":"text","text":"For non-flippable objects, it is recommended to scan them from three different heights and place them against a textured background to enhance visibility."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10191-capture-object-10"}]},{"anchor":"Object-Capture-API","type":"heading","text":"Object Capture API","level":1},{"type":"paragraph","inlineContent":[{"text":"The Object Capture API consists of two steps:","type":"text"}]},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"text":"Image Capture","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Model Reconstruction","type":"text"}]}]}]},{"anchor":"Image-Capture-API","type":"heading","text":"Image Capture API","level":1},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"The Image Capture API comprises two parts: the session from ","type":"text"},{"code":"ObjectCaptureSession","type":"codeVoice"},{"text":", which allows you to control the flow of a state machine during image capture, and the SwiftUI View ","type":"text"},{"code":"ObjectCaptureView","type":"codeVoice"},{"text":", which displays the camera feed and adapts UI elements based on the session’s state.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The session progresses through various states, including initializing, ready, detecting, capturing, finishing, completed, and failed (for errors).","type":"text"}]}]}]},{"anchor":"Adding-Capture-Object-API-to-your-app","type":"heading","text":"Adding Capture Object API to your app","level":2},{"type":"paragraph","inlineContent":[{"text":"To get started, you need to first import RealityKit and SwiftUI, and then create a session object:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["import RealityKit","import SwiftUI ","","var session = ObjectCaptureSession()"]},{"content":[{"inlineContent":[{"text":"Since ","type":"text"},{"code":"ObjectCaptureSession","type":"codeVoice"},{"text":" is a reference type, it is important to keep it alive through ","type":"text"},{"code":"@StateObject","type":"codeVoice"},{"text":" until the capture process is completed.","type":"text"}],"type":"paragraph"}],"type":"aside","name":"Note","style":"note"},{"type":"paragraph","inlineContent":[{"type":"text","text":"These are the different session states:"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-session-states","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Continuing, call the "},{"type":"codeVoice","code":"start"},{"type":"text","text":" method with a directory indicating where to store the captured images:"}]},{"type":"codeListing","syntax":"swift","code":["var configuration = ObjectCaptureSession.Configuration()","configuration.checkpointDirectory = getDocumentsDir().appendingPathComponent(\"Snapshots\/\")","","session.start(imagesDirectory: getDocumentsDir().appendingPathComponent(\"Images\/\"),","            configuration: configuration)"]},{"type":"paragraph","inlineContent":[{"text":"Once this call is done, it will move to ","type":"text"},{"code":"ready","type":"codeVoice"},{"text":" state.","type":"text"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"You can also provide a checkpoint directory to speed up the reconstruction process."}]}],"type":"aside","name":"Note","style":"note"},{"type":"paragraph","inlineContent":[{"text":"Next, create an ","type":"text"},{"type":"codeVoice","code":"ObjectCaptureView"},{"text":" and provide the session object. This view should be wrapped within another SwiftUI view:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["import RealityKit","import SwiftUI","","struct CapturePrimaryView: View {","    var body: some View {","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"ObjectCaptureView"},{"type":"text","text":" displays the UI corresponding to the current session state."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"To start detecting the object, call "},{"type":"codeVoice","code":"session.startDetecting()"},{"type":"text","text":" from the UI:"}]},{"type":"codeListing","syntax":"swift","code":["var body: some View {","    ZStack {","        ObjectCaptureView(session: session)","        if case .ready = session.state {","            CreateButton(label: \"Continue\") { ","                session.startDetecting() ","            }","        }","    }","}"]},{"type":"paragraph","inlineContent":[{"text":"In this state, the bounding box is displayed, allowing you to adjust the capture edges.","type":"text"},{"text":" ","type":"text"},{"text":"- To capture a different object, call ","type":"text"},{"type":"codeVoice","code":"session.resetDetection()"},{"text":" to return to the ","type":"text"},{"type":"codeVoice","code":"ready"},{"text":" state:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["Button {","    session.resetDetection()","} label: {","    Text(\"Reset\")","}"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Once the bounding box is adjusted, call "},{"code":"session.startCapturing()","type":"codeVoice"},{"type":"text","text":" from the UI to progress to the next state:"}]},{"type":"codeListing","syntax":"swift","code":["var body: some View {","    ZStack {","        ObjectCaptureView(session: session)","        if case .ready = session.state {","            CreateButton(label: \"Continue\") { ","                session.startDetecting()","            }","        } else if case .detecting = session.state {","            CreateButton(label: \"Start Capture\") { ","                session.startCapturing()","            }","        }","    }","}"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The session will automatically capture images while you slowly move around the object. "},{"code":"ObjectCaptureView","type":"codeVoice"},{"type":"text","text":" provides a dial indicating areas that require scanning."}]},{"type":"paragraph","inlineContent":[{"text":"Once the scanning is complete, ","type":"text"},{"type":"codeVoice","code":"session.userCompletedScanPass"},{"text":" is set to ","type":"text"},{"type":"codeVoice","code":"true"},{"text":":","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var body: some View {","    if session.userCompletedScanPass {","        VStack {","        }","    } else {","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The session can detect if the scanned object needs to be flipped to capture parts not yet visible to the camera, such as the bottom. This can be determined using "},{"type":"codeVoice","code":"ObjectCaptureSession.Feedback"},{"type":"text","text":" and checking if "},{"type":"codeVoice","code":"session.feedback.contains(.objectNotFlippable)"},{"type":"text","text":":"}]},{"type":"codeListing","syntax":"swift","code":["if !session.feedback.contains(.objectNotFlippable) && !hasIndicatedFlipObjectAnyway {","    session.beginNewScanPass()","} else {","    session.beginNewScanPassAfterFlip()","}"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"If the user decides to flip the object, "},{"type":"codeVoice","code":"beginNewScanPassAfterFlip()"},{"type":"text","text":" sets the session back to the ready state to adjust the bounding box for the new orientation. Otherwise, "},{"type":"codeVoice","code":"beginNewScanPass()"},{"type":"text","text":" keeps the session in the capturing state, allowing capturing from different heights while maintaining the same bounding box."}]},{"type":"paragraph","inlineContent":[{"text":"Once the three scans are completed, the sample app provides a button to end the session by calling ","type":"text"},{"code":"session.finish()","type":"codeVoice"},{"text":". This transitions the session to the finishing state:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var body: some View {","    if session.userCompletedScanPass {","        VStack {","            CreateButton(label: \"Finish\") {","                session.finish() ","            }","        }","    } else {","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"type":"paragraph","inlineContent":[{"text":"Once the process is finished, the session automatically moves to the ","type":"text"},{"code":"completed","type":"codeVoice"},{"text":" state, triggering on-device reconstruction.","type":"text"}]},{"content":[{"inlineContent":[{"type":"text","text":"If an error occurs during the process, the session enters into failed state, requiring the creation of a new session."}],"type":"paragraph"}],"type":"aside","name":"Note","style":"note"},{"type":"paragraph","inlineContent":[{"text":"Optionally, you can use ","type":"text"},{"type":"codeVoice","code":"ObjectCapturePointCloudView"},{"text":" to preview the scanned parts of the object since its initial placement or last flip:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var body: some View {","    if session.userCompletedScanPass {","        VStack {","            ObjectCapturePointCloudView(session: session)","            CreateButton(label: \"Finish\") {","                session.finish() ","            }","        }","    } else {    ","        ZStack {","            ObjectCaptureView(session: session)","        }","    }","}"]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10191-capture-object-11","type":"image"}]},{"anchor":"Model-Reconstruction-API","type":"heading","text":"Model Reconstruction API","level":2},{"type":"paragraph","inlineContent":[{"text":"The following code from the sample app initiates the reconstruction process, applicable for both iOS and macOS:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var body: some View {","  ReconstructionProgressView()","      .task { \/\/ 1","          var configuration = PhotogrammetrySession.Configuration() ","          configuration.checkpointDirectory = getDocumentsDir()","              .appendingPathComponent(\"Snapshots\/\") \/\/ 3","          let session = try PhotogrammetrySession( \/\/ 2","              input: getDocumentsDir().appendingPathComponent(\"Images\/\"),","              configuration: configuration)","          try session.process(requests: [ \/\/ 4","              .modelFile(url: getDocumentsDir().appendingPathComponent(\"model.usdz\")) ","          ])","          for try await output in session.outputs { \/\/ 5","              switch output {","                  case .processingComplete:","                      handleComplete()","                      \/\/ Handle other Output messages here.","              }","          }","      }","}"]},{"type":"orderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"The ","type":"text"},{"code":"task","type":"codeVoice"},{"text":" modifier is attached to ","type":"text"},{"code":"ReconstructionProgressView","type":"codeVoice"},{"text":".","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"A new ","type":"text"},{"type":"codeVoice","code":"PhotogrammetrySession"},{"text":" is created, pointing to the images.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Optionally, the same checkpoint directory used during image capture can be provided to expedite the reconstruction process.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"The "},{"type":"codeVoice","code":"process"},{"type":"text","text":" function is called to request a model file."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Finally, the message stream is awaited in an asynchronous loop, and output messages are handled as they arrive."}]}]}]},{"anchor":"Detail-level-on-iOS","type":"heading","text":"Detail level on iOS","level":1},{"type":"paragraph","inlineContent":[{"type":"text","text":"To optimize model generation and viewing on mobile devices, iOS supports a reduced detail level. The reconstruction model includes the following texture maps: "},{"isActive":true,"identifier":"https:\/\/docs.cryengine.com\/display\/SDKDOC2\/Diffuse+Maps","type":"reference"},{"type":"text","text":", "},{"isActive":true,"identifier":"https:\/\/en.wikipedia.org\/wiki\/Ambient_occlusion","type":"reference"},{"type":"text","text":", and "},{"isActive":true,"identifier":"https:\/\/en.wikipedia.org\/wiki\/Normal_mapping","type":"reference"},{"type":"text","text":". If you require a higher level of detail, you must transfer your images to macOS for reconstruction."}]},{"anchor":"Capturing-for-Mac","type":"heading","text":"Capturing for Mac","level":1},{"type":"paragraph","inlineContent":[{"text":"Starting from 2023, Mac reconstruction also utilizes the saved LiDAR data in the images. By default, ","type":"text"},{"code":"ObjectCaptureSession","type":"codeVoice"},{"text":" stops capturing images when the reconstruction limit for the iOS device is reached. However, for macOS, you can allow the session to capture more images than on-device reconstruction can use by setting ","type":"text"},{"code":"isOverCaptureEnabled","type":"codeVoice"},{"text":" to ","type":"text"},{"code":"true","type":"codeVoice"},{"text":" in the session’s configurations:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var configuration = ObjectCaptureSession.Configuration()","configuration.isOverCaptureEnabled = true","","session.start(imagesDirectory: getDocumentsDir().appendingPathComponent(\"Images\/\"),","            configuration: configuration)"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"These additional shots are not used for on-device reconstruction but are stored in the Images folder."}]},{"anchor":"Model-Reconstruction-in-Mac","type":"heading","text":"Model Reconstruction in Mac","level":1},{"type":"paragraph","inlineContent":[{"type":"text","text":"No code is required. Simply import the images into Reality Composer Pro, choose a detail level, and obtain your model!"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10191-composer-pro"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"For more details, check out ","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083\/","isActive":true,"type":"reference"}]}],"type":"aside","name":"Note","style":"note"},{"anchor":"Reconstruction-Enhancements","type":"heading","text":"Reconstruction Enhancements","level":1},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Improved performance and image quality on Mac."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The API now provides an estimated processing time in addition to progress percentage."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"High-quality poses can now be requested for each image, including the estimated position and orientation of the camera for that image.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"To obtain the poses, add ","type":"text"},{"type":"codeVoice","code":".poses"},{"text":" to the ","type":"text"},{"type":"codeVoice","code":"process()"},{"text":" function call and handle them when they arrive in the message stream:","type":"text"}],"type":"paragraph"}]}]},{"type":"codeListing","syntax":"swift","code":["try session.process(requests: [ ","    .poses ","    .modelFile(url: modelURL),","])","for try await output in session.outputs {","    switch output {","    case .poses(let poses):","        handlePoses(poses)","    case .processingComplete:","        handleComplete()","    }","}"]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"The API now provides a new custom detail level to control the amount of mesh decimation, texture map resolution, format, and which texture maps should be included."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10191-custom-detail-level"}]},{"anchor":"Written-By","type":"heading","text":"Written By","level":2},{"type":"row","columns":[{"content":[{"inlineContent":[{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/17436242?v=4","type":"image"}],"type":"paragraph"}],"size":1},{"content":[{"text":"Pedro Rojas","type":"heading","anchor":"Pedro-Rojas","level":3},{"type":"paragraph","inlineContent":[{"overridingTitle":"Contributed Notes","isActive":true,"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/pitt500","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/pitt500"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/x.com\/swiftandtips"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/www.youtube.com\/@swiftandtips"}]}],"size":4}],"numberOfColumns":5},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"text","text":"Contributions are welcome!"}]},{"anchor":"Related-Sessions","type":"heading","text":"Related Sessions","level":2},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro"]},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}],"kind":"content"}],"schemaVersion":{"major":0,"minor":3,"patch":0},"kind":"article","seeAlsoSections":[{"title":"New Tools & Frameworks","generated":true,"identifiers":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10184-Meet-ActivityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10032-Meet-Assistive-Access","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10147-Meet-Core-Location-Monitor","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10146-Meet-Core-Location-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10043-Meet-MapKit-for-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10025-Meet-Push-Notifications-Console","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10099-Meet-RealityKit-Trace","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10279-Meet-Safari-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10013-Meet-StoreKit-for-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10171-Meet-Swift-OpenAPI-Generator","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10187-Meet-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10109-Meet-SwiftUI-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111215-Meet-UIKit-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10039-Meet-device-management-for-Apple-Watch","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10268-Meet-mergeable-libraries","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10026-Meet-watchOS-10"]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"metadata":{"title":"Meet Object Capture for iOS","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC23","role":"sampleCode"},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10191-meet-object-capture-for-ios"]}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10191-Meet-Object-Capture-for-iOS"},"sampleCodeDownload":{"action":{"type":"reference","overridingTitle":"Watch Video (20 min)","identifier":"https:\/\/developer.apple.com\/wwdc23\/10191","isActive":true},"kind":"sampleDownload"},"sections":[],"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","title":"WWDC23","type":"topic","role":"collectionGroup","url":"\/documentation\/wwdcnotes\/wwdc23","abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14, tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"code":"SwiftData","type":"codeVoice"},{"type":"text","text":", "},{"code":"Observation","type":"codeVoice"},{"type":"text","text":", "},{"code":"StoreKit","type":"codeVoice"},{"type":"text","text":" views, and more."}]},"WWDC23-10191-capture-object-11":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-capture-object-11.png","traits":["1x","light"]}],"alt":"Point Cloud View","identifier":"WWDC23-10191-capture-object-11"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10146-Meet-Core-Location-for-spatial-computing":{"abstract":[{"text":"Discover how Core Location helps your app find its place in the world — literally. We’ll share how you can build a spatial computing app that uses a person’s location while respecting their privacy. You’ll also learn how your app can request location access and how Core Location adapts requests from compatible iPad and iPhone apps.","type":"text"}],"title":"Meet Core Location for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-10146-meet-core-location-for-spatial-computing","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10146-Meet-Core-Location-for-spatial-computing","role":"sampleCode"},"https://docs.cryengine.com/display/SDKDOC2/Diffuse+Maps":{"url":"https:\/\/docs.cryengine.com\/display\/SDKDOC2\/Diffuse+Maps","type":"link","title":"Diffuse","titleInlineContent":[{"type":"text","text":"Diffuse"}],"identifier":"https:\/\/docs.cryengine.com\/display\/SDKDOC2\/Diffuse+Maps"},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","url":"\/documentation\/wwdcnotes","type":"topic","title":"WWDC Notes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"role":"collection","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}]},"WWDC23-10191-capture-object-1":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-capture-object-1.png","traits":["1x","light"]}],"alt":"Point to the object","identifier":"WWDC23-10191-capture-object-1"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10187-Meet-SwiftData":{"url":"\/documentation\/wwdcnotes\/wwdc23-10187-meet-swiftdata","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10187-Meet-SwiftData","title":"Meet SwiftData","type":"topic","abstract":[{"type":"text","text":"SwiftData is a powerful and expressive persistence framework built for Swift. We’ll show you how you can model your data directly from Swift code, use SwiftData to work with your models, and integrate with SwiftUI."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111215-Meet-UIKit-for-spatial-computing":{"abstract":[{"text":"Learn how to bring your UIKit app to visionOS. We’ll show you how to build for a new destination, explore APIs and best practices for spatial computing, and take your content into the third dimension when you use SwiftUI with UIKit in visionOS.","type":"text"}],"title":"Meet UIKit for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-111215-meet-uikit-for-spatial-computing","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111215-Meet-UIKit-for-spatial-computing","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/pitt500":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/pitt500","url":"\/documentation\/wwdcnotes\/pitt500","kind":"article","title":"Pedro Rojas (1 note)","role":"sampleCode","type":"topic","abstract":[{"type":"text","text":"iOS & Swift Specialist | Content Creator | Engineering Manager at Insulet | Formerly Meta and HP"}]},"WWDC23-10191-capture-object-7":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-7.png"}],"alt":"Final 3D Model","identifier":"WWDC23-10191-capture-object-7"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10032-Meet-Assistive-Access":{"url":"\/documentation\/wwdcnotes\/wwdc23-10032-meet-assistive-access","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10032-Meet-Assistive-Access","title":"Meet Assistive Access","type":"topic","abstract":[{"type":"text","text":"Learn how Assistive Access can help people with cognitive disabilities more easily use iPhone and iPad. Discover the design principles that guide Assistive Access and find out how the system experience adapts to lighten cognitive load. We’ll show you how Assistive Access works and what you can do to support this experience in your app."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10013-Meet-StoreKit-for-SwiftUI":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10013-meet-storekit-for-swiftui","abstract":[{"type":"text","text":"Discover how you can use App Store product metadata and Xcode Previews to add in-app purchases to your app with just a few lines of code. Explore a new collection of UI components in StoreKit and learn how you can easily merchandise your products, present subscriptions in a way that helps users make informed decisions, and more."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10013-Meet-StoreKit-for-SwiftUI","title":"Meet StoreKit for SwiftUI","type":"topic","role":"sampleCode"},"WWDC23-10191-custom-detail-level":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-custom-detail-level.png","traits":["1x","light"]}],"alt":"Custom Detail Level","identifier":"WWDC23-10191-custom-detail-level"},"WWDC23-10191-composer-pro":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-composer-pro.png"}],"alt":"Reality Composer Pro","identifier":"WWDC23-10191-composer-pro"},"https://www.youtube.com/@swiftandtips":{"url":"https:\/\/www.youtube.com\/@swiftandtips","type":"link","title":"Blog","titleInlineContent":[{"text":"Blog","type":"text"}],"identifier":"https:\/\/www.youtube.com\/@swiftandtips"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10279-Meet-Safari-for-spatial-computing":{"abstract":[{"text":"Discover the web for visionOS and learn how people can experience your web content in a whole new way. Explore the unique input model powering this platform and learn how you can optimize your website for spatial computing. We’ll also share how emerging standards are helping shape 3D experiences for the web.","type":"text"}],"title":"Meet Safari for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-10279-meet-safari-for-spatial-computing","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10279-Meet-Safari-for-spatial-computing","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10025-Meet-Push-Notifications-Console":{"url":"\/documentation\/wwdcnotes\/wwdc23-10025-meet-push-notifications-console","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10025-Meet-Push-Notifications-Console","title":"Meet Push Notifications Console","type":"topic","abstract":[{"type":"text","text":"The Push Notifications Console is the best way to quickly test user notifications in your app. Learn how you can iterate on new ideas quickly by sending notifications directly from the console and analyze delivery logs to learn more about your pushes. We’ll also show you how to generate and validate tokens to successfully authenticate with Apple Push Notification service (APNs)."}]},"https://developer.apple.com/videos/play/wwdc2021/10076":{"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10076","type":"link","title":"Create 3D models with Object Capture","titleInlineContent":[{"type":"text","text":"Create 3D models with Object Capture"}],"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10076"},"https://github.com/pitt500":{"url":"https:\/\/github.com\/pitt500","type":"link","title":"GitHub","titleInlineContent":[{"text":"GitHub","type":"text"}],"identifier":"https:\/\/github.com\/pitt500"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10082-Meet-ARKit-for-spatial-computing":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10082-meet-arkit-for-spatial-computing","abstract":[{"type":"text","text":"Discover how you can use ARKit’s tracking and scene understanding features to develop a whole new universe of immersive apps and games. Learn how visionOS and ARKit work together to help you create apps that understand a person’s surroundings — all while preserving privacy. Explore the latest updates to the ARKit API and follow along as we demonstrate how to take advantage of hand tracking and scene geometry in your apps."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10082-Meet-ARKit-for-spatial-computing","title":"Meet ARKit for spatial computing","type":"topic","role":"sampleCode"},"WWDC23-10191-capture-object-8":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-8.png"}],"alt":"Chair with low textures","identifier":"WWDC23-10191-capture-object-8"},"WWDCNotes.png":{"type":"image","variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"alt":null,"identifier":"WWDCNotes.png"},"WWDC23-10191-session-states":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-session-states.png"}],"alt":"Session States","identifier":"WWDC23-10191-session-states"},"WWDC23-10191-capture-object-recap":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-capture-object-recap.png","traits":["1x","light"]}],"alt":"Recap of Object Capture for macOS","identifier":"WWDC23-10191-capture-object-recap"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10083-Meet-Reality-Composer-Pro":{"url":"\/documentation\/wwdcnotes\/wwdc23-10083-meet-reality-composer-pro","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro","title":"Meet Reality Composer Pro","type":"topic","abstract":[{"type":"text","text":"Discover how to easily compose, edit, and preview 3D content with Reality Composer Pro. Follow along as we explore this developer tool by setting up a new project, composing scenes, adding particle emitters and audio, and even previewing content on device."}]},"https://developer.apple.com/documentation/realitykit/guided-capture-sample":{"url":"https:\/\/developer.apple.com\/documentation\/realitykit\/guided-capture-sample","type":"link","title":"link","titleInlineContent":[{"type":"text","text":"link"}],"identifier":"https:\/\/developer.apple.com\/documentation\/realitykit\/guided-capture-sample"},"https://en.wikipedia.org/wiki/Normal_mapping":{"url":"https:\/\/en.wikipedia.org\/wiki\/Normal_mapping","type":"link","title":"normal","titleInlineContent":[{"text":"normal","type":"text"}],"identifier":"https:\/\/en.wikipedia.org\/wiki\/Normal_mapping"},"https://avatars.githubusercontent.com/u/17436242?v=4":{"type":"image","variants":[{"traits":["1x","light"],"url":"https:\/\/avatars.githubusercontent.com\/u\/17436242?v=4"}],"alt":"Profile image of Pedro Rojas","identifier":"https:\/\/avatars.githubusercontent.com\/u\/17436242?v=4"},"WWDC23-10191-capture-object-5":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-capture-object-5.png","traits":["1x","light"]}],"alt":"Flipping Object","identifier":"WWDC23-10191-capture-object-5"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10109-Meet-SwiftUI-for-spatial-computing":{"url":"\/documentation\/wwdcnotes\/wwdc23-10109-meet-swiftui-for-spatial-computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10109-Meet-SwiftUI-for-spatial-computing","title":"Meet SwiftUI for spatial computing","role":"sampleCode","type":"topic","abstract":[{"text":"Take a tour of the solar system with us and explore SwiftUI for visionOS! Discover how you can build an entirely new universe of apps with windows, volumes, and spaces. We’ll show you how to get started with SwiftUI on this platform as we build an astronomy app, add 3D content, and create a fully immersive experience to transport people to the stars.","type":"text"}],"kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10184-Meet-ActivityKit":{"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10184-Meet-ActivityKit","abstract":[{"type":"text","text":"Live Activities are a glanceable way for someone to keep track of the progress of a task within your app. We’ll teach you how you can create helpful experiences for the Lock Screen, the Dynamic Island, and StandBy. Learn how to update your app’s Live Activities, monitor activity state, and take advantage of WidgetKit and SwiftUI to build richer experiences."}],"type":"topic","kind":"article","title":"Meet ActivityKit","url":"\/documentation\/wwdcnotes\/wwdc23-10184-meet-activitykit"},"https://x.com/swiftandtips":{"url":"https:\/\/x.com\/swiftandtips","type":"link","title":"X\/Twitter","titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"identifier":"https:\/\/x.com\/swiftandtips"},"WWDC23-10191-capture-object-10":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-capture-object-10.png","traits":["1x","light"]}],"alt":"Chair with low textures","identifier":"WWDC23-10191-capture-object-10"},"WWDC23-10191-capture-object-2":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-2.png"}],"alt":"Bounding box","identifier":"WWDC23-10191-capture-object-2"},"WWDC23-10191-capture-object-9":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-capture-object-9.png","traits":["1x","light"]}],"alt":"Flippable vs Non-flipabble objects","identifier":"WWDC23-10191-capture-object-9"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10171-Meet-Swift-OpenAPI-Generator":{"abstract":[{"text":"Discover how Swift OpenAPI Generator can help you work with HTTP server APIs whether you’re extending an iOS app or writing a server in Swift. We’ll show you how this package plugin can streamline your workflow and simplify your codebase by generating code from an OpenAPI document.","type":"text"}],"title":"Meet Swift OpenAPI Generator","url":"\/documentation\/wwdcnotes\/wwdc23-10171-meet-swift-openapi-generator","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10171-Meet-Swift-OpenAPI-Generator","role":"sampleCode"},"WWDC23-10191-capture-object-4":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10191-capture-object-4.png"}],"alt":"Feedback requesting more lights, moving slower and arrow to center camera","identifier":"WWDC23-10191-capture-object-4"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10268-Meet-mergeable-libraries":{"url":"\/documentation\/wwdcnotes\/wwdc23-10268-meet-mergeable-libraries","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10268-Meet-mergeable-libraries","title":"Meet mergeable libraries","type":"topic","abstract":[{"type":"text","text":"Discover how mergeable libraries combine the best parts of static and dynamic libraries to help improve your app’s productivity and runtime performance. Learn how you can enable faster development while shipping the smallest app. We’ll show you how to adopt mergeable libraries in Xcode 15 and share best practices for working with your code."}]},"WWDC23-10191-capture-object-3":{"type":"image","variants":[{"url":"\/images\/WWDC23-10191-capture-object-3.png","traits":["1x","light"]}],"alt":"Scanning Regions","identifier":"WWDC23-10191-capture-object-3"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10026-Meet-watchOS-10":{"url":"\/documentation\/wwdcnotes\/wwdc23-10026-meet-watchos-10","role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10026-Meet-watchOS-10","title":"Meet watchOS 10","type":"topic","abstract":[{"type":"text","text":"Discover some of the most significant changes to Apple Watch since its introduction as we tour the redesigned user interface and the new Smart Stack. Learn how Apple designers approached the design of watchOS 10 as we explore layout, navigation, and visual style, and find out how you can apply them to create a great app for Apple Watch."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10147-Meet-Core-Location-Monitor":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10147-meet-core-location-monitor","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10147-Meet-Core-Location-Monitor","type":"topic","title":"Meet Core Location Monitor","abstract":[{"type":"text","text":"Discover how Core Location Monitor can help you better understand location and beacon events in your app. Learn how to use Core Location Conditions to describe and track the state of events in your app, and find out how you can better respond to transitions in your apps through Swift semantics and improved reliability."}],"role":"sampleCode"},"https://developer.apple.com/videos/play/wwdc2023/10083/":{"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083\/","type":"link","title":"Meet Reality Composer Pro","titleInlineContent":[{"type":"text","text":"Meet Reality Composer Pro"}],"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083\/"},"https://developer.apple.com/wwdc23/10191":{"url":"https:\/\/developer.apple.com\/wwdc23\/10191","type":"download","checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc23\/10191"},"https://en.wikipedia.org/wiki/Ambient_occlusion":{"title":"ambient occlusion","identifier":"https:\/\/en.wikipedia.org\/wiki\/Ambient_occlusion","url":"https:\/\/en.wikipedia.org\/wiki\/Ambient_occlusion","titleInlineContent":[{"type":"text","text":"ambient occlusion"}],"type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10043-Meet-MapKit-for-SwiftUI":{"abstract":[{"text":"Discover how expanded SwiftUI support for MapKit has made it easier than ever for you to integrate Maps into your app. We’ll show you how to use SwiftUI to add annotations and overlays to a map, control the camera, and more.","type":"text"}],"title":"Meet MapKit for SwiftUI","url":"\/documentation\/wwdcnotes\/wwdc23-10043-meet-mapkit-for-swiftui","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10043-Meet-MapKit-for-SwiftUI","role":"sampleCode"},"WWDC23-10191-capture-object-6":{"alt":"Scans complete","identifier":"WWDC23-10191-capture-object-6","variants":[{"url":"\/images\/WWDC23-10191-capture-object-6.png","traits":["1x","light"]}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10039-Meet-device-management-for-Apple-Watch":{"abstract":[{"text":"Organizations can now deploy and configure Apple Watch in addition to other Apple devices. Learn how to implement device management for watchOS to help organizations improve productivity, support wellness, and provide additional support for their employees.","type":"text"}],"title":"Meet device management for Apple Watch","url":"\/documentation\/wwdcnotes\/wwdc23-10039-meet-device-management-for-apple-watch","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10039-Meet-device-management-for-Apple-Watch","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10099-Meet-RealityKit-Trace":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10099-meet-realitykit-trace","title":"Meet RealityKit Trace","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10099-Meet-RealityKit-Trace","abstract":[{"type":"text","text":"Discover how you can use RealityKit Trace to improve the performance of your spatial computing apps. Explore performance profiling guidelines for this platform and learn how the RealityKit Trace template can help you optimize rendering for your apps. We’ll also provide guidance on profiling various types of content in your app to help pinpoint performance issues."}],"role":"sampleCode","kind":"article"}}}