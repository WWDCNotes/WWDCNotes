{"metadata":{"role":"sampleCode","roleHeading":"WWDC19","title":"Introducing Core Haptics","modules":[{"name":"WWDC Notes"}]},"schemaVersion":{"major":0,"patch":0,"minor":3},"abstract":[{"text":"Core Haptics lets you design fully customized haptic patterns with synchronized audio. See examples of how haptics and audio enables you to create a greater sense of immersion in your app or game. Learn how to create, play back, and share content, and where Core Haptics fits in with other audio and vibration APIs.","type":"text"}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc19-520-introducing-core-haptics"]}],"primaryContentSections":[{"content":[{"type":"heading","text":"Introduction","level":2,"anchor":"Introduction"},{"type":"paragraph","inlineContent":[{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/documentation\/corehaptics"},{"text":" is a new framework in iOS 13.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"It’s an event based audio and haptic rendering API for iPhone (in short, a synthesizer).","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Compatible with iPhone 8 or later.","type":"text"}]},{"type":"heading","text":"`CoreHaptics` vs `UIFeebackGenerator`","level":2,"anchor":"CoreHaptics-vs-UIFeebackGenerator"},{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"CoreHaptics"},{"type":"text","text":" is not a replacement of "},{"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/uikit\/uifeedbackgenerator","isActive":true},{"type":"text","text":", Apple suggests to keep using it for "},{"code":"UIKit","type":"codeVoice"},{"text":" controls, reactions, and similar.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-520-chVSfg"}]},{"type":"heading","text":"Sound","level":2,"anchor":"Sound"},{"type":"paragraph","inlineContent":[{"text":"We can have a tight connection between audio and haptic.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This has been heavily used internally at Apple for the following applications:"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-520-devices","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"(Yes, all of them have sound!)."}]},{"type":"heading","text":"Game Examples","level":2,"anchor":"Game-Examples"},{"type":"paragraph","inlineContent":[{"text":"This framework is very important for games:","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"for boosting feeling","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"simulate physical contact (think of a tennis game, every time the racket hits the ball)"}]}]}]},{"type":"heading","text":"Classes","level":2,"anchor":"Classes"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Two kinds:"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-520-classes"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Content classes represent the content itself.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Playback classes play the content."}],"type":"paragraph"}]}]},{"type":"heading","text":"`CHHapticEvent`","level":3,"anchor":"CHHapticEvent"},{"type":"paragraph","inlineContent":[{"text":"The basic, indivisible content element.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Each element has a time (period), a type, and, optionally, some parameters.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Multiple Haptic events can overlap, the outcome will be a mix of all of them.","type":"text"}]},{"type":"heading","text":"Haptic types","level":3,"anchor":"Haptic-types"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Haptic Transient"}]},{"text":": momentary and instantaneous","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"text":"Haptic Continuos\/Audio Continuos","type":"text"}]},{"type":"text","text":": lasts longer than transient type, with more parameters to toggle"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"inlineContent":[{"type":"text","text":"AudioCustom"}],"type":"strong"},{"text":": we provide the audio to be played in sync with other haptic types","type":"text"}]}]}]},{"type":"paragraph","inlineContent":[{"text":"All the ","type":"text"},{"code":"CHHApticEvents","type":"codeVoice"},{"text":", are then grouped into ","type":"text"},{"inlineContent":[{"type":"text","text":"Haptic Patterns"}],"type":"strong"},{"text":", ","type":"text"},{"identifier":"https:\/\/developer.apple.com\/documentation\/corehaptics\/chhapticpattern","isActive":true,"type":"reference"},{"text":".","type":"text"}]},{"type":"heading","text":"`AHAP` (Apple Haptic Audio Pattern)","level":3,"anchor":"AHAP-Apple-Haptic-Audio-Pattern"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"New file format which describes a (Haptic) pattern as text."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"It uses JSON."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Can be used with Swift Codable."}]}]}]},{"type":"heading","text":"Written By","level":2,"anchor":"Written-By"},{"type":"row","columns":[{"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"https:\/\/avatars.githubusercontent.com\/u\/5277837?v=4"}]}],"size":1},{"content":[{"text":"Federico Zanetello","level":3,"type":"heading","anchor":"Federico-Zanetello"},{"type":"paragraph","inlineContent":[{"isActive":true,"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"type":"reference","overridingTitle":"Contributed Notes"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/github.com\/zntfdr","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/zntfdr.dev","type":"reference"}]}],"size":4}],"numberOfColumns":5},{"type":"paragraph","inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"type":"heading","text":"Related Sessions","level":2,"anchor":"Related-Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10278-Practice-audio-haptic-design","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-810-Designing-AudioHaptic-Experiences"],"style":"list"},{"type":"small","inlineContent":[{"inlineContent":[{"type":"text","text":"Legal Notice"}],"type":"strong"}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}],"kind":"content"}],"kind":"article","sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-520-Introducing-Core-Haptics"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"sampleCodeDownload":{"action":{"overridingTitle":"Watch Video (29 min)","type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc19\/520","isActive":true},"kind":"sampleDownload"},"seeAlsoSections":[{"generated":true,"title":"New Tools & Frameworks","identifiers":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-718-Introducing-Accelerate-for-Swift","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-722-Introducing-Combine","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-249-Introducing-MultiCamera-Capture-for-iOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-260-Introducing-Photo-Segmentation-Mattes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-245-Introducing-the-Indoor-Maps-Program"]}],"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-722-Introducing-Combine":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-722-Introducing-Combine","kind":"article","title":"Introducing Combine","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-722-introducing-combine","abstract":[{"type":"text","text":"Combine is a unified declarative framework for processing values over time. Learn how it can simplify asynchronous code like networking, key value observing, notifications and callbacks."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"role":"collectionGroup","abstract":[{"text":"Xcode 11, Swift 5.1, iOS 12, macOS 10.15, tvOS 13, watchOS 6.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Core Haptics","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"title":"WWDC19","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"url":"\/documentation\/wwdcnotes\/wwdc19","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","kind":"article"},"WWDC19-520-classes":{"identifier":"WWDC19-520-classes","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-520-classes.png"}],"type":"image"},"https://zntfdr.dev":{"identifier":"https:\/\/zntfdr.dev","titleInlineContent":[{"type":"text","text":"Blog"}],"url":"https:\/\/zntfdr.dev","title":"Blog","type":"link"},"WWDC19-520-devices":{"identifier":"WWDC19-520-devices","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-520-devices.png"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-430-Introducing-the-Create-ML-App":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","kind":"article","title":"Introducing the Create ML App","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-430-introducing-the-create-ml-app","abstract":[{"type":"text","text":"Bringing the power of Core ML to your app begins with one challenge. How do you create your model? The new Create ML app provides an intuitive workflow for model creation. See how to train, evaluate, test, and preview your models quickly in this easy-to-use tool. Get started with one of the many available templates handling a number of powerful machine learning tasks. Learn more about the many features for continuous model improvement and experimentation."}]},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"role":"sampleCode","abstract":[{"text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more.","type":"text"}],"type":"topic","url":"\/documentation\/wwdcnotes\/zntfdr","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","title":"Federico Zanetello (214 notes)","kind":"article"},"https://developer.apple.com/wwdc19/520":{"identifier":"https:\/\/developer.apple.com\/wwdc19\/520","url":"https:\/\/developer.apple.com\/wwdc19\/520","type":"download","checksum":null},"https://avatars.githubusercontent.com/u/5277837?v=4":{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/5277837?v=4","alt":"Profile image of Federico Zanetello","variants":[{"traits":["1x","light"],"url":"https:\/\/avatars.githubusercontent.com\/u\/5277837?v=4"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","kind":"symbol","title":"WWDC Notes","role":"collection","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"type":"topic","url":"\/documentation\/wwdcnotes","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-260-Introducing-Photo-Segmentation-Mattes":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-260-Introducing-Photo-Segmentation-Mattes","kind":"article","title":"Introducing Photo Segmentation Mattes","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-260-introducing-photo-segmentation-mattes","abstract":[{"text":"Photos captured in Portrait Mode on iOS 12 contain an embedded person segmentation matte that made it easy to create creative visual effects like background replacement. iOS 13 leverages on-device machine learning to provide new segmentation mattes for any captured photo. Learn about the new semantic segmentation mattes available to you from both AVCapture and Core Image to isolate a person’s hair, skin, and teeth. Using any of these individual mattes or combining all of them, your app can now offer a tremendous amount of photo editing control.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-718-Introducing-Accelerate-for-Swift":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-718-Introducing-Accelerate-for-Swift","kind":"article","title":"Introducing Accelerate for Swift","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-718-introducing-accelerate-for-swift","abstract":[{"text":"Accelerate framework provides hundreds of computational functions that are highly optimized to the system architecture your device is running on. Learn how to access all of these powerful functions directly in Swift. Understand how the power of vector programming can deliver incredible performance to your iOS, macOS, tvOS, and watchOS apps.","type":"text"}]},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","type":"link"},"WWDC19-520-chVSfg":{"identifier":"WWDC19-520-chVSfg","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-520-chVSfg.png"}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-810-Designing-AudioHaptic-Experiences":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-810-Designing-AudioHaptic-Experiences","kind":"article","title":"Designing Audio-Haptic Experiences","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-810-designing-audiohaptic-experiences","abstract":[{"type":"text","text":"Learn essential sound and haptic design principles and concepts for creating meaningful and delightful experiences that engage a wider range of human senses. Discover how to combine audio and haptics, using the Taptic Engine, to add a new level of realism and improve feedback in your app or game."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10278-Practice-audio-haptic-design":{"role":"sampleCode","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10278-Practice-audio-haptic-design","title":"Practice audio haptic design","url":"\/documentation\/wwdcnotes\/wwdc21-10278-practice-audio-haptic-design","abstract":[{"text":"Discover how you can deliver rich app experiences that include animation, sound, and haptics on iPhone. Learn key concepts for designing multimodal experiences within the Core Haptics framework. We’ll take you through our sample HapticRicochet app — where haptic and sound feedback is designed in harmony with key interactive moments — and show you how to create magical and delightful experiences.","type":"text"}]},"https://developer.apple.com/documentation/corehaptics/chhapticpattern":{"identifier":"https:\/\/developer.apple.com\/documentation\/corehaptics\/chhapticpattern","titleInlineContent":[{"code":"CHHapticPattern","type":"codeVoice"}],"url":"https:\/\/developer.apple.com\/documentation\/corehaptics\/chhapticpattern","title":"CHHapticPattern","type":"link"},"https://developer.apple.com/documentation/uikit/uifeedbackgenerator":{"identifier":"https:\/\/developer.apple.com\/documentation\/uikit\/uifeedbackgenerator","titleInlineContent":[{"code":"UIFeebackGenerator","type":"codeVoice"}],"url":"https:\/\/developer.apple.com\/documentation\/uikit\/uifeedbackgenerator","title":"UIFeebackGenerator","type":"link"},"https://github.com/zntfdr":{"identifier":"https:\/\/github.com\/zntfdr","titleInlineContent":[{"type":"text","text":"GitHub"}],"url":"https:\/\/github.com\/zntfdr","title":"GitHub","type":"link"},"https://developer.apple.com/documentation/corehaptics":{"identifier":"https:\/\/developer.apple.com\/documentation\/corehaptics","titleInlineContent":[{"code":"CoreHaptics","type":"codeVoice"}],"url":"https:\/\/developer.apple.com\/documentation\/corehaptics","title":"CoreHaptics","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-245-Introducing-the-Indoor-Maps-Program":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-245-Introducing-the-Indoor-Maps-Program","kind":"article","title":"Introducing the Indoor Maps Program","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-245-introducing-the-indoor-maps-program","abstract":[{"text":"The Indoor Maps Program enables organizations with large public or private spaces to deliver user experiences that leverage precise location information and present stunning indoor maps. Learn the entire enablement workflow including, creation of a standards-based map definition, map validation, testing and calibration, and details on how to use MapKit and MapKit JS to integrate it all into your app or website.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-249-Introducing-MultiCamera-Capture-for-iOS":{"url":"\/documentation\/wwdcnotes\/wwdc19-249-introducing-multicamera-capture-for-ios","title":"Introducing Multi-Camera Capture for iOS","abstract":[{"type":"text","text":"In AVCapture on iOS 13 it is now possible to simultaneously capture photos and video from multiple cameras on iPhone XS, iPhone XS Max, iPhone XR, and the latest iPad Pro. It is also possible to configure the multiple microphones on the device to shape the sound that is captured. Learn how to leverage these powerful capabilities to bring creative new features like picture-in-picture and spatial audio to your camera apps. Gain a deeper understanding of the performance considerations that may influence your app design."}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-249-Introducing-MultiCamera-Capture-for-iOS","type":"topic","role":"sampleCode"},"WWDCNotes.png":{"identifier":"WWDCNotes.png","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"type":"image"}}}