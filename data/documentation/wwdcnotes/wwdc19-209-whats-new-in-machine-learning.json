{"metadata":{"role":"sampleCode","modules":[{"name":"WWDC Notes"}],"title":"What’s New in Machine Learning","roleHeading":"WWDC19"},"kind":"article","hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc19\/209","overridingTitle":"Watch Video (14 min)"}},"sections":[],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","interfaceLanguage":"swift"},"primaryContentSections":[{"kind":"content","content":[{"level":2,"type":"heading","text":"Overview","anchor":"overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Core ML 3 runs "},{"type":"strong","inlineContent":[{"type":"text","text":"entirely on device"}]},{"type":"text","text":", is hardware accelerated (neural networks), and can update models locally as well."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Domains updates (reminder: all of this runs on device now):"}]},{"level":2,"type":"heading","text":"Vision Updates","anchor":"Vision-Updates"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Image classification","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Image Similarity"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Face Capture Quality"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Human Detection"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Object Saliency: vision can figure out the main subject of a given image and let you know where it is in the image","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Attention Saliency"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Text Recognition"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Animal Detection","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Document Camera (Document recognition and auto OCR)"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Improved Object Tracker"}]}]},{"content":[{"inlineContent":[{"text":"Improved Face Landmarks","type":"text"}],"type":"paragraph"}]}]},{"level":2,"type":"heading","text":"Natural Language Updates","anchor":"Natural-Language-Updates"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Sentiment Analysis: tells if a given phase is positive\/negative etc."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Word Embeddings: tells words similarity (topic-wise, e.g. cloudy and Thunderstorm)","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Text Catalogs"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Transfer Learning","type":"text"}],"type":"paragraph"}]}]},{"level":2,"type":"heading","text":"Speech and Sound Updates","anchor":"Speech-and-Sound-Updates"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Speech Recognition","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Voice Analysis: beside telling you what has been spoken, voice analysis provides also how it is spoken (normal voice and more)"}]}]}]},{"type":"paragraph","inlineContent":[{"text":"We can combine different CoreML Domains seamlessly, for example:","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"categorize images and assign tags to them"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"use speech recognition to catalog text and find the images that match that categorization using word embeddings."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"text":"Improved compatibility with other ML creation tools (TensorFlow and others).","type":"text"}]},{"level":2,"type":"heading","text":"Model Fine-Tuning on device","anchor":"Model-Fine-Tuning-on-device"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"It is now possible to create personalized model per each user, privacy, no server costs."}]}]},{"content":[{"inlineContent":[{"text":"This can be done in the background (see ","type":"text"},{"type":"codeVoice","code":"BackgroundTasks"},{"text":" framework).","type":"text"}],"type":"paragraph"}]}]},{"level":2,"type":"heading","text":"Written By","anchor":"Written-By"},{"type":"row","numberOfColumns":5,"columns":[{"size":1,"content":[{"inlineContent":[{"type":"image","identifier":"zntfdr"}],"type":"paragraph"}]},{"size":4,"content":[{"type":"heading","level":3,"text":"Federico Zanetello","anchor":"Federico-Zanetello"},{"inlineContent":[{"isActive":true,"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"overridingTitle":"Contributed Notes","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/github.com\/zntfdr","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/zntfdr.dev","type":"reference"}],"type":"paragraph"}]}]},{"type":"thematicBreak"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true}]},{"level":2,"type":"heading","text":"Related Sessions","anchor":"Related-Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-222-Understanding-Images-in-Vision-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-232-Advances-in-Natural-Language-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-234-Text-Recognition-in-Vision-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-256-Advances-in-Speech-Recognition","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-426-Building-Activity-Classification-Models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-803-Designing-Great-ML-Experiences"],"style":"list"},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2025 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}]}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc19-209-whats-new-in-machine-learning"],"traits":[{"interfaceLanguage":"swift"}]}],"abstract":[{"text":"Core ML 3 has been greatly expanded to enable even more amazing, on-device machine learning capabilities in your app. Learn about the new Create ML app which makes it easy to build Core ML models for many tasks. Get an overview of model personalization; exciting updates in Vision, Natural Language, Sound, and Speech; and added support for cutting-edge model types.","type":"text"}],"schemaVersion":{"major":0,"patch":0,"minor":3},"references":{"WWDC19-Icon.png":{"alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-Icon.png","traits":["1x","light"]}],"identifier":"WWDC19-Icon.png"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"type":"link","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification":{"title":"Create ML for Object Detection and Sound Classification","url":"\/documentation\/wwdcnotes\/wwdc19-406-create-ml-for-object-detection-and-sound-classification","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification","kind":"article","abstract":[{"type":"text","text":"Create ML enables you to create, evaluate, and test powerful, production-class Core ML models. See how easy it is to create your own Object Detection and Sound Classification models for use in your apps. Learn strategies for balancing your training data to achieve great model accuracy."}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","role":"collection","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","kind":"symbol","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC Notes","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"url":"\/documentation\/wwdcnotes"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-232-Advances-in-Natural-Language-Framework":{"kind":"article","title":"Advances in Natural Language Framework","abstract":[{"text":"Natural Language is a framework designed to provide high-performance, on-device APIs for natural language processing tasks across all Apple platforms. Learn about the addition of Sentiment Analysis and Text Catalog support in the framework. Gain a deeper understanding of transfer learning for text-based models and the new support for Word Embeddings which can power great search experiences in your app.","type":"text"}],"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-232-Advances-in-Natural-Language-Framework","url":"\/documentation\/wwdcnotes\/wwdc19-232-advances-in-natural-language-framework"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-803-Designing-Great-ML-Experiences":{"title":"Designing Great ML Experiences","abstract":[{"text":"Machine learning enables new experiences that understand what we say, suggest things that we may love, and allow us to express ourselves in new, rich ways. Machine learning can make existing experiences better by automating mundane tasks and improving the accuracy and speed of interactions. Learn how to incorporate ML experiences into your apps, and gain practical approaches to designing user interfaces that feel effortlessly helpful.","type":"text"}],"type":"topic","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-803-Designing-Great-ML-Experiences","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19-803-designing-great-ml-experiences"},"zntfdr.jpeg":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/zntfdr.jpeg"}],"identifier":"zntfdr.jpeg","type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-222-Understanding-Images-in-Vision-Framework":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-222-Understanding-Images-in-Vision-Framework","abstract":[{"type":"text","text":"Learn all about the many advances in the Vision Framework including effortless image classification, image saliency, determining image similarity, and improvements in facial feature detection, and face capture quality scoring. This packed session will show you how easy it is to bring powerful computer vision techniques to your apps."}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc19-222-understanding-images-in-vision-framework","kind":"article","title":"Understanding Images in Vision Framework"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"abstract":[{"text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Core Haptics","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"title":"WWDC19","type":"topic","role":"collectionGroup","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","kind":"article","images":[{"type":"icon","identifier":"WWDC19-Icon.png"},{"type":"card","identifier":"WWDC19.jpeg"}],"url":"\/documentation\/wwdcnotes\/wwdc19"},"https://zntfdr.dev":{"url":"https:\/\/zntfdr.dev","title":"Blog","type":"link","titleInlineContent":[{"type":"text","text":"Blog"}],"identifier":"https:\/\/zntfdr.dev"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-234-Text-Recognition-in-Vision-Framework":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-234-Text-Recognition-in-Vision-Framework","title":"Text Recognition in Vision Framework","url":"\/documentation\/wwdcnotes\/wwdc19-234-text-recognition-in-vision-framework","role":"sampleCode","abstract":[{"text":"Document Camera and Text Recognition features in Vision Framework enable you to extract text data from images. Learn how to leverage this built-in machine learning technology in your app. Gain a deeper understanding of the differences between fast versus accurate processing as well as character-based versus language-based recognition.","type":"text"}],"type":"topic","kind":"article"},"WWDC19.jpeg":{"type":"image","alt":null,"identifier":"WWDC19.jpeg","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC19.jpeg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-426-Building-Activity-Classification-Models-in-Create-ML":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-426-Building-Activity-Classification-Models-in-Create-ML","title":"Building Activity Classification Models in Create ML","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-426-building-activity-classification-models-in-create-ml","abstract":[{"type":"text","text":"Your iPhone and Apple Watch are loaded with a number of powerful sensors including an accelerometer and gyroscope. Activity Classifiers can be trained on data from these sensors to bring some magic to your app, such as knowing when someone is running or swinging a bat. Learn how the Create ML app makes it easy to train and evaluate one of these Core ML models. Gain a deeper understanding of how to collect the raw data needed for training. See the use of these models in action."}],"kind":"article","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML":{"kind":"article","abstract":[{"type":"text","text":"Learn how to quickly and easily create Core ML models capable of classifying the sounds heard in audio files and live audio streams. In addition to providing you the ability to train and evaluate these models, the Create ML app allows you to test the model performance in real-time using the microphone on your Mac. Leverage these on-device models in your app using the new Sound Analysis framework."}],"url":"\/documentation\/wwdcnotes\/wwdc19-425-training-sound-classification-models-in-create-ml","role":"sampleCode","title":"Training Sound Classification Models in Create ML","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-256-Advances-in-Speech-Recognition":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-256-Advances-in-Speech-Recognition","type":"topic","title":"Advances in Speech Recognition","abstract":[{"type":"text","text":"Speech Recognizer can now be used locally on iOS or macOS devices with no network connection. Learn how you can bring text-to-speech support to your app while maintaining privacy and eliminating the limitations of server-based processing. Speech recognition API has also been enhanced to provide richer analytics including speaking rate, pause duration, and voice quality."}],"url":"\/documentation\/wwdcnotes\/wwdc19-256-advances-in-speech-recognition","kind":"article","role":"sampleCode"},"https://github.com/zntfdr":{"url":"https:\/\/github.com\/zntfdr","title":"GitHub","identifier":"https:\/\/github.com\/zntfdr","type":"link","titleInlineContent":[{"text":"GitHub","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-430-Introducing-the-Create-ML-App":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-430-introducing-the-create-ml-app","title":"Introducing the Create ML App","abstract":[{"type":"text","text":"Bringing the power of Core ML to your app begins with one challenge. How do you create your model? The new Create ML app provides an intuitive workflow for model creation. See how to train, evaluate, test, and preview your models quickly in this easy-to-use tool. Get started with one of the many available templates handling a number of powerful machine learning tasks. Learn more about the many features for continuous model improvement and experimentation."}],"kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App"},"https://developer.apple.com/wwdc19/209":{"checksum":null,"type":"download","url":"https:\/\/developer.apple.com\/wwdc19\/209","identifier":"https:\/\/developer.apple.com\/wwdc19\/209"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"title":"Federico Zanetello (332 notes)","abstract":[{"type":"text","text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more."}],"type":"topic","role":"sampleCode","images":[{"type":"card","identifier":"zntfdr.jpeg"},{"type":"icon","identifier":"zntfdr.jpeg"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","url":"\/documentation\/wwdcnotes\/zntfdr"},"zntfdr":{"type":"image","alt":"Profile image of Federico Zanetello","identifier":"zntfdr","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/zntfdr.jpeg"}]},"WWDCNotes.png":{"alt":null,"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDCNotes.png"}],"identifier":"WWDCNotes.png"}}}