{"schemaVersion":{"major":0,"patch":0,"minor":3},"abstract":[{"text":"Discover how to reduce the footprint of machine learning models in your app with Core ML Tools. Learn how to use techniques like palettization, pruning, and quantization to dramatically reduce model size while still achieving great accuracy. Explore comparisons between compression during the training stages and on fully trained models, and learn how compressed models can run even faster when your app takes full advantage of the Apple Neural Engine.","type":"text"}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"kind":"article","sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","overridingTitle":"Watch Video (25 min)","identifier":"https:\/\/developer.apple.com\/wwdc23\/10047","isActive":true}},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10047-use-core-ml-tools-for-machine-learning-model-compression"]}],"metadata":{"title":"Use Core ML Tools for machine learning model compression","roleHeading":"WWDC23","role":"sampleCode","modules":[{"name":"WWDC Notes"}]},"sections":[],"primaryContentSections":[{"content":[{"anchor":"overview","level":2,"type":"heading","text":"Overview"},{"inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"reference"}],"type":"paragraph"},{"anchor":"Related-Sessions","level":2,"type":"heading","text":"Related Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10153-Get-models-on-device-using-Core-ML-Converters"],"type":"links","style":"list"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright ¬© 2012 ‚Äì 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}],"kind":"content"}],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression","interfaceLanguage":"swift"},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10153-Get-models-on-device-using-Core-ML-Converters":{"title":"Get models on device using Core ML Converters","abstract":[{"text":"With Core ML you can bring incredible machine learning models to your app and run them entirely on-device. And when you use Core ML Converters, you can incorporate almost any trained model from TensorFlow or PyTorch and take full advantage of the GPU, CPU, and Neural Engine. Discover everything you need to begin converting existing models from other ML platforms and explore how to create custom operations that extend the capabilities of your models.","type":"text"}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc20-10153-get-models-on-device-using-core-ml-converters","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10153-Get-models-on-device-using-Core-ML-Converters","type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"title":"WWDC23","role":"collectionGroup","kind":"article","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","url":"\/documentation\/wwdcnotes\/wwdc23","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}]},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"type":"link","title":"Learn More‚Ä¶","identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}]},"https://developer.apple.com/wwdc23/10047":{"type":"download","identifier":"https:\/\/developer.apple.com\/wwdc23\/10047","url":"https:\/\/developer.apple.com\/wwdc23\/10047","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","title":"WWDC Notes","kind":"symbol","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection"},"WWDCNotes.png":{"type":"image","alt":null,"identifier":"WWDCNotes.png","variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}]}}}