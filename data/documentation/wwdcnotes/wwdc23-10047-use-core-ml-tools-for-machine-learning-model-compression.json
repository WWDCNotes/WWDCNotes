{"sections":[],"metadata":{"roleHeading":"WWDC23","modules":[{"name":"WWDC Notes"}],"role":"sampleCode","title":"Use Core ML Tools for machine learning model compression"},"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10047","overridingTitle":"Watch Video (25 min)","isActive":true}},"schemaVersion":{"minor":3,"patch":0,"major":0},"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression"},"kind":"article","abstract":[{"type":"text","text":"Discover how to reduce the footprint of machine learning models in your app with Core ML Tools. Learn how to use techniques like palettization, pruning, and quantization to dramatically reduce model size while still achieving great accuracy. Explore comparisons between compression during the training stages and on fully trained models, and learn how compressed models can run even faster when your app takes full advantage of the Apple Neural Engine."}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10047-use-core-ml-tools-for-machine-learning-model-compression"]}],"primaryContentSections":[{"kind":"content","content":[{"type":"heading","text":"Overview","anchor":"overview","level":2},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true,"type":"reference"}]},{"type":"heading","text":"Related Sessions","anchor":"Related-Sessions","level":2},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10153-Get-models-on-device-using-Core-ML-Converters"]}]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10153-Get-models-on-device-using-Core-ML-Converters":{"url":"\/documentation\/wwdcnotes\/wwdc20-10153-get-models-on-device-using-core-ml-converters","role":"sampleCode","title":"Get models on device using Core ML Converters","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10153-Get-models-on-device-using-Core-ML-Converters","abstract":[{"text":"With Core ML you can bring incredible machine learning models to your app and run them entirely on-device. And when you use Core ML Converters, you can incorporate almost any trained model from TensorFlow or PyTorch and take full advantage of the GPU, CPU, and Neural Engine. Discover everything you need to begin converting existing models from other ML platforms and explore how to create custom operations that extend the capabilities of your models.","type":"text"}],"type":"topic","kind":"article"},"https://developer.apple.com/videos/play/wwdc2023/10047":{"type":"download","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10047","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10047","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes":{"url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","role":"collection","title":"WWDC Notes","type":"topic","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"kind":"symbol"},"WWDCNotes.png":{"identifier":"WWDCNotes.png","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"url":"\/documentation\/wwdcnotes\/wwdc23","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","role":"collectionGroup","title":"WWDC23","type":"topic","abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"SwiftData"},{"type":"text","text":", "},{"type":"codeVoice","code":"Observation"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit"},{"type":"text","text":" views, and more."}],"images":[{"type":"icon","identifier":"WWDC23-Icon.png"},{"type":"card","identifier":"WWDC23.jpeg"}],"kind":"article"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"type":"link","titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶"},"WWDC23-Icon.png":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-Icon.png","traits":["1x","light"]}],"identifier":"WWDC23-Icon.png","alt":null},"WWDC23.jpeg":{"identifier":"WWDC23.jpeg","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23.jpeg","traits":["1x","light"]}],"alt":null}}}