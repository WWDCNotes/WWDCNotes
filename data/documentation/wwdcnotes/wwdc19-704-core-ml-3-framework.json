{"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework","interfaceLanguage":"swift"},"schemaVersion":{"minor":3,"patch":0,"major":0},"kind":"article","sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"overridingTitle":"Watch Video (40 min)","type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc19\/704"}},"metadata":{"modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC19","title":"Core ML 3 Framework","role":"sampleCode"},"abstract":[{"type":"text","text":"Core ML 3 now enables support for advanced model types that were never before available in on-device machine learning. Learn how model personalization brings amazing personalization opportunities to your app. Gain a deeper understanding of strategies for linking models and improvements to Core ML tools used for conversion of existing models."}],"sections":[],"primaryContentSections":[{"content":[{"anchor":"On-device-model-personalization","level":2,"type":"heading","text":"On device model personalization"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Previously only bundles in the app"}]}]},{"content":[{"inlineContent":[{"text":"Apple has intentionally avoided trying to load models from the cloud as this compromises privacy and scalability","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Provide some new training examples and receive a new model"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Demo uses pencil kit to capture input drawing and find a emoji. With the new API the user can train the model to understand the user’s way of drawing."}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Import an editable CoreML model this model has some parameters for updating declared","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"CoreML generates some classes to work with these inputs"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"To update:","type":"text"}]},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Load the model from the bundle"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Prepare the training data","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Create an MLUpdateTask which async produces a CoreML model","type":"text"}]}]}],"type":"unorderedList"}]}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"We can update the model in the background. iOS will give you up to several minutes (see notes on the background tasks framework)","type":"text"}]}]}],"type":"unorderedList"},{"type":"paragraph","inlineContent":[{"text":"In short, Core ML 3 lets us update the model entirely locally (ad-hoc for the given user).","type":"text"}]},{"anchor":"Whats-inside-a-MLmodel","level":2,"type":"heading","text":"What’s inside a MLmodel"},{"type":"paragraph","inlineContent":[{"text":"| ","type":"text"},{"identifier":"WWDC19-704-ml2","type":"image"},{"text":" | ","type":"text"},{"identifier":"WWDC19-704-ml3","type":"image"},{"text":" |","type":"text"},{"text":" ","type":"text"},{"text":"| Core ML 2 | Core ML 3 |","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"In CoreML 2 models consist mostly of:","type":"text"}]},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Metadata: licensing and authors"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Interface: describes how our app can interact with this model.","type":"text"}]}]}],"type":"unorderedList"},{"type":"paragraph","inlineContent":[{"type":"text","text":"What’s new on CoreML 3:"}]},{"items":[{"content":[{"inlineContent":[{"text":"Update parameters: describes what parts of the model are updatable and how it’s updatable.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Update Interface: model interface our app can leverage to make these updates happen.","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"anchor":"Neural-Networks","level":2,"type":"heading","text":"Neural Networks"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Neural networks are great at solving challenging task, such as understanding the content of an image or a document or an audio clip."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-704-segmentation","type":"image"}]},{"items":[{"content":[{"inlineContent":[{"text":"Core ML 3 has more advanced features available for Control flow in Neural Networks (e.g. Branching, Loops)","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Internal improvements: Made existing layers more generic, control flow, add new mathematical operations","type":"text"},{"text":" ","type":"text"},{"identifier":"WWDC19-704-layers","type":"image"}]}]}],"type":"unorderedList"},{"anchor":"BERT-in-depth","level":2,"type":"heading","text":"BERT in depth"},{"type":"paragraph","inlineContent":[{"type":"text","text":"BERT ("},{"inlineContent":[{"type":"text","text":"B"}],"type":"strong"},{"type":"text","text":"idirectional "},{"inlineContent":[{"type":"text","text":"E"}],"type":"strong"},{"type":"text","text":"ncoder "},{"inlineContent":[{"type":"text","text":"R"}],"type":"strong"},{"type":"text","text":"epresentation from "},{"inlineContent":[{"text":"T","type":"text"}],"type":"strong"},{"type":"text","text":"ransformers) is state of the art machine learning model."}]},{"type":"paragraph","inlineContent":[{"text":"The BERT model is actually a neural network that can perform multiple tasks for natural language understanding.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Inside the BERT model there are a bunch of modules."}]},{"type":"paragraph","inlineContent":[{"text":"And inside these modules there are a bunch of layers.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-704-tokenized","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"(Models on the left, layers on the right)"}]},{"type":"paragraph","inlineContent":[{"text":"In short this is what’s happening behind the scenes of the demo:","type":"text"},{"text":" ","type":"text"},{"identifier":"WWDC19-704-when","type":"image"},{"text":" ","type":"text"},{"identifier":"WWDC19-704-when","type":"image"}]},{"anchor":"Other-improvements","level":2,"type":"heading","text":"Other improvements"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Linked Models: Shared models between pipelines.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Image Features: now images (from url, or ","type":"text"},{"code":"CGImageSource","type":"codeVoice"},{"text":") are supported for models rather than just pixel buffers.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"code":"MLModelConfiguration","type":"codeVoice"},{"text":" can set preferred MetalDevice (use full for multi GPU env) and an option to speed up the performance of the GPU calculations (at accuracy cost).","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"anchor":"Written-By","level":2,"type":"heading","text":"Written By"},{"columns":[{"size":1,"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"zntfdr"}]}]},{"size":4,"content":[{"text":"Federico Zanetello","type":"heading","level":3,"anchor":"Federico-Zanetello"},{"type":"paragraph","inlineContent":[{"isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"overridingTitle":"Contributed Notes","type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/zntfdr"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/zntfdr.dev"}]}]}],"numberOfColumns":5,"type":"row"},{"type":"paragraph","inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"anchor":"Related-Sessions","level":2,"type":"heading","text":"Related Sessions"},{"style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-232-Advances-in-Natural-Language-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-234-Text-Recognition-in-Vision-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-256-Advances-in-Speech-Recognition","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML"],"type":"links"},{"type":"small","inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}],"kind":"content"}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc19-704-core-ml-3-framework"]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"references":{"WWDC19-704-segmentation":{"identifier":"WWDC19-704-segmentation","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-704-segmentation.png"}]},"doc://WWDCNotes/documentation/WWDCNotes":{"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","kind":"symbol","title":"WWDC Notes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"url":"\/documentation\/wwdcnotes","type":"topic","role":"collection"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-232-Advances-in-Natural-Language-Framework":{"url":"\/documentation\/wwdcnotes\/wwdc19-232-advances-in-natural-language-framework","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-232-Advances-in-Natural-Language-Framework","role":"sampleCode","kind":"article","title":"Advances in Natural Language Framework","type":"topic","abstract":[{"type":"text","text":"Natural Language is a framework designed to provide high-performance, on-device APIs for natural language processing tasks across all Apple platforms. Learn about the addition of Sentiment Analysis and Text Catalog support in the framework. Gain a deeper understanding of transfer learning for text-based models and the new support for Word Embeddings which can power great search experiences in your app."}]},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!"},"WWDC19-704-tokenized":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-704-tokenized.png"}],"alt":null,"identifier":"WWDC19-704-tokenized"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-256-Advances-in-Speech-Recognition":{"role":"sampleCode","type":"topic","kind":"article","title":"Advances in Speech Recognition","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-256-Advances-in-Speech-Recognition","abstract":[{"type":"text","text":"Speech Recognizer can now be used locally on iOS or macOS devices with no network connection. Learn how you can bring text-to-speech support to your app while maintaining privacy and eliminating the limitations of server-based processing. Speech recognition API has also been enhanced to provide richer analytics including speaking rate, pause duration, and voice quality."}],"url":"\/documentation\/wwdcnotes\/wwdc19-256-advances-in-speech-recognition"},"zntfdr":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/zntfdr.jpeg"}],"alt":"Profile image of Federico Zanetello","identifier":"zntfdr"},"zntfdr.jpeg":{"identifier":"zntfdr.jpeg","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/zntfdr.jpeg"}]},"WWDC19-704-ml2":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-704-ml2.png"}],"type":"image","identifier":"WWDC19-704-ml2"},"WWDC19-Icon.png":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-Icon.png"}],"alt":null,"identifier":"WWDC19-Icon.png"},"WWDC19.jpeg":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19.jpeg"}],"alt":null,"identifier":"WWDC19.jpeg"},"https://github.com/zntfdr":{"url":"https:\/\/github.com\/zntfdr","title":"GitHub","identifier":"https:\/\/github.com\/zntfdr","type":"link","titleInlineContent":[{"text":"GitHub","type":"text"}]},"WWDC19-704-layers":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-704-layers.png"}],"type":"image","identifier":"WWDC19-704-layers"},"WWDC19-704-ml3":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-704-ml3.png"}],"alt":null,"identifier":"WWDC19-704-ml3"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"abstract":[{"text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more.","type":"text"}],"type":"topic","url":"\/documentation\/wwdcnotes\/zntfdr","role":"sampleCode","title":"Federico Zanetello (332 notes)","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","kind":"article","images":[{"type":"card","identifier":"zntfdr.jpeg"},{"type":"icon","identifier":"zntfdr.jpeg"}]},"https://zntfdr.dev":{"title":"Blog","type":"link","url":"https:\/\/zntfdr.dev","titleInlineContent":[{"text":"Blog","type":"text"}],"identifier":"https:\/\/zntfdr.dev"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"title":"WWDC19","abstract":[{"type":"text","text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6."},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Core Haptics","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"kind":"article","role":"collectionGroup","url":"\/documentation\/wwdcnotes\/wwdc19","type":"topic","images":[{"type":"icon","identifier":"WWDC19-Icon.png"},{"type":"card","identifier":"WWDC19.jpeg"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"},"WWDC19-704-when":{"identifier":"WWDC19-704-when","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-704-when.png"}]},"https://developer.apple.com/wwdc19/704":{"url":"https:\/\/developer.apple.com\/wwdc19\/704","checksum":null,"type":"download","identifier":"https:\/\/developer.apple.com\/wwdc19\/704"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML":{"url":"\/documentation\/wwdcnotes\/wwdc19-425-training-sound-classification-models-in-create-ml","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","role":"sampleCode","kind":"article","title":"Training Sound Classification Models in Create ML","type":"topic","abstract":[{"type":"text","text":"Learn how to quickly and easily create Core ML models capable of classifying the sounds heard in audio files and live audio streams. In addition to providing you the ability to train and evaluate these models, the Create ML app allows you to test the model performance in real-time using the microphone on your Mac. Leverage these on-device models in your app using the new Sound Analysis framework."}]},"WWDCNotes.png":{"identifier":"WWDCNotes.png","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-234-Text-Recognition-in-Vision-Framework":{"url":"\/documentation\/wwdcnotes\/wwdc19-234-text-recognition-in-vision-framework","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-234-Text-Recognition-in-Vision-Framework","role":"sampleCode","kind":"article","title":"Text Recognition in Vision Framework","type":"topic","abstract":[{"type":"text","text":"Document Camera and Text Recognition features in Vision Framework enable you to extract text data from images. Learn how to leverage this built-in machine learning technology in your app. Gain a deeper understanding of the differences between fast versus accurate processing as well as character-based versus language-based recognition."}]}}}