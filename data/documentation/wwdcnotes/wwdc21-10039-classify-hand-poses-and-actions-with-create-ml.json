{"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML"},"metadata":{"role":"sampleCode","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC21","title":"Classify hand poses and actions with Create ML"},"sampleCodeDownload":{"kind":"sampleDownload","action":{"identifier":"https:\/\/developer.apple.com\/wwdc21\/10039","isActive":true,"overridingTitle":"Watch Video (26 min)","type":"reference"}},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc21-10039-classify-hand-poses-and-actions-with-create-ml"],"traits":[{"interfaceLanguage":"swift"}]}],"sections":[],"abstract":[{"text":"With Create ML, your app‚Äôs ability to understand the expressiveness of the human hand has never been easier. Discover how you can build off the support for Hand Pose Detection in Vision and train custom Hand Pose and Hand Action classifiers using the Create ML app and framework. Learn how simple it is to collect data, train a model, and integrate it with Vision, Camera, and ARKit to create a fun, entertaining app experience.","type":"text"}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21"]]},"kind":"article","schemaVersion":{"minor":3,"major":0,"patch":0},"primaryContentSections":[{"content":[{"type":"heading","anchor":"overview","level":2,"text":"Overview"},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true,"type":"reference"}]},{"type":"heading","anchor":"Related-Sessions","level":2,"text":"Related Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110332-Whats-new-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision"],"type":"links","style":"list"}],"kind":"content"}],"references":{"WWDCNotes.png":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"alt":null,"identifier":"WWDCNotes.png"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}],"type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit":{"type":"topic","title":"Integrate with motorized iPhone stands using DockKit","abstract":[{"type":"text","text":"Discover how you can create incredible photo and video experiences in your camera app when integrating with DockKit-compatible motorized stands. We‚Äôll show how your app can automatically track subjects in live video across a 360-degree field of view, take direct control of the stand to customize framing, directly control the motors, and provide your own inference model for tracking other objects. Finally, we‚Äôll demonstrate how to create a sense of emotion through dynamic device animations."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10304-integrate-with-motorized-iphone-stands-using-dockkit","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision","url":"\/documentation\/wwdcnotes\/wwdc20-10653-detect-body-and-hand-pose-with-vision","title":"Detect Body and Hand Pose with Vision","type":"topic","kind":"article","abstract":[{"type":"text","text":"Explore how the Vision framework can help your app detect body and hand poses in photos and video. With pose detection, your app can analyze the poses, movements, and gestures of people to offer new video editing possibilities, or to perform action classification when paired with an action classifier built in Create ML. And we‚Äôll show you how you can bring gesture recognition into your app through hand pose, delivering a whole new form of interaction."}],"role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10019-Get-to-know-Create-ML-Components":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","title":"Get to know Create ML Components","type":"topic","abstract":[{"type":"text","text":"Create ML makes it easy to build custom machine learning models for image classification, object detection, sound classification, hand pose classification, action classification, tabular data regression, and more. And with the Create ML Components framework, you can further customize underlying tasks and improve your model. We‚Äôll explore the feature extractors, transformers, and estimators that make up these tasks, and show you how you can combine them with other components and pre-processing steps to build custom tasks for concepts like image regression."}],"url":"\/documentation\/wwdcnotes\/wwdc22-10019-get-to-know-create-ml-components","role":"sampleCode","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-110332-Whats-new-in-Create-ML":{"type":"topic","role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc22-110332-whats-new-in-create-ml","title":"What‚Äôs new in Create ML","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110332-Whats-new-in-Create-ML","abstract":[{"type":"text","text":"Discover the latest updates to Create ML. We‚Äôll share improvements to Create ML‚Äôs evaluation tools that can help you understand how your custom models will perform on real-world data. Learn how you can check model performance on each type of image in your test data and identify problems within individual images to help you troubleshoot mistaken classifications, poorly labeled data, and other errors. We‚Äôll also show you how to test your model with iPhone and iPad in live preview using Continuity Camera, and share how you can take Action Classification even further with the new Repetition Counting capabilities of the Create ML Components framework."}]},"WWDC21.jpeg":{"type":"image","alt":null,"identifier":"WWDC21.jpeg","variants":[{"url":"\/images\/WWDCNotes\/WWDC21.jpeg","traits":["1x","light"]}]},"https://developer.apple.com/wwdc21/10039":{"url":"https:\/\/developer.apple.com\/wwdc21\/10039","type":"download","identifier":"https:\/\/developer.apple.com\/wwdc21\/10039","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes":{"url":"\/documentation\/wwdcnotes","kind":"symbol","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"type":"topic","role":"collection","title":"WWDC Notes"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21":{"abstract":[{"text":"Xcode 13, Swift 5.5, iOS 15, macOS 12 (Monterey), tvOS 15, watchOS 8.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"type":"codeVoice","code":"MusicKit"},{"text":", ","type":"text"},{"type":"codeVoice","code":"DocC"},{"text":", ","type":"text"},{"type":"codeVoice","code":"StoreKit 2"},{"text":", and more.","type":"text"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21","role":"collectionGroup","title":"WWDC21","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc21","images":[{"type":"icon","identifier":"WWDC21-Icon.png"},{"type":"card","identifier":"WWDC21.jpeg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","url":"\/documentation\/wwdcnotes\/wwdc20-10043-build-an-action-classifier-with-create-ml","title":"Build an Action Classifier with Create ML","type":"topic","kind":"article","abstract":[{"type":"text","text":"Discover how to build Action Classification models in Create ML. With a custom action classifier, your app can recognize and understand body movements in real-time from videos or through a camera. We‚Äôll show you how to use samples to easily train a Core ML model to identify human actions like jumping jacks, squats, and dance moves. Learn how this is powered by the Body Pose estimation features of the Vision Framework. Get inspired to create apps that can provide coaching for fitness routines, deliver feedback on athletic form, and more."}],"role":"sampleCode"},"WWDC21-Icon.png":{"identifier":"WWDC21-Icon.png","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC21-Icon.png","traits":["1x","light"]}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10040-Detect-people-faces-and-poses-using-Vision":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","title":"Detect people, faces, and poses using Vision","type":"topic","abstract":[{"type":"text","text":"Discover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app."}],"url":"\/documentation\/wwdcnotes\/wwdc21-10040-detect-people-faces-and-poses-using-vision","role":"sampleCode","kind":"article"}}}