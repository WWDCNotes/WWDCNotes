{"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-426-Building-Activity-Classification-Models-in-Create-ML"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"sections":[],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc19-426-building-activity-classification-models-in-create-ml"],"traits":[{"interfaceLanguage":"swift"}]}],"schemaVersion":{"major":0,"patch":0,"minor":3},"kind":"article","abstract":[{"text":"Your iPhone and Apple Watch are loaded with a number of powerful sensors including an accelerometer and gyroscope. Activity Classifiers can be trained on data from these sensors to bring some magic to your app, such as knowing when someone is running or swinging a bat. Learn how the Create ML app makes it easy to train and evaluate one of these Core ML models. Gain a deeper understanding of how to collect the raw data needed for training. See the use of these models in action.","type":"text"}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"overridingTitle":"Watch Video (15 min)","type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc19\/426"}},"primaryContentSections":[{"kind":"content","content":[{"text":"Sensors","anchor":"Sensors","level":2,"type":"heading"},{"inlineContent":[{"text":"Our devices have tons of sensors:","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC19-426-sensors"}],"type":"paragraph"},{"text":"What is an Activity Classification?","anchor":"What-is-an-Activity-Classification","level":2,"type":"heading"},{"inlineContent":[{"type":"text","text":"Activity Classification is a task that allows us to recognize our pre-defined set of physical actions that the user does with their devices."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"In the session, the presenter shows us an example of a Fressbee throws classifier."}],"type":"paragraph"},{"inlineContent":[{"text":"An example of activity data (it’s a csv table with time stamps and ","type":"text"},{"code":"x, y, z","type":"codeVoice"},{"text":" values):","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC19-426-training","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"In Create ML we can filter which axis of which acceleration\/rotation we should consider for the training, we can also define a Prediction Window Size to let Create ML know how much is the size of the sample to analyze (this way we can have multiple gestures\/measurements in one table).","type":"text"}],"type":"paragraph"},{"text":"Best practices","anchor":"Best-practices","level":2,"type":"heading"},{"items":[{"content":[{"inlineContent":[{"text":"Use relevant sensor for your motion (understand your motion)","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Collect irrelevant motions as “other”, to avoid false positive"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Provide balanced classes (same number of samples for each class\/category)","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Provide raw data instead of processed device motion data","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"text":"Written By","anchor":"Written-By","level":2,"type":"heading"},{"numberOfColumns":5,"columns":[{"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"zntfdr"}]}],"size":1},{"content":[{"level":3,"anchor":"Federico-Zanetello","text":"Federico Zanetello","type":"heading"},{"inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","overridingTitle":"Contributed Notes","type":"reference","isActive":true,"overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}]},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/github.com\/zntfdr","type":"reference","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"text":" ","type":"text"},{"identifier":"https:\/\/zntfdr.dev","isActive":true,"type":"reference"}],"type":"paragraph"}],"size":4}],"type":"row"},{"inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}],"type":"paragraph"},{"text":"Related Sessions","anchor":"Related-Sessions","level":2,"type":"heading"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App"],"style":"list","type":"links"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}],"type":"small"},{"inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}]}],"metadata":{"title":"Building Activity Classification Models in Create ML","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC19","role":"sampleCode"},"references":{"WWDC19-Icon.png":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-Icon.png"}],"alt":null,"identifier":"WWDC19-Icon.png","type":"image"},"zntfdr":{"variants":[{"url":"\/images\/zntfdr.jpeg","traits":["1x","light"]}],"alt":"Profile image of Federico Zanetello","identifier":"zntfdr","type":"image"},"https://zntfdr.dev":{"title":"Blog","titleInlineContent":[{"type":"text","text":"Blog"}],"url":"https:\/\/zntfdr.dev","identifier":"https:\/\/zntfdr.dev","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification":{"abstract":[{"text":"Create ML enables you to create, evaluate, and test powerful, production-class Core ML models. See how easy it is to create your own Object Detection and Sound Classification models for use in your apps. Learn strategies for balancing your training data to achieve great model accuracy.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification","url":"\/documentation\/wwdcnotes\/wwdc19-406-create-ml-for-object-detection-and-sound-classification","title":"Create ML for Object Detection and Sound Classification","type":"topic","kind":"article","role":"sampleCode"},"WWDC19-426-sensors":{"variants":[{"url":"\/images\/WWDC19-426-sensors.png","traits":["1x","light"]}],"alt":null,"identifier":"WWDC19-426-sensors","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-430-Introducing-the-Create-ML-App":{"type":"topic","kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc19-430-introducing-the-create-ml-app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","abstract":[{"type":"text","text":"Bringing the power of Core ML to your app begins with one challenge. How do you create your model? The new Create ML app provides an intuitive workflow for model creation. See how to train, evaluate, test, and preview your models quickly in this easy-to-use tool. Get started with one of the many available templates handling a number of powerful machine learning tasks. Learn more about the many features for continuous model improvement and experimentation."}],"title":"Introducing the Create ML App"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"images":[{"identifier":"zntfdr.jpeg","type":"card"},{"identifier":"zntfdr.jpeg","type":"icon"}],"type":"topic","url":"\/documentation\/wwdcnotes\/zntfdr","abstract":[{"type":"text","text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more."}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","title":"Federico Zanetello (332 notes)","role":"sampleCode"},"WWDC19-426-training":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-426-training.png"}],"alt":null,"identifier":"WWDC19-426-training","type":"image"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"alt":null,"identifier":"WWDCNotes.png","type":"image"},"WWDC19.jpeg":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19.jpeg"}],"alt":null,"identifier":"WWDC19.jpeg","type":"image"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Contributions are welcome!","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","images":[{"identifier":"WWDC19-Icon.png","type":"icon"},{"identifier":"WWDC19.jpeg","type":"card"}],"url":"\/documentation\/wwdcnotes\/wwdc19","type":"topic","kind":"article","abstract":[{"text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Core Haptics","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"role":"collectionGroup","title":"WWDC19"},"https://developer.apple.com/wwdc19/426":{"url":"https:\/\/developer.apple.com\/wwdc19\/426","identifier":"https:\/\/developer.apple.com\/wwdc19\/426","type":"download","checksum":null},"https://github.com/zntfdr":{"title":"GitHub","titleInlineContent":[{"type":"text","text":"GitHub"}],"url":"https:\/\/github.com\/zntfdr","type":"link","identifier":"https:\/\/github.com\/zntfdr"},"zntfdr.jpeg":{"alt":null,"type":"image","identifier":"zntfdr.jpeg","variants":[{"traits":["1x","light"],"url":"\/images\/zntfdr.jpeg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-209-Whats-New-in-Machine-Learning":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","title":"What’s New in Machine Learning","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-209-whats-new-in-machine-learning","abstract":[{"text":"Core ML 3 has been greatly expanded to enable even more amazing, on-device machine learning capabilities in your app. Learn about the new Create ML app which makes it easy to build Core ML models for many tasks. Get an overview of model personalization; exciting updates in Vision, Natural Language, Sound, and Speech; and added support for cutting-edge model types.","type":"text"}],"kind":"article","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes":{"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"type":"topic","kind":"symbol","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC Notes","role":"collection","url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes"}}}