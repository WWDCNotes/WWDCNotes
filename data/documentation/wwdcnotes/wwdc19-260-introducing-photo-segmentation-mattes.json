{"primaryContentSections":[{"content":[{"type":"heading","text":"Overview","anchor":"overview","level":2},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:"},{"type":"text","text":" "},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"type":"heading","text":"Related Sessions","anchor":"Related-Sessions","level":2}],"kind":"content"}],"abstract":[{"type":"text","text":"Photos captured in Portrait Mode on iOS 12 contain an embedded person segmentation matte that made it easy to create creative visual effects like background replacement. iOS 13 leverages on-device machine learning to provide new segmentation mattes for any captured photo. Learn about the new semantic segmentation mattes available to you from both AVCapture and Core Image to isolate a person‚Äôs hair, skin, and teeth. Using any of these individual mattes or combining all of them, your app can now offer a tremendous amount of photo editing control."}],"sections":[],"sampleCodeDownload":{"kind":"sampleDownload","action":{"identifier":"https:\/\/developer.apple.com\/wwdc19\/260","type":"reference","overridingTitle":"Watch Video (15 min)","isActive":true}},"kind":"article","metadata":{"title":"Introducing Photo Segmentation Mattes","roleHeading":"WWDC19","modules":[{"name":"WWDC Notes"}],"role":"sampleCode"},"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-260-Introducing-Photo-Segmentation-Mattes"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc19-260-introducing-photo-segmentation-mattes"]}],"schemaVersion":{"minor":3,"major":0,"patch":0},"references":{"https://developer.apple.com/wwdc19/260":{"url":"https:\/\/developer.apple.com\/wwdc19\/260","identifier":"https:\/\/developer.apple.com\/wwdc19\/260","type":"download","checksum":null},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"text":"Learn More‚Ä¶","type":"text"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19","images":[{"identifier":"WWDC19-Icon.png","type":"icon"},{"identifier":"WWDC19.jpeg","type":"card"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","abstract":[{"text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Core Haptics","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"title":"WWDC19","kind":"article","role":"collectionGroup"},"doc://WWDCNotes/documentation/WWDCNotes":{"role":"collection","type":"topic","url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"kind":"symbol","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","title":"WWDC Notes"},"WWDCNotes.png":{"alt":null,"identifier":"WWDCNotes.png","type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}]},"WWDC19.jpeg":{"alt":null,"identifier":"WWDC19.jpeg","type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19.jpeg"}]},"WWDC19-Icon.png":{"alt":null,"identifier":"WWDC19-Icon.png","type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC19-Icon.png"}]}}}