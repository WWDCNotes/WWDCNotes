{"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22"],["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML"]]},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc22-10020-compose-advanced-models-with-create-ml-components"],"traits":[{"interfaceLanguage":"swift"}]}],"schemaVersion":{"major":0,"patch":0,"minor":3},"kind":"article","primaryContentSections":[{"content":[{"anchor":"overview","text":"Overview","type":"heading","level":2},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference"}]},{"anchor":"Related-Sessions","text":"Related Sessions","type":"heading","level":2},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110332-Whats-new-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10036-Discover-builtin-sound-classification-in-SoundAnalysis","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10058-Meet-AsyncSequence","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10256-Meet-the-Swift-Algorithms-and-Collections-packages","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML"]}],"kind":"content"}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10020-Compose-advanced-models-with-Create-ML-Components"},"sections":[],"abstract":[{"type":"text","text":"Take your custom machine learning models to the next level with Create ML Components. We‚Äôll show you how to work with temporal data like video or audio and compose models that can count repetitive human actions or provide advanced sound classification. We‚Äôll also share best practices on using incremental fitting to speed up model training with new data."}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc22\/10020","isActive":true,"overridingTitle":"Watch Video (13 min)"}},"metadata":{"modules":[{"name":"WWDC Notes"}],"title":"Compose advanced models with Create ML Components","role":"sampleCode","roleHeading":"WWDC22"},"references":{"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Learn More‚Ä¶","type":"link","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}]},"https://developer.apple.com/wwdc22/10020":{"checksum":null,"type":"download","url":"https:\/\/developer.apple.com\/wwdc22\/10020","identifier":"https:\/\/developer.apple.com\/wwdc22\/10020"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23","images":[{"identifier":"WWDC23-Icon.png","type":"icon"},{"identifier":"WWDC23.jpeg","type":"card"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}],"title":"WWDC23","kind":"article","role":"collectionGroup"},"WWDC22.jpeg":{"variants":[{"url":"\/images\/WWDC22.jpeg","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC22.jpeg"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10256-Meet-the-Swift-Algorithms-and-Collections-packages":{"title":"Meet the Swift Algorithms and Collections packages","type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc21-10256-meet-the-swift-algorithms-and-collections-packages","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10256-Meet-the-Swift-Algorithms-and-Collections-packages","abstract":[{"type":"text","text":"Discover two of the latest additions to the list of open-source Swift packages from Apple: Swift Algorithms and Swift Collections. Not only can you use these packages immediately, they also incubate new algorithms and data structures for eventual inclusion in the Swift Standard Library. We‚Äôll show you how you can integrate these packages into your projects and select the right algorithms and data structures to make your code clearer and faster."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22","title":"WWDC22","url":"\/documentation\/wwdcnotes\/wwdc22","images":[{"type":"icon","identifier":"WWDC22-Icon.png"},{"type":"card","identifier":"WWDC22.jpeg"}],"role":"collectionGroup","type":"topic","abstract":[{"type":"text","text":"Xcode 14, Swift 5.7, iOS 16, macOS 13 (Ventura), tvOS 16, watchOS 9."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"code":"WeatherKit","type":"codeVoice"},{"type":"text","text":", "},{"code":"ScreenCaptureKit","type":"codeVoice"},{"type":"text","text":", "},{"code":"Swift Regex","type":"codeVoice"},{"type":"text","text":", and more."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10019-Get-to-know-Create-ML-Components":{"url":"\/documentation\/wwdcnotes\/wwdc22-10019-get-to-know-create-ml-components","kind":"article","title":"Get to know Create ML Components","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","type":"topic","abstract":[{"type":"text","text":"Create ML makes it easy to build custom machine learning models for image classification, object detection, sound classification, hand pose classification, action classification, tabular data regression, and more. And with the Create ML Components framework, you can further customize underlying tasks and improve your model. We‚Äôll explore the feature extractors, transformers, and estimators that make up these tasks, and show you how you can combine them with other components and pre-processing steps to build custom tasks for concepts like image regression."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","title":"Training Sound Classification Models in Create ML","url":"\/documentation\/wwdcnotes\/wwdc19-425-training-sound-classification-models-in-create-ml","role":"sampleCode","abstract":[{"text":"Learn how to quickly and easily create Core ML models capable of classifying the sounds heard in audio files and live audio streams. In addition to providing you the ability to train and evaluate these models, the Create ML app allows you to test the model performance in real-time using the microphone on your Mac. Leverage these on-device models in your app using the new Sound Analysis framework.","type":"text"}]},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDCNotes.png"},"WWDC22-Icon.png":{"variants":[{"url":"\/images\/WWDC22-Icon.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC22-Icon.png"},"WWDC23.jpeg":{"variants":[{"url":"\/images\/WWDC23.jpeg","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC23.jpeg"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML":{"abstract":[{"type":"text","text":"Find out how Create ML can help you do even more with machine learning models. Learn about the latest updates to image understanding and text-based tasks with multilingual BERT embeddings. Discover how easy it is to train models that can understand the content of images using multi-label classification. We‚Äôll also share information about interactive model evaluation and the latest APIs for custom training data augmentations."}],"type":"topic","title":"Discover machine learning enhancements in Create ML","url":"\/documentation\/wwdcnotes\/wwdc23-10044-discover-machine-learning-enhancements-in-create-ml","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10058-Meet-AsyncSequence":{"title":"Meet AsyncSequence","type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc21-10058-meet-asyncsequence","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10058-Meet-AsyncSequence","abstract":[{"type":"text","text":"Iterating over a sequence of values over time is now as easy as writing a ‚Äúfor‚Äù loop. Find out how the new AsyncSequence protocol enables a natural, simple syntax for iterating over anything from notifications to bytes being streamed from a server. We‚Äôll also show you how to adapt existing code to provide asynchronous sequences of your own."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-110332-Whats-new-in-Create-ML":{"abstract":[{"type":"text","text":"Discover the latest updates to Create ML. We‚Äôll share improvements to Create ML‚Äôs evaluation tools that can help you understand how your custom models will perform on real-world data. Learn how you can check model performance on each type of image in your test data and identify problems within individual images to help you troubleshoot mistaken classifications, poorly labeled data, and other errors. We‚Äôll also show you how to test your model with iPhone and iPad in live preview using Continuity Camera, and share how you can take Action Classification even further with the new Repetition Counting capabilities of the Create ML Components framework."}],"type":"topic","title":"What‚Äôs new in Create ML","url":"\/documentation\/wwdcnotes\/wwdc22-110332-whats-new-in-create-ml","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110332-Whats-new-in-Create-ML","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10036-Discover-builtin-sound-classification-in-SoundAnalysis":{"title":"Discover built-in sound classification in SoundAnalysis","type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc21-10036-discover-builtin-sound-classification-in-soundanalysis","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10036-Discover-builtin-sound-classification-in-SoundAnalysis","abstract":[{"type":"text","text":"Explore how you can use the Sound Analysis framework in your app to detect and classify discrete sounds from any audio source ‚Äî including live sounds from a microphone or from a video or audio file ‚Äî and identify precisely in a moment where that sound occurs. Learn how the built-in sound classifier makes it easy for you to identify over 300 different types of sounds without the need for a custom trained model. This includes a variety of noises, ranging from human sounds, musical instruments, animals, and various items."}]},"WWDC23-Icon.png":{"variants":[{"url":"\/images\/WWDC23-Icon.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC23-Icon.png"},"doc://WWDCNotes/documentation/WWDCNotes":{"role":"collection","type":"topic","url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"kind":"symbol","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","title":"WWDC Notes"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML":{"title":"Build an Action Classifier with Create ML","type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc20-10043-build-an-action-classifier-with-create-ml","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","abstract":[{"type":"text","text":"Discover how to build Action Classification models in Create ML. With a custom action classifier, your app can recognize and understand body movements in real-time from videos or through a camera. We‚Äôll show you how to use samples to easily train a Core ML model to identify human actions like jumping jacks, squats, and dance moves. Learn how this is powered by the Body Pose estimation features of the Vision Framework. Get inspired to create apps that can provide coaching for fitness routines, deliver feedback on athletic form, and more."}]}}}