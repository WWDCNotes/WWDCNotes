{"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc19-234-text-recognition-in-vision-framework"],"traits":[{"interfaceLanguage":"swift"}]}],"schemaVersion":{"patch":0,"minor":3,"major":0},"sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-234-Text-Recognition-in-Vision-Framework"},"abstract":[{"text":"Document Camera and Text Recognition features in Vision Framework enable you to extract text data from images. Learn how to leverage this built-in machine learning technology in your app. Gain a deeper understanding of the differences between fast versus accurate processing as well as character-based versus language-based recognition.","type":"text"}],"sampleCodeDownload":{"action":{"identifier":"https:\/\/developer.apple.com\/wwdc19\/234","type":"reference","isActive":true,"overridingTitle":"Watch Video (38 min)"},"kind":"sampleDownload"},"metadata":{"title":"Text Recognition in Vision Framework","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC19","role":"sampleCode"},"primaryContentSections":[{"content":[{"level":2,"anchor":"overview","type":"heading","text":"Overview"},{"inlineContent":[{"type":"text","text":"New "},{"code":"VNRecognizeTestRequest","type":"codeVoice"},{"type":"text","text":": this object turns an image into text."}],"type":"paragraph"},{"level":2,"anchor":"How-it-works","type":"heading","text":"How it works"},{"inlineContent":[{"text":"Two possibilities\/ways:","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Fast","type":"text"}]},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"detects characters","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"recognizes characters (with a small model)","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"(Optional) Runs detection into a NLP process (Neuro-Linguistic Programming)"}],"type":"paragraph"}]}],"type":"unorderedList"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Accurate","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Uses a Neural Network to find the text (in terms of sentences placement in the image)","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Uses another Neural Network to read the text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"(Optional) Runs detection into a NLP process (Neuro-Linguistic Programming)"}]}]}]}]}],"type":"unorderedList"},{"inlineContent":[{"text":"Fast is…much faster (on a small note picture, ~0.25s vs 2s), however is less accurate (the more standard the text is, the better results).","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Accurate is slower but much accurate, especially when “special” fonts are used."}],"type":"paragraph"},{"level":2,"anchor":"Other-differences","type":"heading","text":"Other differences"},{"inlineContent":[{"type":"image","identifier":"WWDC19-234-table"}],"type":"paragraph"},{"inlineContent":[{"text":"Regarding the ","type":"text"},{"inlineContent":[{"text":"NLP Processing phase","type":"text"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Corrects typical recognition errors (zeroes to letter “o” for example)","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"It can get in the way for codes (c001 can become cool)","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Increases the processing time and memory budget"}]}]}],"type":"unorderedList"},{"inlineContent":[{"text":"Once recognized the text, we can also query for bounding box in the images for recognized text (for example, to highlight one recognized word).","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"When using the camera, we can also have an on-screen “region of interest” (ROI), which helps with the performance by, instead of scanning the whole camera frame, just that region. We set what this region is.","type":"text"}],"type":"paragraph"},{"level":2,"anchor":"Best-practices","type":"heading","text":"Best practices"},{"inlineContent":[{"type":"text","text":"If you know the language of the document to be scanned, set it."},{"type":"text","text":" "},{"type":"text","text":"If you know the format\/lexicon of what you want to scan, pass it to the Vision recognizer (like the pattern for a phone number: "},{"code":"+1 (0) xxx - xxx xxxx","type":"codeVoice"},{"type":"text","text":") ."},{"type":"text","text":" "},{"type":"text","text":"This also helps the NLP process as the result will match the passed lexicon."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"If the text is big, you can set the minimum text height, which is a number from 0 to 1, where 0.3 for example is 30% of the camera image. This helps with performance (faster scanning) and lower memory usage."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"If the textRecognition is not the core experience of your product, you can set it to run only on the CPU to give more gpu\/neural network for other core experiences (like ARKit)"}],"type":"paragraph"},{"inlineContent":[{"text":"We also have available a progress handler that we can use to tell the user what it the state of the scan:","type":"text"},{"text":" ","type":"text"},{"text":"textRecognitionRequest.progressHandler = …","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"We can cancel a recognition if no longer needed via ","type":"text"},{"type":"codeVoice","code":"textRecognitionRequest.cancel()"}],"type":"paragraph"},{"level":2,"anchor":"Processing-Results","type":"heading","text":"Processing Results"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Compare spatial info (position, scale, rotation)","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Use ","type":"text"},{"code":"NSDataDetector","type":"codeVoice"},{"text":"s for addresses, URLs","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"level":2,"anchor":"Written-By","type":"heading","text":"Written By"},{"columns":[{"content":[{"type":"paragraph","inlineContent":[{"identifier":"zntfdr","type":"image"}]}],"size":1},{"content":[{"text":"Federico Zanetello","anchor":"Federico-Zanetello","type":"heading","level":3},{"type":"paragraph","inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"overridingTitle":"Contributed Notes","isActive":true,"type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/github.com\/zntfdr","isActive":true,"type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/zntfdr.dev","isActive":true,"type":"reference"}]}],"size":4}],"type":"row","numberOfColumns":5},{"type":"thematicBreak"},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true,"type":"reference"}],"type":"paragraph"},{"level":2,"anchor":"Related-Sessions","type":"heading","text":"Related Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10041-Extract-document-data-using-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10673-Explore-Computer-Vision-APIs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-222-Understanding-Images-in-Vision-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-232-Advances-in-Natural-Language-Framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework"],"type":"links","style":"list"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2025 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}],"kind":"content"}],"kind":"article","hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-222-Understanding-Images-in-Vision-Framework":{"title":"Understanding Images in Vision Framework","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-222-Understanding-Images-in-Vision-Framework","role":"sampleCode","abstract":[{"text":"Learn all about the many advances in the Vision Framework including effortless image classification, image saliency, determining image similarity, and improvements in facial feature detection, and face capture quality scoring. This packed session will show you how easy it is to bring powerful computer vision techniques to your apps.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc19-222-understanding-images-in-vision-framework","type":"topic"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"identifier":"WWDCNotes.png","type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10673-Explore-Computer-Vision-APIs":{"title":"Explore Computer Vision APIs","kind":"article","type":"topic","role":"sampleCode","abstract":[{"type":"text","text":"Learn how to bring Computer Vision intelligence to your app when you combine the power of Core Image, Vision, and Core ML. Go beyond machine learning alone and gain a deeper understanding of images and video. Discover new APIs in Core Image and Vision to bring Computer Vision to your application like new thresholding filters as well as Contour Detection and Optical Flow. And consider ways to use Core Image for preprocessing and visualization of these results."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10673-Explore-Computer-Vision-APIs","url":"\/documentation\/wwdcnotes\/wwdc20-10673-explore-computer-vision-apis"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-232-Advances-in-Natural-Language-Framework":{"url":"\/documentation\/wwdcnotes\/wwdc19-232-advances-in-natural-language-framework","type":"topic","abstract":[{"text":"Natural Language is a framework designed to provide high-performance, on-device APIs for natural language processing tasks across all Apple platforms. Learn about the addition of Sentiment Analysis and Text Catalog support in the framework. Gain a deeper understanding of transfer learning for text-based models and the new support for Word Embeddings which can power great search experiences in your app.","type":"text"}],"title":"Advances in Natural Language Framework","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-232-Advances-in-Natural-Language-Framework","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"url":"\/documentation\/wwdcnotes\/wwdc19","abstract":[{"type":"text","text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"Combine"},{"type":"text","text":", "},{"type":"codeVoice","code":"Core Haptics"},{"type":"text","text":", "},{"type":"codeVoice","code":"Create ML"},{"type":"text","text":", and more."}],"kind":"article","title":"WWDC19","images":[{"identifier":"WWDC19-Icon.png","type":"icon"},{"identifier":"WWDC19.jpeg","type":"card"}],"type":"topic","role":"collectionGroup","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"},"zntfdr":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/zntfdr.jpeg","traits":["1x","light"]}],"identifier":"zntfdr","alt":"Profile image of Federico Zanetello"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-704-Core-ML-3-Framework":{"url":"\/documentation\/wwdcnotes\/wwdc19-704-core-ml-3-framework","title":"Core ML 3 Framework","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework","role":"sampleCode","abstract":[{"text":"Core ML 3 now enables support for advanced model types that were never before available in on-device machine learning. Learn how model personalization brings amazing personalization opportunities to your app. Gain a deeper understanding of strategies for linking models and improvements to Core ML tools used for conversion of existing models.","type":"text"}],"kind":"article","type":"topic"},"WWDC19.jpeg":{"identifier":"WWDC19.jpeg","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC19.jpeg","traits":["1x","light"]}],"alt":null},"https://zntfdr.dev":{"titleInlineContent":[{"type":"text","text":"Blog"}],"url":"https:\/\/zntfdr.dev","identifier":"https:\/\/zntfdr.dev","type":"link","title":"Blog"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"type":"link","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10041-Extract-document-data-using-Vision":{"role":"sampleCode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10041-Extract-document-data-using-Vision","type":"topic","abstract":[{"type":"text","text":"Discover how Vision can provide expert image recognition and analysis in your app to extract information from documents, recognize text in multiple languages, and identify barcodes. We’ll explore the latest updates to Text Recognition and Barcode Detection, show you how to bring all these tools together with Core ML, and help your app make greater sense of the world through images or the live camera."}],"title":"Extract document data using Vision","url":"\/documentation\/wwdcnotes\/wwdc21-10041-extract-document-data-using-vision"},"https://github.com/zntfdr":{"identifier":"https:\/\/github.com\/zntfdr","titleInlineContent":[{"text":"GitHub","type":"text"}],"type":"link","url":"https:\/\/github.com\/zntfdr","title":"GitHub"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"url":"\/documentation\/wwdcnotes\/zntfdr","abstract":[{"type":"text","text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more."}],"kind":"article","title":"Federico Zanetello (332 notes)","images":[{"type":"card","identifier":"zntfdr.jpeg"},{"type":"icon","identifier":"zntfdr.jpeg"}],"type":"topic","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr"},"https://developer.apple.com/wwdc19/234":{"url":"https:\/\/developer.apple.com\/wwdc19\/234","identifier":"https:\/\/developer.apple.com\/wwdc19\/234","type":"download","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-209-Whats-New-in-Machine-Learning":{"role":"sampleCode","title":"What’s New in Machine Learning","url":"\/documentation\/wwdcnotes\/wwdc19-209-whats-new-in-machine-learning","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","abstract":[{"type":"text","text":"Core ML 3 has been greatly expanded to enable even more amazing, on-device machine learning capabilities in your app. Learn about the new Create ML app which makes it easy to build Core ML models for many tasks. Get an overview of model personalization; exciting updates in Vision, Natural Language, Sound, and Speech; and added support for cutting-edge model types."}],"kind":"article","type":"topic"},"WWDC19-234-table":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-234-table.png","traits":["1x","light"]}],"identifier":"WWDC19-234-table","alt":null},"zntfdr.jpeg":{"variants":[{"url":"\/images\/WWDCNotes\/zntfdr.jpeg","traits":["1x","light"]}],"identifier":"zntfdr.jpeg","type":"image","alt":null},"WWDC19-Icon.png":{"identifier":"WWDC19-Icon.png","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-Icon.png","traits":["1x","light"]}],"alt":null},"doc://WWDCNotes/documentation/WWDCNotes":{"role":"collection","title":"WWDC Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"url":"\/documentation\/wwdcnotes","kind":"symbol","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"type":"topic"}}}