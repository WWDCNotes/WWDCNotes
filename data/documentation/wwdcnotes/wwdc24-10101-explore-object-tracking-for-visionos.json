{"schemaVersion":{"patch":0,"minor":3,"major":0},"primaryContentSections":[{"content":[{"anchor":"overview","level":2,"text":"Overview","type":"heading"},{"inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true}],"type":"paragraph"},{"anchor":"Related-Sessions","level":2,"text":"Related Sessions","type":"heading"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10191-Meet-Object-Capture-for-iOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10100-Create-enhanced-spatial-computing-experiences-with-ARKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10183-Whats-new-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10102-Compose-interactive-3D-content-in-Reality-Composer-Pro"],"type":"links","style":"list"},{"inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright ¬© 2012 ‚Äì 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}],"kind":"content"}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10101-Explore-object-tracking-for-visionOS"},"sampleCodeDownload":{"action":{"identifier":"https:\/\/developer.apple.com\/wwdc24\/10101","isActive":true,"type":"reference","overridingTitle":"Watch Video (17 min)"},"kind":"sampleDownload"},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc24-10101-explore-object-tracking-for-visionos"]}],"sections":[],"kind":"article","metadata":{"title":"Explore object tracking for visionOS","roleHeading":"WWDC24","role":"sampleCode","modules":[{"name":"WWDC Notes"}]},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24"]]},"abstract":[{"type":"text","text":"Find out how you can use object tracking to turn real-world objects into virtual anchors in your visionOS app. Learn how you can build spatial experiences with object tracking from start to finish. Find out how to create a reference object using machine learning in Create ML and attach content relative to your target object in Reality Composer Pro, RealityKit or ARKit APIs."}],"references":{"WWDCNotes.png":{"identifier":"WWDCNotes.png","alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24":{"title":"WWDC24","url":"\/documentation\/wwdcnotes\/wwdc24","type":"topic","images":[{"type":"icon","identifier":"WWDC24-Icon.png"},{"type":"card","identifier":"WWDC24.jpeg"}],"abstract":[{"text":"Xcode 16, Swift 6, iOS 18, macOS 15 (Sequoia), tvOS 18, visionOS 2, watchOS 11.","type":"text"},{"type":"text","text":" "},{"type":"text","text":"New APIs: Swift Testing, "},{"code":"FinanceKit","type":"codeVoice"},{"type":"text","text":", "},{"code":"TabletopKit","type":"codeVoice"},{"type":"text","text":", and more."}],"role":"collectionGroup","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10191-Meet-Object-Capture-for-iOS":{"type":"topic","title":"Meet Object Capture for iOS","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10191-Meet-Object-Capture-for-iOS","url":"\/documentation\/wwdcnotes\/wwdc23-10191-meet-object-capture-for-ios","abstract":[{"type":"text","text":"Discover how you can offer an end-to-end Object Capture experience directly in your iOS apps to help people turn their objects into ready-to-use 3D models. Learn how you can create a fully automated Object Capture scan flow with our sample app and how you can assist people in automatically capturing the best content for their model. We‚Äôll also discuss LiDAR data and provide best practices for scanning objects."}],"role":"sampleCode"},"WWDC24-Icon.png":{"identifier":"WWDC24-Icon.png","alt":null,"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC24-Icon.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10183-Whats-new-in-Create-ML":{"type":"topic","kind":"article","abstract":[{"type":"text","text":"Explore updates to Create ML, including interactive data source previews and a new template for building object tracking models for visionOS apps. We‚Äôll also cover important framework improvements, including new time-series forecasting and classification APIs."}],"role":"sampleCode","title":"What‚Äôs new in Create ML","url":"\/documentation\/wwdcnotes\/wwdc24-10183-whats-new-in-create-ml","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10183-Whats-new-in-Create-ML"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"text":"Learn More‚Ä¶","type":"text"}],"title":"Learn More‚Ä¶","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link"},"https://developer.apple.com/wwdc24/10101":{"checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc24\/10101","url":"https:\/\/developer.apple.com\/wwdc24\/10101","type":"download"},"WWDC24.jpeg":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC24.jpeg"}],"identifier":"WWDC24.jpeg","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10100-Create-enhanced-spatial-computing-experiences-with-ARKit":{"url":"\/documentation\/wwdcnotes\/wwdc24-10100-create-enhanced-spatial-computing-experiences-with-arkit","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10100-Create-enhanced-spatial-computing-experiences-with-ARKit","kind":"article","role":"sampleCode","abstract":[{"type":"text","text":"Learn how to create captivating immersive experiences with ARKit‚Äôs latest features. Explore ways to use room tracking and object tracking to further engage with your surroundings. We‚Äôll also share how your app can react to changes in your environment‚Äôs lighting on this platform. Discover improvements in hand tracking and plane detection which can make your spatial experiences more intuitive."}],"title":"Create enhanced spatial computing experiences with ARKit"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit":{"type":"topic","kind":"article","title":"Build a spatial drawing app with RealityKit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit","abstract":[{"text":"Harness the power of RealityKit through the process of building a spatial drawing app. As you create an eye-catching spatial experience that integrates RealityKit with ARKit and SwiftUI, you‚Äôll explore how resources work in RealityKit and how to use features like low-level mesh and texture APIs to achieve fast updates of the users‚Äô brush strokes.","type":"text"}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc24-10104-build-a-spatial-drawing-app-with-realitykit"},"doc://WWDCNotes/documentation/WWDCNotes":{"title":"WWDC Notes","url":"\/documentation\/wwdcnotes","type":"topic","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","kind":"symbol"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10102-Compose-interactive-3D-content-in-Reality-Composer-Pro":{"type":"topic","kind":"article","title":"Compose interactive 3D content in Reality Composer Pro","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10102-Compose-interactive-3D-content-in-Reality-Composer-Pro","abstract":[{"text":"Discover how the Timeline view in Reality Composer Pro can bring your 3D content to life. Learn how to create an animated story in which characters and objects interact with each other and the world around them using inverse kinematics, blend shapes, and skeletal poses. We‚Äôll also show you how to use built-in and custom actions, sequence your actions, apply triggers, and implement natural movements.","type":"text"}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc24-10102-compose-interactive-3d-content-in-reality-composer-pro"}}}