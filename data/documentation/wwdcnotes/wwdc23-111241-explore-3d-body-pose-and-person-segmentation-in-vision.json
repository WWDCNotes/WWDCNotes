{"sections":[],"sampleCodeDownload":{"action":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/111241","overridingTitle":"Watch Video (14 min)","type":"reference","isActive":true},"kind":"sampleDownload"},"abstract":[{"type":"text","text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We’ll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image."}],"primaryContentSections":[{"content":[{"type":"heading","anchor":"overview","level":2,"text":"Overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Speaker: Andrew Rauh, Software Engineer"}]},{"type":"heading","anchor":"Human-Body-Pose-in-Vision","level":1,"text":"Human Body Pose in Vision"},{"type":"heading","anchor":"Previous-Version","level":2,"text":"Previous Version"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"In the initial version of Vision framework, body pose detection uses 2D coordinate system to locate and track the positions of body parts."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"It specifically uses x and y coordinates for detecting body part in the image or video."}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"See more in the session “Detect Body and Handpose with Vision” from WWDC20."}]},{"type":"heading","anchor":"Human-Body-Pose-in-3D","level":2,"text":"Human Body Pose in 3D"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"New Vision framework expands and supports detecting human body in 3D with VNDetectHumanBodyPose3DRequest.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"This 3D request can detect 17 joints. These joints can be accessed by joint names or can be used as joint group name."}]}]},{"content":[{"inlineContent":[{"text":"The position of 3D joint is captured in meters from the relative to the image captured. This relative position can be defined as root joint.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"For example, when there are multiple persons present in real world, initial version will detect the most prominent person in the frame as a root joint.","type":"text"},{"text":" ","type":"text"},{"type":"image","identifier":"WWDC23-111241-1-human_detect"}]}]}]},{"type":"heading","anchor":"VNHumanBodyPose3DObservationJointName","level":3,"text":"VNHumanBodyPose3DObservation.JointName"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"The 3D human body pose is being demonstrated with skeleton structure with different joint groups."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Namely head, torso, left arm, right arm, left leg and right leg group names. (VNHumanBodyPose3DObservation.JointsGroupName)","type":"text"}],"type":"paragraph"}]}]},{"type":"heading","anchor":"head-group","level":4,"text":".head group"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".centerHead - A joint name that represents the center of the head."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".topHead - A joint name that represents the top of the head."},{"type":"text","text":" "},{"identifier":"WWDC23-111241-2-head-group","type":"image"}]}]}]},{"type":"heading","anchor":"torso-group","level":4,"text":".torso group"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".leftShoulder - A joint name that represents the left shoulder."}]}]},{"content":[{"inlineContent":[{"type":"text","text":".centertShoulder - A joint name that represents the point between the shoulders."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":".rightShoulder - A joint name that represents the right shoulder.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":".spine - A joint name that represents the spine."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".root - A joint name that represents the point between the left hip and right hip."}]}]},{"content":[{"inlineContent":[{"type":"text","text":".leftHip - A joint name that represents the left hip."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".rightHip - A joint name that represents the right hip."},{"type":"text","text":" "},{"type":"image","identifier":"WWDC23-111241-3-torso-group"}]}]}]},{"type":"heading","anchor":"leftArm-group","level":4,"text":".leftArm group"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".leftWrist - A joint name that represents the left wrist."}]}]},{"content":[{"inlineContent":[{"type":"text","text":".leftShoulder - A joint name that represents the left shoulder."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".leftElbow - A joint name that represents the left elbow."},{"type":"text","text":" "},{"type":"image","identifier":"WWDC23-111241-4-left-arm-group"}]}]}]},{"type":"heading","anchor":"rightArm-group","level":4,"text":".rightArm group"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".rightWrist - A joint name that represents the right wrist."}]}]},{"content":[{"inlineContent":[{"text":".rightShoulder - A joint name that represents the right shoulder.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":".rightElbow - A joint name that represents the right elbow."},{"type":"text","text":" "},{"type":"image","identifier":"WWDC23-111241-5-right-arm-group"}],"type":"paragraph"}]}]},{"type":"heading","anchor":"leftleg-group","level":4,"text":".leftleg group"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":".leftHip - A joint name that represents the left hip."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":".leftKnee - A joint name that represents the left knee."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":".leftAnkle - A joint name that represents the left ankle.","type":"text"},{"text":" ","type":"text"},{"identifier":"WWDC23-111241-6-left-leg-group","type":"image"}]}]}]},{"type":"heading","anchor":"rightleg-group","level":4,"text":".rightleg group"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":".rightHip - A joint name that represents the right hip.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":".rightKnee - A joint name that represents the right knee.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":".rightAnkle - A joint name that represents the right ankle."},{"type":"text","text":" "},{"type":"image","identifier":"WWDC23-111241-7-right-leg-group"}],"type":"paragraph"}]}]},{"type":"heading","anchor":"Snippet-converting-a-2D-image-to-3D-world","level":3,"text":"Snippet converting a 2D image to 3D world"},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-111241-8-snippet","type":"image"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"You need to initialize an image asset and call the request function.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"If the request function is successful, a VNHumanBodyPose3DObservation will be returned without error."}],"type":"paragraph"}]}]},{"type":"heading","anchor":"There-are-two-ways-we-can-retrieve-position-VNHumanBodyPose3DObservation","level":3,"text":"There are two ways we can retrieve position VNHumanBodyPose3DObservation:"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"For accessing a specific joint’s position.","type":"text"}],"type":"paragraph"}]}]},{"type":"codeListing","code":["let recognizedPoint = try observation.recognizedPoint(.centerHead)"],"syntax":"swift"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"For accessing a collection of joints with a specified group name.","type":"text"}],"type":"paragraph"}]}]},{"type":"codeListing","code":["let recognizedPoints = try observation.recognizedPoints(.torso)"],"syntax":"swift"},{"type":"heading","anchor":"Other-advantages","level":3,"text":"Other advantages"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"bodyHeight and heightEstimation"},{"type":"text","text":" "},{"type":"image","identifier":"WWDC23-111241-9-advantage1"}]}]},{"content":[{"inlineContent":[{"text":"Understanding where the camera was relative to the person when the frame was captured.","type":"text"},{"text":" ","type":"text"},{"type":"image","identifier":"WWDC23-111241-10-advantage2"}],"type":"paragraph"}]}]},{"type":"heading","anchor":"3D-Positions-in-Vision","level":2,"text":"3D Positions in Vision"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"VNPoint3D is the base class that defines 4x4 matrix for storing 3D position. This notation is consistent with ARKit and available for all rotations and translations."}]}]},{"content":[{"inlineContent":[{"text":"VNRecognizedPoint3D is used to store corresponding information like joint name and inherits position and adds an identifier.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"VNHumanBodyRecognizedPoint3D is used to get more specifics around how to work with properties of the point like local position and the parent joint."},{"type":"text","text":" "},{"identifier":"WWDC23-111241-11-3d","type":"image"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"point.position - It retrieves skeleton’s root joint at the center of hip. For example: .leftWrist"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"point.localPosition - It is the position relative to a parent joint. It works only one area of the body. Eg. determining the angle between child and parent joint."}]}]}]},{"type":"codeListing","code":["    public func calculateLocalAngleToParent(joint: VNHumanBodyPose3DObservation.JointName) -> simd_float3 {","        var angleVector: simd_float3 = simd_float3()","        do {","            if let observation = self.humanObservation {","                let recognizedPoint = try observation.recognizedPoint(joint)","                let childPosition = recognizedPoint.localPosition","                let translationC  = childPosition.translationVector","                \/\/ The rotation for x, y, z.","                \/\/ Rotate 90 degrees from the default orientation of the node. Add yaw and pitch, and connect the child to the parent.","                let pitch = (Float.pi \/ 2)","                let yaw = acos(translationC.z \/ simd_length(translationC))","                let roll = atan2((translationC.y), (translationC.x))","                angleVector = simd_float3(pitch, yaw, roll)","            }","        } catch {","            print(\"Unable to return point: \\(error).\")","        }","        return angleVector","    }"],"syntax":"swift"},{"type":"heading","anchor":"Depth-in-Vision","level":1,"text":"Depth in Vision"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Vision framework is now accepting depth as input along with image or frame buffer"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"New API request accepting depth as parameter"}]}]}]},{"type":"codeListing","code":["let requestHandler = VNImageRequestHandler(cmSampleBuffer: frameBuffer, depthData: depthData, orientation: .up, options: [:])","let requestHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, depthData: depthData, orientation: .up, options: [:])"],"syntax":"swift"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"API will fetch depth from URL","type":"text"}],"type":"paragraph"}]}]},{"type":"codeListing","code":["let requestHandler = VNImageRequestHandler(url: imageURL)"],"syntax":"swift"},{"type":"heading","anchor":"Working-with-Depth","level":2,"text":"Working with Depth"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"AVDepthData is the container class which interfaces all Depth metadata (which is captured from camera sensors)."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Depth map has either Disparity (DisparityFloat16, DisparityFloat32) or Depth format (DepthFloat16, DepthFloat32).","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Depth map data can be interchangable and converted to other format using AVFoundation."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Depth metadata is used to construct 3D scene. Depth metadata also has camera calibration data (instrinsics, extrinsics, and lens distortion).","type":"text"},{"text":" ","type":"text"},{"text":"See more in the session “Discover advancements in iOS camera capture” from WWDC22.","type":"text"}]}]}]},{"type":"heading","anchor":"Sources-of-Depth","level":2,"text":"Sources of Depth"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Camera captured session or file.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Images captured by camera app like portrait images, which stores dispartiy data."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"LiDAR enables high accuracy measurment of scene."}],"type":"paragraph"}]}]},{"type":"heading","anchor":"Person-Instance-Mask","level":1,"text":"Person Instance Mask"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Vision API is now supporting when it interacts with more than one person in the image."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Person Segmentation technique is used to separate people from the surroundings."}],"type":"paragraph"}]}]},{"type":"heading","anchor":"VNGeneratePersonSegmentationRequest","level":2,"text":"VNGeneratePersonSegmentationRequest"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Uses single mask for all people in the frame.","type":"text"},{"text":" ","type":"text"},{"identifier":"WWDC23-111241-12-person_segmentation1","type":"image"},{"text":" ","type":"text"},{"identifier":"WWDC23-111241-13-person_segmentation2","type":"image"}],"type":"paragraph"}]}]},{"type":"heading","anchor":"VNGeneratePersonInstanceMaskRequest","level":2,"text":"VNGeneratePersonInstanceMaskRequest"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"API allows you to be more selective."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"You can select and lift the subjects other than people."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"It allows upto 4 people in foreground (individual mask)"}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"See more in the session “Lift subjects from images in your app” from WWDC23."}]},{"type":"heading","anchor":"Selecting-personal-instance-mask-which-you-want-from-an-image","level":3,"text":"Selecting personal instance mask which you want from an image"},{"type":"codeListing","code":["result.createMattedImage(","  ofInstances: result.allInstances, ","  from: requestHandler,","  croppedToInstancesExtent: false",")"],"syntax":"swift"},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-111241-14-instances","type":"image"}]},{"type":"heading","anchor":"Selecting-personal-instance-mask-with-many-people","level":3,"text":"Selecting personal instance mask with many people"},{"type":"paragraph","inlineContent":[{"type":"text","text":"When more than four people are present request may be missing people in background or merge people in close contact. You can use either any one technique."}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Use face detection API to filter four or more people."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Use person segmentation request and work with one mask for everyone."},{"type":"text","text":" "},{"type":"image","identifier":"WWDC23-111241-15-multiple"}]}]}]},{"type":"heading","anchor":"Wrap-up","level":1,"text":"Wrap-up"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Vision framework is now offering a powerful ways to understand people, environment supporting depth, 3D Human Body Pose, and person instance masks."}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Also See more in the session “Detect animal poses in Vision” from WWDC23."}]},{"type":"heading","anchor":"Written-By","level":2,"text":"Written By"},{"numberOfColumns":5,"type":"row","columns":[{"size":1,"content":[{"inlineContent":[{"type":"image","identifier":"rakeshneela"}],"type":"paragraph"}]},{"size":4,"content":[{"text":"Rakesh Neela","level":3,"type":"heading","anchor":"Rakesh-Neela"},{"inlineContent":[{"isActive":true,"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/rakeshneela","overridingTitle":"Contributed Notes","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/github.com\/rakeshneela","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/","type":"reference"}],"type":"paragraph"}]}]},{"type":"thematicBreak"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"reference","isActive":true,"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"}]},{"type":"heading","anchor":"Related-Sessions","level":2,"text":"Related Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110429-Discover-advancements-in-iOS-camera-capture-Depth-focus-and-multitasking","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision"],"style":"list"},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2025 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}],"kind":"content"}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision"],"traits":[{"interfaceLanguage":"swift"}]}],"kind":"article","metadata":{"roleHeading":"WWDC23","role":"sampleCode","title":"Explore 3D body pose and person segmentation in Vision","modules":[{"name":"WWDC Notes"}]},"schemaVersion":{"patch":0,"major":0,"minor":3},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","interfaceLanguage":"swift"},"references":{"WWDC23-111241-3-torso-group":{"type":"image","alt":"torso group","identifier":"WWDC23-111241-3-torso-group","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-3-torso-group.jpg","traits":["1x","light"]}]},"WWDC23-111241-14-instances":{"identifier":"WWDC23-111241-14-instances","alt":"Selecting personal instance from an image","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-14-instances.jpg","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-110429-Discover-advancements-in-iOS-camera-capture-Depth-focus-and-multitasking":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110429-Discover-advancements-in-iOS-camera-capture-Depth-focus-and-multitasking","abstract":[{"type":"text","text":"Discover how you can take advantage of advanced camera capture features in your app. We’ll show you how to use the LiDAR scanner to create photo and video effects and perform accurate depth measurement. Learn how your app can use the camera for picture-in-picture or multitasking, control face-driven autofocus and autoexposure during camera capture, and more. We’ll also share strategies for using multiple video outputs so that you can optimize live preview while capturing high-quality video output."}],"title":"Discover advancements in iOS camera capture: Depth, focus, and multitasking","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc22-110429-discover-advancements-in-ios-camera-capture-depth-focus-and-multitasking"},"WWDC23-111241-8-snippet":{"identifier":"WWDC23-111241-8-snippet","alt":"Snippet converting 2D image to 3D","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-8-snippet.jpg","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/rakeshneela":{"url":"\/documentation\/wwdcnotes\/rakeshneela","type":"topic","abstract":[{"type":"text","text":"No Bio on GitHub"}],"role":"sampleCode","title":"Rakesh Neela (1 note)","kind":"article","images":[{"type":"card","identifier":"rakeshneela.jpeg"},{"type":"icon","identifier":"rakeshneela.jpeg"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/rakeshneela"},"WWDC23-111241-13-person_segmentation2":{"identifier":"WWDC23-111241-13-person_segmentation2","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-13-person_segmentation2.jpg","traits":["1x","light"]}],"type":"image","alt":"Single mask on multiple people"},"rakeshneela.jpeg":{"type":"image","alt":null,"identifier":"rakeshneela.jpeg","variants":[{"url":"\/images\/WWDCNotes\/rakeshneela.jpeg","traits":["1x","light"]}]},"rakeshneela":{"identifier":"rakeshneela","alt":"Profile image of Rakesh Neela","type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/rakeshneela.jpeg"}]},"WWDC23-111241-12-person_segmentation1":{"identifier":"WWDC23-111241-12-person_segmentation1","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-12-person_segmentation1.jpg","traits":["1x","light"]}],"type":"image","alt":"Single mask on 1 person"},"https://developer.apple.com/wwdc23/111241":{"type":"download","url":"https:\/\/developer.apple.com\/wwdc23\/111241","identifier":"https:\/\/developer.apple.com\/wwdc23\/111241","checksum":null},"WWDC23-111241-4-left-arm-group":{"identifier":"WWDC23-111241-4-left-arm-group","alt":"leftArm group","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-4-left-arm-group.jpg","traits":["1x","light"]}]},"WWDC23-111241-9-advantage1":{"type":"image","alt":"Advantage 1","identifier":"WWDC23-111241-9-advantage1","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC23-111241-9-advantage1.jpg"}]},"WWDC23-111241-15-multiple":{"identifier":"WWDC23-111241-15-multiple","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-15-multiple.jpg","traits":["1x","light"]}],"type":"image","alt":"Selecting personal instance from many people"},"https://":{"url":"https:\/\/","title":"Blog","identifier":"https:\/\/","titleInlineContent":[{"type":"text","text":"Blog"}],"type":"link"},"WWDC23-111241-11-3d":{"type":"image","alt":"3D positions in vision","identifier":"WWDC23-111241-11-3d","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC23-111241-11-3d.jpg"}]},"https://github.com/rakeshneela":{"identifier":"https:\/\/github.com\/rakeshneela","url":"https:\/\/github.com\/rakeshneela","type":"link","title":"GitHub","titleInlineContent":[{"text":"GitHub","type":"text"}]},"WWDC23-111241-5-right-arm-group":{"identifier":"WWDC23-111241-5-right-arm-group","alt":"rightArm group","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-5-right-arm-group.jpg","traits":["1x","light"]}]},"WWDCNotes.png":{"type":"image","alt":null,"identifier":"WWDCNotes.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDCNotes.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10176-Lift-subjects-from-images-in-your-app":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","abstract":[{"text":"Discover how you can easily pull the subject of an image from its background in your apps. Learn how to lift the primary subject or to access the subject at a given point with VisionKit. We’ll also share how you can lift subjects using Vision and combine that with lower-level frameworks like Core Image to create fun image effects and more complex compositing pipelines.","type":"text"}],"title":"Lift subjects from images in your app","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10176-lift-subjects-from-images-in-your-app"},"doc://WWDCNotes/documentation/WWDCNotes":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","role":"collection","url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC Notes","type":"topic","kind":"symbol"},"WWDC23-111241-6-left-leg-group":{"type":"image","alt":"leftleg group","identifier":"WWDC23-111241-6-left-leg-group","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC23-111241-6-left-leg-group.jpg"}]},"WWDC23-111241-10-advantage2":{"identifier":"WWDC23-111241-10-advantage2","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-10-advantage2.jpg","traits":["1x","light"]}],"type":"image","alt":"Advantage 2"},"WWDC23-Icon.png":{"identifier":"WWDC23-Icon.png","alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-Icon.png","traits":["1x","light"]}]},"WWDC23-111241-1-human_detect":{"type":"image","alt":"Human Body Pose in 3D ","identifier":"WWDC23-111241-1-human_detect","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC23-111241-1-human_detect.jpg"}]},"WWDC23-111241-2-head-group":{"identifier":"WWDC23-111241-2-head-group","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-2-head-group.jpg","traits":["1x","light"]}],"type":"image","alt":"head group"},"WWDC23-111241-7-right-leg-group":{"identifier":"WWDC23-111241-7-right-leg-group","alt":"rightleg group","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23-111241-7-right-leg-group.jpg","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"SwiftData"},{"type":"text","text":", "},{"type":"codeVoice","code":"Observation"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit"},{"type":"text","text":" views, and more."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","url":"\/documentation\/wwdcnotes\/wwdc23","type":"topic","images":[{"type":"icon","identifier":"WWDC23-Icon.png"},{"type":"card","identifier":"WWDC23.jpeg"}],"role":"collectionGroup","title":"WWDC23","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision","abstract":[{"type":"text","text":"Explore how the Vision framework can help your app detect body and hand poses in photos and video. With pose detection, your app can analyze the poses, movements, and gestures of people to offer new video editing possibilities, or to perform action classification when paired with an action classifier built in Create ML. And we’ll show you how you can bring gesture recognition into your app through hand pose, delivering a whole new form of interaction."}],"title":"Detect Body and Hand Pose with Vision","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc20-10653-detect-body-and-hand-pose-with-vision"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10045-Detect-animal-poses-in-Vision":{"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10045-detect-animal-poses-in-vision","type":"topic","abstract":[{"type":"text","text":"Go beyond detecting cats and dogs in images. We’ll show you how to use Vision to detect the individual joints and poses of these animals as well — all in real time — and share how you can enable exciting features like animal tracking for a camera app, creative embellishment on an animal photo, and more. We’ll also explore other important enhancements to Vision and share best practices."}],"kind":"article","title":"Detect animal poses in Vision","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision"},"WWDC23.jpeg":{"identifier":"WWDC23.jpeg","alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC23.jpeg","traits":["1x","light"]}]}}}