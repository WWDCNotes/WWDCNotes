{"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc24-10161-deploy-machine-learning-and-ai-models-ondevice-with-core-ml"]}],"primaryContentSections":[{"content":[{"level":2,"text":"Key takeaways","type":"heading","anchor":"Key-takeaways"},{"inlineContent":[{"text":"ðŸ¦¾ Core ML enhances the deployment and execution of ML and AI models.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"ðŸ§® Integration with MLTensor for common math and transformation operations.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"ðŸ“ˆ Improve inference efficiency by leveraging stateful models.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"ðŸš€ Deploy models with multiple functionalities."}],"type":"paragraph"},{"inlineContent":[{"text":"ðŸ”§ Utilize new profiling and debugging information to optimize your models","type":"text"}],"type":"paragraph"},{"level":2,"text":"Presenter","type":"heading","anchor":"Presenter"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Joshua Newnham, Core ML Engineer","type":"text"}]}]}],"type":"unorderedList"},{"level":1,"text":"Integeration","type":"heading","anchor":"Integeration"},{"inlineContent":[{"type":"text","text":"The machine learning workflow consists of three main phases:"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"inlineContent":[{"type":"text","text":"Model Training"}],"type":"strong"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"inlineContent":[{"text":"Preparation","type":"text"}],"type":"strong"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"Integration","type":"text"}],"type":"strong"}]}]}],"type":"unorderedList"},{"inlineContent":[{"inlineContent":[{"text":"Model Integration","type":"text"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Begins with an ML package created during the preparation phase."}]}]},{"content":[{"inlineContent":[{"code":"Core ML","type":"codeVoice"},{"text":" simplifies integration and usage in your app, offering a unified API for on-device inference across various model types.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Utilizes Apple siliconâ€™s CPU, GPU, and Neural Engine via MPS Graph and BNNS Graph for high performance, especially with Metal integration or real-time CPU inference."}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Performance Improvements"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"Core ML"},{"type":"text","text":" now delivers better performance with significant inference stack improvements."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"iOS 18 is faster than iOS 17 for many models, with no need for recompiling or code changes."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Speed improvements vary by model and hardware."}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Model Integration Steps","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Simple integration: pass input, read output.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Advanced use cases (e.g., generative AI): may require multiple models and iterative processes."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Additional computation outside the model may involve implementing operations from scratch or using low-level APIs, leading to complex code.","type":"text"}]}]}],"type":"unorderedList"},{"level":1,"text":"MLTensor","type":"heading","anchor":"MLTensor"},{"level":3,"text":"Introducing MLTensor","type":"heading","anchor":"Introducing-MLTensor"},{"inlineContent":[{"inlineContent":[{"text":"Overview","type":"text"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"MLTensor"},{"type":"text","text":" is a new type in Core ML for efficient computation."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Supports common math and transformation operations typical of machine learning frameworks."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Executes using Apple siliconâ€™s powerful compute capabilities for high performance.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Similar to popular Python numerical libraries, making it intuitive for those familiar with machine learning.","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Simplifying Large Language Models"}]},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"MLTensor simplifies decoding outputs from language models."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Example model: An autoregressive language model predicts the next word based on preceding words.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Generates sentences by predicting words until an end-of-sequence token or set length is reached.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Outputs scores for all words in a vocabulary, representing confidence for the next word.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"A decoder selects the next word using various strategies (e.g., highest score, random sampling).","type":"text"}]}]}],"type":"unorderedList"},{"type":"codeListing","syntax":"swift","code":["\/\/ Introduction to MLTensor","","\/\/Creating","let xArray = MLSHapedArray<Float>(repeating: 0.0, shape: [3])","let x = MLTensor(xArray)","let y = MLTensor([[0.5, 0.1, 0.2], [0.4, 2.0, 0.4], [0.2, 0.6, -0.1]])","","print(\"\\(x.shape), \\(x.scalarType)\") \/\/ [3], Float","print(\"\\(y.shape), \\(y.scalarType)\") \/\/ [3, 3], Float","","\/\/ Math","var result = x + y ","result += 2","let mean = result.mean()","","\/\/ Comparison","let mask = result .>= mean","let filtered = result * mask","","\/\/Indexing and shape transformation","let sliced = filtered[0, ...] \/\/ Shape [2, 3] -> [3]","let reshaped = sliced.reshaped(to: [1, 3]) \/\/ Shape [3] -> [1, 3] ","","\/\/ Materialize to a MLShapedArray","let reshapedArray = await reshaped.shapedArray(of: Float.self) ",""]},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Benefits"}]},{"type":"text","text":":"}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"MLTensor requires less code to achieve the same functionality compared to low-level APIs.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"While low-level APIs are still necessary for some tasks, MLTensor provides a concise alternative for common tasks.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Focus more on creating great experiences, less on low-level details."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"Summary","type":"text"}],"type":"strong"},{"type":"text","text":":"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"MLTensor offers a convenient, efficient, and high-performance way to handle common machine learning computations in Core ML."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Ideal for simplifying and improving tasks like decoding language model outputs."}]}]}]},{"type":"heading","level":1,"text":"Models with state","anchor":"Models-with-state"},{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"text":"Stateless vs. Stateful Models","type":"text"}]},{"type":"text","text":":"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"inlineContent":[{"type":"text","text":"Stateless Models"}],"type":"strong"},{"type":"text","text":": Process each input independently without retaining history (e.g., image classifiers using Convolutional Neural Networks)."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Stateful Models"}]},{"type":"text","text":": Retain history from previous inputs (e.g., Recurrent Neural Networks for sequence data)."}]}]}]},{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"text":"Manual State Management","type":"text"}]},{"type":"text","text":":"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Traditionally, state management involves passing the state as an input and retrieving it from the output, incurring overhead especially with larger states.","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"Core ML Enhancements","type":"text"}],"type":"strong"},{"text":":","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Core ML now supports automatic state management, reducing overhead and improving efficiency.","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Example: Language Models and KV Cache"}]},{"type":"text","text":":"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Language models use "},{"code":"key-value (KV)","type":"codeVoice"},{"type":"text","text":" vectors for each word to enhance contextual predictions."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Previously, handling the KV cache manually added overhead."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Core ML can now manage the KV cache using states, leading to faster prediction times."}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Implementation"}]},{"text":":","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"States must be explicitly added during the preparation phase of the model.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Verify state support in Xcode, where states appear above model inputs in the predictions tab.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Update code to use Core ML states:"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Core ML preallocates buffers for the key and value vectors."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Pass the state handle instead of the cache."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"In-place updates eliminate the need to manually update the cache."}]}]}],"type":"unorderedList"}]}]},{"type":"paragraph","inlineContent":[{"inlineContent":[{"type":"text","text":"Performance Improvement"}],"type":"strong"},{"type":"text","text":":"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Example comparison using the Mistral 7 billion model on a MacBook Pro with M3 Max:","type":"text"}]},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Without state: ~8 seconds","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"With state: ~5 seconds (1.6x speedup)"}]}]}],"type":"unorderedList"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Performance gains vary by model and hardware but demonstrate significant efficiency improvements with state support.","type":"text"}]}]}]},{"level":1,"text":"Multifunction Models in Core ML","anchor":"Multifunction-Models-in-Core-ML","type":"heading"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Flexible Deployment","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Core ML now supports models with multiple functionalities, represented as functions."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Traditionally, neural networks in Core ML consist of a single function block, but now multiple functions are supported."}]}]},{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Example: Adapters","type":"text"}]}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Adapters are small modules trained with a networkâ€™s knowledge for another task, extending a modelâ€™s functionality without adjusting its weights."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Multiple adapters can now be merged into a single model, each exposed as a separate function."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Efficiently deploy models with multiple adapters without needing multiple specialized models or passing adapter weights as inputs."}]}]}]}]}],"type":"unorderedList"},{"level":1,"text":"Enhanced Performance Tools","anchor":"Enhanced-Performance-Tools","type":"heading"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Core ML Performance Report"}]},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Generate performance reports for any connected device without writing code.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Steps: Open model in Xcode â†’ Select performance tab â†’ Create new report â†’ Select device and compute unit â†’ Run test.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Reports include load and prediction times, compute unit usage, and estimated time for each operation."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Sort operations by estimated time to identify bottlenecks.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Hover over unsupported operations for hints on compatibility issues.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Debugging and Profiling","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Performance reports can now be exported and compared across runs to assess the impact of model changes.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"The new MLComputePlan API offers debugging and profiling information similar to the performance report."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"MLComputePlan API provides model structure, runtime information, supported\/preferred compute devices, operation support status, and estimated relative cost for each operation."}]}]}],"type":"unorderedList"},{"level":2,"text":"Written By","anchor":"Written-By","type":"heading"},{"numberOfColumns":5,"columns":[{"size":1,"content":[{"inlineContent":[{"identifier":"RamitSharma991","type":"image"}],"type":"paragraph"}]},{"size":4,"content":[{"type":"heading","anchor":"Ramit-Sharma","text":"Ramit Sharma","level":3},{"type":"paragraph","inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/RamitSharma991","isActive":true,"overridingTitle":"Contributed Notes","overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"type":"reference"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/github.com\/RamitSharma991","isActive":true,"type":"reference"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/x.com\/iosDev_ramit","isActive":true,"type":"reference"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/","isActive":true,"type":"reference"}]}]}],"type":"row"},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"}],"type":"paragraph"},{"level":2,"text":"Related Sessions","anchor":"Related-Sessions","type":"heading"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression"],"style":"list","type":"links"},{"inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}],"type":"small"},{"inlineContent":[{"text":"All content copyright Â© 2012 â€“ 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}],"kind":"content"}],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10161-Deploy-machine-learning-and-AI-models-ondevice-with-Core-ML","interfaceLanguage":"swift"},"schemaVersion":{"patch":0,"minor":3,"major":0},"abstract":[{"text":"Learn new ways to optimize speed and memory performance when you convert and run machine learning and AI models through Core ML. Weâ€™ll cover new options for model representations, performance insights, execution, and model stitching which can be used together to create compelling and private on-device experiences.","type":"text"}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24"]]},"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc24\/10161","overridingTitle":"Watch Video","isActive":true}},"metadata":{"roleHeading":"WWDC24","title":"Deploy machine learning and AI models on-device with Core ML","modules":[{"name":"WWDC Notes"}],"role":"sampleCode"},"sections":[],"kind":"article","references":{"RamitSharma991":{"variants":[{"url":"\/images\/RamitSharma991.jpeg","traits":["1x","light"]}],"identifier":"RamitSharma991","alt":"Profile image of Ramit Sharma","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24":{"kind":"article","type":"topic","title":"WWDC24","abstract":[{"type":"text","text":"Xcode 16, Swift 6, iOS 18, macOS 15 (Sequoia), tvOS 18, visionOS 2, watchOS 11."},{"type":"text","text":" "},{"type":"text","text":"New APIs: Swift Testing, "},{"type":"codeVoice","code":"FinanceKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"TabletopKit"},{"type":"text","text":", and more."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24","images":[{"identifier":"WWDC24-Icon.png","type":"icon"},{"identifier":"WWDC24.jpeg","type":"card"}],"url":"\/documentation\/wwdcnotes\/wwdc24","role":"collectionGroup"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"type":"link"},"WWDC24-Icon.png":{"variants":[{"url":"\/images\/WWDC24-Icon.png","traits":["1x","light"]}],"identifier":"WWDC24-Icon.png","alt":null,"type":"image"},"WWDC24.jpeg":{"variants":[{"url":"\/images\/WWDC24.jpeg","traits":["1x","light"]}],"identifier":"WWDC24.jpeg","alt":null,"type":"image"},"https://github.com/RamitSharma991":{"url":"https:\/\/github.com\/RamitSharma991","title":"GitHub","identifier":"https:\/\/github.com\/RamitSharma991","titleInlineContent":[{"text":"GitHub","type":"text"}],"type":"link"},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"kind":"symbol","title":"WWDC Notes","role":"collection","url":"\/documentation\/wwdcnotes"},"doc://WWDCNotes/documentation/WWDCNotes/RamitSharma991":{"type":"topic","images":[{"type":"card","identifier":"RamitSharma991.jpeg"},{"type":"icon","identifier":"RamitSharma991.jpeg"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/RamitSharma991","title":"Ramit Sharma (15 notes)","kind":"article","url":"\/documentation\/wwdcnotes\/ramitsharma991","role":"sampleCode","abstract":[{"type":"text","text":"Indie iOS Dev. Swift, SwiftUI, Obj-C, UX and related."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression":{"kind":"article","title":"Use Core ML Tools for machine learning model compression","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10047-use-core-ml-tools-for-machine-learning-model-compression","abstract":[{"type":"text","text":"Discover how to reduce the footprint of machine learning models in your app with Core ML Tools. Learn how to use techniques like palettization, pruning, and quantization to dramatically reduce model size while still achieving great accuracy. Explore comparisons between compression during the training stages and on fully trained models, and learn how compressed models can run even faster when your app takes full advantage of the Apple Neural Engine."}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression"},"RamitSharma991.jpeg":{"variants":[{"url":"\/images\/RamitSharma991.jpeg","traits":["1x","light"]}],"identifier":"RamitSharma991.jpeg","alt":null,"type":"image"},"https://":{"url":"https:\/\/","title":"Blog","identifier":"https:\/\/","titleInlineContent":[{"text":"Blog","type":"text"}],"type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction":{"title":"Improve Core ML integration with async prediction","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","abstract":[{"text":"Learn how to speed up machine learning features in your app with the latest Core ML execution engine improvements and find out how aggressive asset caching can help with inference and faster model loads. Weâ€™ll show you some of the latest options for async prediction and discuss considerations for balancing performance with overall memory usage to help you create a highly responsive app. Discover APIs to help you understand and maximize hardware utilization for your models.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10049-improve-core-ml-integration-with-async-prediction","kind":"article","type":"topic","role":"sampleCode"},"https://developer.apple.com/wwdc24/10161":{"url":"https:\/\/developer.apple.com\/wwdc24\/10161","checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc24\/10161","type":"download"},"https://x.com/iosDev_ramit":{"url":"https:\/\/x.com\/iosDev_ramit","title":"X\/Twitter","identifier":"https:\/\/x.com\/iosDev_ramit","titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"type":"link"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"identifier":"WWDCNotes.png","alt":null,"type":"image"}}}