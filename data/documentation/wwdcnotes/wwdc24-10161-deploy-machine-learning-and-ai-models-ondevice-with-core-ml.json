{"sections":[],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10161-Deploy-machine-learning-and-AI-models-ondevice-with-Core-ML","interfaceLanguage":"swift"},"primaryContentSections":[{"kind":"content","content":[{"type":"heading","anchor":"Key-takeaways","level":2,"text":"Key takeaways"},{"type":"paragraph","inlineContent":[{"text":"ðŸ¦¾ Core ML enhances the deployment and execution of ML and AI models.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"ðŸ§® Integration with MLTensor for common math and transformation operations.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"ðŸ“ˆ Improve inference efficiency by leveraging stateful models.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"ðŸš€ Deploy models with multiple functionalities.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"ðŸ”§ Utilize new profiling and debugging information to optimize your models"}]},{"anchor":"Presenter","text":"Presenter","type":"heading","level":2},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Joshua Newnham, Core ML Engineer"}],"type":"paragraph"}]}],"type":"unorderedList"},{"text":"Integeration","type":"heading","anchor":"Integeration","level":1},{"type":"paragraph","inlineContent":[{"type":"text","text":"The machine learning workflow consists of three main phases:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"inlineContent":[{"type":"text","text":"Model Training"}],"type":"strong"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Preparation"}]}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Integration","type":"text"}]}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"inlineContent":[{"type":"text","text":"Model Integration"}],"type":"strong"},{"text":":","type":"text"}]},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Begins with an ML package created during the preparation phase."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"Core ML"},{"type":"text","text":" simplifies integration and usage in your app, offering a unified API for on-device inference across various model types."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Utilizes Apple siliconâ€™s CPU, GPU, and Neural Engine via MPS Graph and BNNS Graph for high performance, especially with Metal integration or real-time CPU inference."}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"Performance Improvements"}],"type":"strong"},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"code":"Core ML","type":"codeVoice"},{"type":"text","text":" now delivers better performance with significant inference stack improvements."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"iOS 18 is faster than iOS 17 for many models, with no need for recompiling or code changes."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Speed improvements vary by model and hardware."}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"inlineContent":[{"text":"Model Integration Steps","type":"text"}],"type":"strong"},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Simple integration: pass input, read output."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Advanced use cases (e.g., generative AI): may require multiple models and iterative processes."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Additional computation outside the model may involve implementing operations from scratch or using low-level APIs, leading to complex code."}],"type":"paragraph"}]}],"type":"unorderedList"},{"text":"MLTensor","type":"heading","level":1,"anchor":"MLTensor"},{"text":"Introducing MLTensor","type":"heading","level":3,"anchor":"Introducing-MLTensor"},{"inlineContent":[{"inlineContent":[{"text":"Overview","type":"text"}],"type":"strong"},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"code":"MLTensor","type":"codeVoice"},{"text":" is a new type in Core ML for efficient computation.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Supports common math and transformation operations typical of machine learning frameworks."}]}]},{"content":[{"inlineContent":[{"text":"Executes using Apple siliconâ€™s powerful compute capabilities for high performance.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Similar to popular Python numerical libraries, making it intuitive for those familiar with machine learning."}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Simplifying Large Language Models","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"MLTensor simplifies decoding outputs from language models.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Example model: An autoregressive language model predicts the next word based on preceding words.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Generates sentences by predicting words until an end-of-sequence token or set length is reached.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Outputs scores for all words in a vocabulary, representing confidence for the next word."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"A decoder selects the next word using various strategies (e.g., highest score, random sampling).","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"code":["\/\/ Introduction to MLTensor","","\/\/Creating","let xArray = MLSHapedArray<Float>(repeating: 0.0, shape: [3])","let x = MLTensor(xArray)","let y = MLTensor([[0.5, 0.1, 0.2], [0.4, 2.0, 0.4], [0.2, 0.6, -0.1]])","","print(\"\\(x.shape), \\(x.scalarType)\") \/\/ [3], Float","print(\"\\(y.shape), \\(y.scalarType)\") \/\/ [3, 3], Float","","\/\/ Math","var result = x + y ","result += 2","let mean = result.mean()","","\/\/ Comparison","let mask = result .>= mean","let filtered = result * mask","","\/\/Indexing and shape transformation","let sliced = filtered[0, ...] \/\/ Shape [2, 3] -> [3]","let reshaped = sliced.reshaped(to: [1, 3]) \/\/ Shape [3] -> [1, 3] ","","\/\/ Materialize to a MLShapedArray","let reshapedArray = await reshaped.shapedArray(of: Float.self) ",""],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"inlineContent":[{"text":"Benefits","type":"text"}],"type":"strong"},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"MLTensor requires less code to achieve the same functionality compared to low-level APIs."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"While low-level APIs are still necessary for some tasks, MLTensor provides a concise alternative for common tasks."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Focus more on creating great experiences, less on low-level details.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Summary"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"MLTensor offers a convenient, efficient, and high-performance way to handle common machine learning computations in Core ML."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Ideal for simplifying and improving tasks like decoding language model outputs."}],"type":"paragraph"}]}],"type":"unorderedList"},{"text":"Models with state","type":"heading","level":1,"anchor":"Models-with-state"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Stateless vs. Stateful Models","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Stateless Models"}]},{"text":": Process each input independently without retaining history (e.g., image classifiers using Convolutional Neural Networks).","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"Stateful Models","type":"text"}],"type":"strong"},{"type":"text","text":": Retain history from previous inputs (e.g., Recurrent Neural Networks for sequence data)."}]}]}],"type":"unorderedList"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"Manual State Management"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Traditionally, state management involves passing the state as an input and retrieving it from the output, incurring overhead especially with larger states."}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Core ML Enhancements","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Core ML now supports automatic state management, reducing overhead and improving efficiency.","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"Example: Language Models and KV Cache"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Language models use "},{"type":"codeVoice","code":"key-value (KV)"},{"type":"text","text":" vectors for each word to enhance contextual predictions."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Previously, handling the KV cache manually added overhead.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Core ML can now manage the KV cache using states, leading to faster prediction times.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Implementation","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"States must be explicitly added during the preparation phase of the model."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Verify state support in Xcode, where states appear above model inputs in the predictions tab."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Update code to use Core ML states:","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Core ML preallocates buffers for the key and value vectors.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Pass the state handle instead of the cache."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"In-place updates eliminate the need to manually update the cache.","type":"text"}]}]}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Performance Improvement"}]},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Example comparison using the Mistral 7 billion model on a MacBook Pro with M3 Max:"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Without state: ~8 seconds"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"With state: ~5 seconds (1.6x speedup)"}],"type":"paragraph"}]}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Performance gains vary by model and hardware but demonstrate significant efficiency improvements with state support."}],"type":"paragraph"}]}],"type":"unorderedList"},{"text":"Multifunction Models in Core ML","type":"heading","level":1,"anchor":"Multifunction-Models-in-Core-ML"},{"inlineContent":[{"inlineContent":[{"text":"Flexible Deployment","type":"text"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Core ML now supports models with multiple functionalities, represented as functions.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Traditionally, neural networks in Core ML consist of a single function block, but now multiple functions are supported."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Example: Adapters"}]}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Adapters are small modules trained with a networkâ€™s knowledge for another task, extending a modelâ€™s functionality without adjusting its weights.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Multiple adapters can now be merged into a single model, each exposed as a separate function."}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Efficiently deploy models with multiple adapters without needing multiple specialized models or passing adapter weights as inputs."}],"type":"paragraph"}]}]}]}],"type":"unorderedList"},{"text":"Enhanced Performance Tools","type":"heading","level":1,"anchor":"Enhanced-Performance-Tools"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Core ML Performance Report","type":"text"}]},{"type":"text","text":":"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Generate performance reports for any connected device without writing code.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Steps: Open model in Xcode â†’ Select performance tab â†’ Create new report â†’ Select device and compute unit â†’ Run test.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Reports include load and prediction times, compute unit usage, and estimated time for each operation."}]}]},{"content":[{"inlineContent":[{"text":"Sort operations by estimated time to identify bottlenecks.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Hover over unsupported operations for hints on compatibility issues."}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"inlineContent":[{"text":"Debugging and Profiling","type":"text"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Performance reports can now be exported and compared across runs to assess the impact of model changes.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The new MLComputePlan API offers debugging and profiling information similar to the performance report.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"MLComputePlan API provides model structure, runtime information, supported\/preferred compute devices, operation support status, and estimated relative cost for each operation.","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"text":"Written By","type":"heading","level":2,"anchor":"Written-By"},{"numberOfColumns":5,"type":"row","columns":[{"content":[{"inlineContent":[{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/20319411?v=4","type":"image"}],"type":"paragraph"}],"size":1},{"content":[{"type":"heading","anchor":"Ramit-Sharma","level":3,"text":"Ramit Sharma"},{"inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/RamitSharma991","type":"reference","overridingTitle":"Contributed Notes","isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/github.com\/RamitSharma991","type":"reference","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/x.com\/iosDev_ramit","type":"reference","isActive":true},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"identifier":"https:\/\/","type":"reference","isActive":true}],"type":"paragraph"}],"size":4}]},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"isActive":true,"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"reference"}],"type":"paragraph"},{"text":"Related Sessions","type":"heading","level":2,"anchor":"Related-Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression"],"type":"links","style":"list"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"text":"All content copyright Â© 2012 â€“ 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}]}],"kind":"article","variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc24-10161-deploy-machine-learning-and-ai-models-ondevice-with-core-ml"],"traits":[{"interfaceLanguage":"swift"}]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24"]]},"schemaVersion":{"major":0,"minor":3,"patch":0},"seeAlsoSections":[{"identifiers":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10184-A-Swift-Tour-Explore-Swifts-features-and-design","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10218-Accelerate-machine-learning-with-Metal","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10140-Add-personality-to-your-app-through-UX-writing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10173-Analyze-heap-memory","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10172-Break-into-the-RealityKit-debugger","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10067-Bring-context-to-todays-weather","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10220-Bring-expression-to-your-app-with-Genmoji","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10068-Bring-your-Live-Activity-to-Apple-Watch","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10133-Bring-your-app-to-Siri","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10210-Bring-your-apps-core-features-to-users-with-App-Intents","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10093-Bring-your-iOS-or-iPadOS-game-to-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10159-Bring-your-machine-learning-and-AI-models-to-Apple-silicon","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10069-Broadcast-updates-to-your-Live-Activities","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10204-Build-a-great-Lock-Screen-camera-capture-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10166-Build-compelling-spatial-photo-and-video-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10084-Build-custom-swimming-workouts-with-WorkoutKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10066-Build-immersive-web-experiences-with-WebXR","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10185-Build-multilingualready-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10088-Capture-HDR-content-with-ScreenCaptureKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10073-Catch-up-on-accessibility-in-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10102-Compose-interactive-3D-content-in-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10170-Consume-noncopyable-types-in-Swift","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10138-Create-a-custom-data-store-with-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10087-Create-custom-environments-for-your-immersive-apps-in-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10152-Create-custom-hover-effects-in-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10151-Create-custom-visual-effects-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10100-Create-enhanced-spatial-computing-experiences-with-ARKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10070-Customize-feature-discovery-with-TipKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10201-Customize-spatial-Persona-templates-in-SharePlay","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10146-Demystify-SwiftUI-containers","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10171-Demystify-explicitly-built-modules","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10176-Design-App-Intents-for-system-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10098-Design-Live-Activities-for-Apple-Watch","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10085-Design-advanced-games-for-Apple-platforms","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10086-Design-great-visionOS-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10096-Design-interactive-experiences-for-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10103-Discover-RealityKit-APIs-for-iOS-macOS-and-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10163-Discover-Swift-enhancements-in-the-Vision-framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10107-Discover-area-mode-for-Object-Capture","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10113-Discover-media-performance-metrics-in-AVFoundation","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10153-Dive-deep-into-volumes-and-immersive-spaces","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10147-Elevate-your-tab-and-sidebar-experience-in-iPadOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10114-Enhance-ad-experiences-with-HLS-interstitials","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10115-Enhance-the-immersion-of-media-viewing-in-custom-environments","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10145-Enhance-your-UI-animations-and-transitions","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-111801-Enhance-your-spatial-computing-app-with-RealityKit-audio","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10209-Enhanced-suggestions-for-your-journaling-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10132-Evolve-your-document-launch-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10062-Explore-App-Store-server-APIs-for-InApp-Purchase","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10217-Explore-Swift-performance","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10094-Explore-game-input-in-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10223-Explore-machine-learning-on-Apple-platforms","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10116-Explore-multiview-video-playback-in-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10101-Explore-object-tracking-for-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10216-Explore-the-Swift-on-Server-ecosystem","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10109-Explore-wellbeing-APIs-in-HealthKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10200-Extend-your-Xcode-Cloud-workflows","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10157-Extend-your-apps-controls-across-the-system","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10074-Get-started-with-Dynamic-Type","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10083-Get-started-with-HealthKit-in-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10168-Get-started-with-Writing-Tools","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10195-Go-further-with-Swift-Testing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10197-Go-small-with-Embedded-Swift","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10110-Implement-App-Store-Offers","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10162-Keep-colors-consistent-across-captures","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10207-Migrate-your-TVML-app-to-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10169-Migrate-your-app-to-Swift-6","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10065-Optimize-for-the-spatial-web","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10186-Optimize-your-3D-assets-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10089-Port-advanced-games-to-Apple-platforms","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10092-Render-Metal-with-passthrough-in-visionOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10198-Run-Break-Inspect-Explore-effective-debugging-in-LLDB","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10112-Say-hello-to-the-next-generation-of-CarPlay-design-system","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10214-Squeeze-the-most-out-of-Apple-Pencil","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10125-Streamline-signin-with-passkey-upgrades-and-credential-managers","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10211-Support-realtime-ML-inference-on-the-CPU","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10131-Support-semantic-search-with-Core-Spotlight","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10155-Swift-Charts-Vectorized-and-function-plots","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10150-SwiftUI-essentials","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10148-Tailor-macOS-windows-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10075-Track-model-changes-with-SwiftData-history","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10160-Train-your-machine-learning-and-AI-models-on-Apple-GPUs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10097-Unlock-the-power-of-places-with-MapKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10122-Use-CloudKit-Console-to-monitor-and-optimize-database-activity","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10177-Use-HDR-for-dynamic-image-experiences-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10149-Work-with-windows-in-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10181-Xcode-essentials"],"generated":true,"title":"Deep Dives into Topics"}],"sampleCodeDownload":{"action":{"isActive":true,"overridingTitle":"Watch Video","identifier":"https:\/\/developer.apple.com\/wwdc24\/10161","type":"reference"},"kind":"sampleDownload"},"abstract":[{"text":"Learn new ways to optimize speed and memory performance when you convert and run machine learning and AI models through Core ML. Weâ€™ll cover new options for model representations, performance insights, execution, and model stitching which can be used together to create compelling and private on-device experiences.","type":"text"}],"metadata":{"modules":[{"name":"WWDC Notes"}],"role":"sampleCode","title":"Deploy machine learning and AI models on-device with Core ML","roleHeading":"WWDC24"},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10216-Explore-the-Swift-on-Server-ecosystem":{"title":"Explore the Swift on Server ecosystem","abstract":[{"type":"text","text":"Swift is a great language for writing your server applications, and powers critical services across Appleâ€™s cloud products. Weâ€™ll explore tooling, delve into the Swift server package ecosystem, and demonstrate how to interact with databases and add observability to applications."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10216-explore-the-swift-on-server-ecosystem","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10216-Explore-the-Swift-on-Server-ecosystem","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10065-Optimize-for-the-spatial-web":{"title":"Optimize for the spatial web","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10065-Optimize-for-the-spatial-web","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10065-optimize-for-the-spatial-web","kind":"article","role":"sampleCode","abstract":[{"text":"Discover how to make the most of visionOS capabilities on the web. Explore recent updates like improvements to selection highlighting, and the ability to present spatial photos and panorama images in fullscreen. Learn to take advantage of existing web standards for dictation and text-to-speech with WebSpeech, spatial soundscapes with WebAudio, and immersive experiences with WebXR.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10204-Build-a-great-Lock-Screen-camera-capture-experience":{"url":"\/documentation\/wwdcnotes\/wwdc24-10204-build-a-great-lock-screen-camera-capture-experience","abstract":[{"text":"Find out how the LockedCameraCapture API can help you bring your capture applicationâ€™s most useful information directly to the Lock Screen. Examine the APIâ€™s features and functionality, learn how to get started creating a capture extension, and find out how that extension behaves when the device is locked.","type":"text"}],"kind":"article","title":"Build a great Lock Screen camera capture experience","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10204-Build-a-great-Lock-Screen-camera-capture-experience","type":"topic"},"WWDCNotes.png":{"identifier":"WWDCNotes.png","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10074-Get-started-with-Dynamic-Type":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10074-Get-started-with-Dynamic-Type","title":"Get started with Dynamic Type","abstract":[{"type":"text","text":"Dynamic Type lets people choose their preferred text size across the system and all of their apps. To help you get started supporting Dynamic Type, weâ€™ll cover the fundamentals: How it works, how to find issues with scaling text in your app, and how to take practical steps using SwiftUI and UIKit to create a great Dynamic Type experience. Weâ€™ll also show how you can best use the Large Content Viewer to make navigation controls accessible to everyone."}],"kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc24-10074-get-started-with-dynamic-type"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10166-Build-compelling-spatial-photo-and-video-experiences":{"title":"Build compelling spatial photo and video experiences","abstract":[{"type":"text","text":"Learn how to adopt spatial photos and videos in your apps. Explore the different types of stereoscopic media and find out how to capture spatial videos in your iOS app on iPhone 15 Pro. Discover the various ways to detect and present spatial media, including the new QuickLook Preview Application API in visionOS. And take a deep dive into the metadata and stereo concepts that make a photo or video spatial."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10166-build-compelling-spatial-photo-and-video-experiences","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10166-Build-compelling-spatial-photo-and-video-experiences","role":"sampleCode"},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"link","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"title":"Contributions are welcome!"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10155-Swift-Charts-Vectorized-and-function-plots":{"abstract":[{"type":"text","text":"The plot thickens! Learn how to render beautiful charts representing math functions and extensive datasets using function and vectorized plots in your app. Whether youâ€™re looking to display functions common in aerodynamics, magnetism, and higher order field theory, or create large interactive heat maps, Swift Charts has you covered."}],"title":"Swift Charts: Vectorized and function plots","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10155-swift-charts-vectorized-and-function-plots","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10155-Swift-Charts-Vectorized-and-function-plots","role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10068-Bring-your-Live-Activity-to-Apple-Watch":{"kind":"article","role":"sampleCode","title":"Bring your Live Activity to Apple Watch","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10068-bring-your-live-activity-to-apple-watch","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10068-Bring-your-Live-Activity-to-Apple-Watch","abstract":[{"text":"Bring Live Activities into the Smart Stack on Apple Watch with iOS 18 and watchOS 11. Weâ€™ll cover how Live Activities are presented on Apple Watch, as well as how you can enhance their presentation for the Smart Stack. Weâ€™ll also explore additional considerations to ensure Live Activities on Apple Watch always present up-to-date information.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10145-Enhance-your-UI-animations-and-transitions":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10145-Enhance-your-UI-animations-and-transitions","title":"Enhance your UI animations and transitions","abstract":[{"type":"text","text":"Explore how to adopt the zoom transition in navigation and presentations to increase the sense of continuity in your app, and learn how to animate UIKit views with SwiftUI animations to make it easier to build animations that feel continuous."}],"kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc24-10145-enhance-your-ui-animations-and-transitions"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10109-Explore-wellbeing-APIs-in-HealthKit":{"url":"\/documentation\/wwdcnotes\/wwdc24-10109-explore-wellbeing-apis-in-healthkit","abstract":[{"text":"Learn how to incorporate mental health and wellbeing into your app using HealthKit. There are new APIs for State of Mind, as well as for Depression Risk and Anxiety Risk. Weâ€™ll dive into principles of emotion science to cover how reflecting on feelings can be beneficial, and how State of Mind can be used to represent different types of mood and emotion.","type":"text"}],"kind":"article","title":"Explore wellbeing APIs in HealthKit","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10109-Explore-wellbeing-APIs-in-HealthKit","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10097-Unlock-the-power-of-places-with-MapKit":{"title":"Unlock the power of places with MapKit","abstract":[{"type":"text","text":"Discover powerful new ways to integrate maps into your apps and websites with MapKit and MapKit JS.  Learn how to save and reference unique places using Place ID. Check out improvements to search that make it more efficient to find relevant places.  Get introduced to the new Place Card API that lets you display rich information about places so customers can explore destinations right in your app. And, weâ€™ll show you quick ways to embed maps in your website with our simplified token provisioning and Web Embed API."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10097-unlock-the-power-of-places-with-mapkit","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10097-Unlock-the-power-of-places-with-MapKit","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10096-Design-interactive-experiences-for-visionOS":{"title":"Design interactive experiences for visionOS","abstract":[{"text":"Learn how you can design a compelling interactive narrative experience for Apple Vision Pro from the designers of Encounter Dinosaurs. Discover how these types of experiences differ from existing apps, media, and games, and explore how to design narratives that bring audiences into new worlds. Find out how you can create stories that adapt to any space and size, provide multiple levels of interaction to make them accessible to all, and use animation, spatial audio, and custom gestures to further immerse people in your experience.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10096-design-interactive-experiences-for-visionos","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10096-Design-interactive-experiences-for-visionOS","role":"sampleCode"},"https://x.com/iosDev_ramit":{"identifier":"https:\/\/x.com\/iosDev_ramit","url":"https:\/\/x.com\/iosDev_ramit","type":"link","titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"title":"X\/Twitter"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10107-Discover-area-mode-for-Object-Capture":{"title":"Discover area mode for Object Capture","abstract":[{"text":"Discover how area mode for Object Capture enables new 3D capture possibilities on iOS by extending the functionality of Object Capture to support capture and reconstruction of an area. Learn how to optimize the quality of iOS captures using the new macOS sample app for reconstruction, and find out how to view the final results with Quick Look on Apple Vision Pro, iPhone, iPad or Mac. Learn about improvements to 3D reconstruction, including a new API that allows you to create your own custom image processing pipelines.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10107-discover-area-mode-for-object-capture","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10107-Discover-area-mode-for-Object-Capture","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10104-Build-a-spatial-drawing-app-with-RealityKit","role":"sampleCode","type":"topic","abstract":[{"type":"text","text":"Harness the power of RealityKit through the process of building a spatial drawing app. As you create an eye-catching spatial experience that integrates RealityKit with ARKit and SwiftUI, youâ€™ll explore how resources work in RealityKit and how to use features like low-level mesh and texture APIs to achieve fast updates of the usersâ€™ brush strokes."}],"title":"Build a spatial drawing app with RealityKit","url":"\/documentation\/wwdcnotes\/wwdc24-10104-build-a-spatial-drawing-app-with-realitykit"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10177-Use-HDR-for-dynamic-image-experiences-in-your-app":{"title":"Use HDR for dynamic image experiences in your app","abstract":[{"type":"text","text":"Discover how to read and write HDR images and process HDR content in your app. Explore the new supported HDR image formats and advanced methods for displaying HDR images. Find out how HDR content can coexist with your user interface â€” and what to watch out for when adding HDR image support to your app."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10177-use-hdr-for-dynamic-image-experiences-in-your-app","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10177-Use-HDR-for-dynamic-image-experiences-in-your-app","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10169-Migrate-your-app-to-Swift-6":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10169-migrate-your-app-to-swift-6","abstract":[{"text":"Experience Swift 6 migration in action as we update an existing sample app. Learn how to migrate incrementally, module by module, and how the compiler helps you identify code thatâ€™s at risk of data races.  Discover different techniques for ensuring clear isolation boundaries and eliminating concurrent access to shared mutable state.","type":"text"}],"title":"Migrate your app to Swift 6","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10169-Migrate-your-app-to-Swift-6"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10168-Get-started-with-Writing-Tools":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10168-get-started-with-writing-tools","abstract":[{"type":"text","text":"Learn how Writing Tools help users proofread, rewrite, and transform text in your app. Get the details on how Writing Tools interact with your app so users can refine what they have written in any text view. Understand how text is retrieved and processed, and how to support Writing Tools in custom text views."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10168-Get-started-with-Writing-Tools","type":"topic","title":"Get started with Writing Tools","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10197-Go-small-with-Embedded-Swift":{"title":"Go small with Embedded Swift","abstract":[{"type":"text","text":"Embedded Swift brings the safety and expressivity of Swift to constrained environments. Explore how Embedded Swift runs on a variety of microcontrollers through a demonstration using an off-the-shelf Matter device. Learn how the Embedded Swift subset packs the benefits of Swift into a tiny footprint with no runtime, and discover plenty of resources to start your own Embedded Swift adventure."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10197-go-small-with-embedded-swift","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10197-Go-small-with-Embedded-Swift","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10218-Accelerate-machine-learning-with-Metal":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10218-accelerate-machine-learning-with-metal","abstract":[{"text":"Learn how to accelerate your machine learning transformer models with new features in Metal Performance Shaders Graph. Weâ€™ll also cover how to improve your modelâ€™s compute bandwidth and quality, and visualize it in the all new MPSGraph viewer.","type":"text"}],"title":"Accelerate machine learning with Metal","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10218-Accelerate-machine-learning-with-Metal"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10147-Elevate-your-tab-and-sidebar-experience-in-iPadOS":{"url":"\/documentation\/wwdcnotes\/wwdc24-10147-elevate-your-tab-and-sidebar-experience-in-ipados","abstract":[{"text":"iPadOS 18 introduces a new navigation system that gives people the flexibility to choose between using a tab bar or sidebar. The newly redesigned tab bar provides more space for content and other functionality. Learn how to use SwiftUI and UIKit to enable customization features â€“ like adding, removing and reordering tabs â€“ to enable a more personal touch in your app.","type":"text"}],"kind":"article","title":"Elevate your tab and sidebar experience in iPadOS","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10147-Elevate-your-tab-and-sidebar-experience-in-iPadOS","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10149-Work-with-windows-in-SwiftUI":{"title":"Work with windows in SwiftUI","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10149-Work-with-windows-in-SwiftUI","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10149-work-with-windows-in-swiftui","kind":"article","role":"sampleCode","abstract":[{"text":"Learn how to create great single and multi-window apps in visionOS, macOS, and iPadOS. Discover tools that let you programmatically open and close windows, adjust position and size, and even replace one window with another. Weâ€™ll also explore design principles for windows that help people use your app within their workflows.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10094-Explore-game-input-in-visionOS":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10094-explore-game-input-in-visionos","abstract":[{"text":"Discover how to design and implement great input for your game in visionOS. Learn how system gestures let you provide frictionless ways for players to interact with your games. And explore best practices for supporting custom gestures and game controllers.","type":"text"}],"title":"Explore game input in visionOS","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10094-Explore-game-input-in-visionOS"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10140-Add-personality-to-your-app-through-UX-writing":{"title":"Add personality to your app through UX writing","abstract":[{"type":"text","text":"Every app has a personality that comes across in what you say â€” and how you say it. Learn how to define your appâ€™s voice and modulate your tone for every situation, from celebratory notifications to error messages. Weâ€™ll help you get specific about your appâ€™s purpose and audience and practice writing in different tones."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10140-add-personality-to-your-app-through-ux-writing","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10140-Add-personality-to-your-app-through-UX-writing","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10148-Tailor-macOS-windows-with-SwiftUI":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10148-tailor-macos-windows-with-swiftui","abstract":[{"type":"text","text":"Make your windows feel tailor-made for macOS. Fine-tune your appâ€™s windows for focused purposes, ease of use, and to express functionality. Use SwiftUI to style window toolbars and backgrounds. Arrange your windows with precision, and make smart decisions about restoration and minimization."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10148-Tailor-macOS-windows-with-SwiftUI","type":"topic","title":"Tailor macOS windows with SwiftUI","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10110-Implement-App-Store-Offers":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10110-implement-app-store-offers","abstract":[{"text":"Learn how to engage customers with App Store Offers using App Store Connect, as well as the latest StoreKit features and APIs. Discover how you can set up win-back offers (a new way to re-engage previous subscribers) and generate offer codes for Mac apps. And find out how to test offers in sandbox and Xcode to make sure they work smoothly.","type":"text"}],"title":"Implement App Store Offers","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10110-Implement-App-Store-Offers"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10153-Dive-deep-into-volumes-and-immersive-spaces":{"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10153-Dive-deep-into-volumes-and-immersive-spaces","url":"\/documentation\/wwdcnotes\/wwdc24-10153-dive-deep-into-volumes-and-immersive-spaces","kind":"article","title":"Dive deep into volumes and immersive spaces","abstract":[{"text":"Discover powerful new ways to customize volumes and immersive spaces in visionOS. Learn to fine-tune how volumes resize and respond to people moving around them. Make volumes and immersive spaces interact through the power of coordinate conversions. Find out how to make your app react when people adjust immersion with the Digital Crown, and use a surrounding effect to dynamically customize the passthrough tint in your immersive space experience.","type":"text"}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10085-Design-advanced-games-for-Apple-platforms":{"title":"Design advanced games for Apple platforms","abstract":[{"type":"text","text":"Learn how to adapt your high-end game so it feels at home on Mac, iPad, and iPhone. Weâ€™ll go over how to make your game look stunning on different displays, tailor your input and controls to be intuitive on each device, and take advantage of Apple technologies that deliver great player experiences."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10085-design-advanced-games-for-apple-platforms","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10085-Design-advanced-games-for-Apple-platforms","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10214-Squeeze-the-most-out-of-Apple-Pencil":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10214-Squeeze-the-most-out-of-Apple-Pencil","title":"Squeeze the most out of Apple Pencil","abstract":[{"type":"text","text":"New in iOS 18, iPadOS 18, and visionOS 2, the PencilKit tool picker gains the ability to have completely custom tools, with custom attributes. Learn how to express your custom drawing experience in the tool picker using the same great tool picking experience available across the system. Discover how to access the new features of the Apple Pencil Pro, including roll angle, the squeeze gesture, and haptic feedback."}],"kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc24-10214-squeeze-the-most-out-of-apple-pencil"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24":{"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC24","abstract":[{"type":"text","text":"Xcode 16, Swift 6, iOS 18, macOS 15, tvOS 18, visionOS 2, watchOS 11."},{"type":"text","text":" "},{"type":"text","text":"New APIs: Swift Testing, "},{"type":"codeVoice","code":"FinanceKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"TabletopKit"},{"type":"text","text":", and more."}],"url":"\/documentation\/wwdcnotes\/wwdc24","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24","role":"collectionGroup"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10066-Build-immersive-web-experiences-with-WebXR":{"title":"Build immersive web experiences with WebXR","abstract":[{"type":"text","text":"Discover how WebXR empowers you to add fully immersive experiences to your website in visionOS. Find out how to build WebXR experiences that take full advantage of the input capabilities of visionOS, and learn how you can use Simulator to test WebXR experiences on macOS."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10066-build-immersive-web-experiences-with-webxr","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10066-Build-immersive-web-experiences-with-WebXR","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10098-Design-Live-Activities-for-Apple-Watch":{"role":"sampleCode","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10098-design-live-activities-for-apple-watch","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10098-Design-Live-Activities-for-Apple-Watch","abstract":[{"type":"text","text":"Starting in watchOS 11, Live Activities from your iOS app will automatically appear in the Smart Stack on a connected Apple Watch. Learn how to optimize the layout of your Live Activity for the wrist, and provide the right level of information and interactivity at the right time."}],"title":"Design Live Activities for Apple Watch"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10211-Support-realtime-ML-inference-on-the-CPU":{"title":"Support real-time ML inference on the CPU","abstract":[{"type":"text","text":"Discover how you can use BNNSGraph to accelerate the execution of your machine learning model on the CPU. We will show you how to use BNNSGraph to compile and execute a machine learning model on the CPU and share how it provides real-time guarantees such as no runtime memory allocation and single-threaded running for audio or signal processing models."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10211-support-realtime-ml-inference-on-the-cpu","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10211-Support-realtime-ML-inference-on-the-CPU","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10184-A-Swift-Tour-Explore-Swifts-features-and-design":{"role":"sampleCode","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10184-a-swift-tour-explore-swifts-features-and-design","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10184-A-Swift-Tour-Explore-Swifts-features-and-design","abstract":[{"type":"text","text":"Learn the essential features and design philosophy of the Swift programming language. Weâ€™ll explore how to model data, handle errors, use protocols, write concurrent code, and more while building up a Swift package that has a library, an HTTP server, and a command line client. Whether youâ€™re just beginning your Swift journey or have been with us from the start, this talk will help you get the most out of the language."}],"title":"A Swift Tour: Explore Swiftâ€™s features and design"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10112-Say-hello-to-the-next-generation-of-CarPlay-design-system":{"title":"Say hello to the next generation of CarPlay design system","abstract":[{"text":"Explore the design system at the heart of the next generation of CarPlay that allows each automaker to express your vehicleâ€™s character and brand. Learn how gauges, layouts, dynamic content, and more are deeply customizable and adaptable, allowing you to express your own design philosophy and create an iconic, tailored look. This session is intended for automakers, system developers, and anyone designing a system that supports the next generation of CarPlay.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10112-say-hello-to-the-next-generation-of-carplay-design-system","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10112-Say-hello-to-the-next-generation-of-CarPlay-design-system","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10092-Render-Metal-with-passthrough-in-visionOS":{"url":"\/documentation\/wwdcnotes\/wwdc24-10092-render-metal-with-passthrough-in-visionos","abstract":[{"text":"Get ready to extend your Metal experiences for visionOS. Learn best practices for integrating your rendered content with peopleâ€™s physical environments with passthrough. Find out how to position rendered content to match the physical world, reduce latency with trackable anchor prediction, and more.","type":"text"}],"kind":"article","title":"Render Metal with passthrough in visionOS","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10092-Render-Metal-with-passthrough-in-visionOS","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10181-Xcode-essentials":{"title":"Xcode essentials","type":"topic","abstract":[{"type":"text","text":"Edit, debug, commit, repeat. Explore the suite of tools in Xcode that help you iterate quickly when developing apps. Discover tips and tricks to help optimize and boost your development workflow."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10181-xcode-essentials","kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10181-Xcode-essentials"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction":{"title":"Improve Core ML integration with async prediction","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","abstract":[{"type":"text","text":"Learn how to speed up machine learning features in your app with the latest Core ML execution engine improvements and find out how aggressive asset caching can help with inference and faster model loads. Weâ€™ll show you some of the latest options for async prediction and discuss considerations for balancing performance with overall memory usage to help you create a highly responsive app. Discover APIs to help you understand and maximize hardware utilization for your models."}],"url":"\/documentation\/wwdcnotes\/wwdc23-10049-improve-core-ml-integration-with-async-prediction","kind":"article","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10131-Support-semantic-search-with-Core-Spotlight":{"url":"\/documentation\/wwdcnotes\/wwdc24-10131-support-semantic-search-with-core-spotlight","abstract":[{"text":"Learn how to provide semantic search results in your app using Core Spotlight. Understand how to make your appâ€™s content available in the userâ€™s private, on-device index so people can search for items using natural language. Weâ€™ll also share how to optimize your appâ€™s performance by scheduling indexing activities.","type":"text"}],"kind":"article","title":"Support semantic search with Core Spotlight","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10131-Support-semantic-search-with-Core-Spotlight","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10173-Analyze-heap-memory":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10173-Analyze-heap-memory","role":"sampleCode","type":"topic","abstract":[{"type":"text","text":"Dive into the basis for your appâ€™s dynamic memory: the heap! Explore how to use Instruments and Xcode to measure, analyze, and fix common heap issues. Weâ€™ll also cover some techniques and best practices for diagnosing transient growth, persistent growth, and leaks in your app."}],"title":"Analyze heap memory","url":"\/documentation\/wwdcnotes\/wwdc24-10173-analyze-heap-memory"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10069-Broadcast-updates-to-your-Live-Activities":{"title":"Broadcast updates to your Live Activities","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10069-Broadcast-updates-to-your-Live-Activities","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10069-broadcast-updates-to-your-live-activities","kind":"article","role":"sampleCode","abstract":[{"text":"With broadcast push notifications, your app can send updates to thousands of Live Activities with a single request. Weâ€™ll discover how broadcast push notifications work between an app, a server, and the Apple Push Notification service, then weâ€™ll walk through best practices for this capability and how to implement it.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10103-Discover-RealityKit-APIs-for-iOS-macOS-and-visionOS":{"title":"Discover RealityKit APIs for iOS, macOS and visionOS","url":"\/documentation\/wwdcnotes\/wwdc24-10103-discover-realitykit-apis-for-ios-macos-and-visionos","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10103-Discover-RealityKit-APIs-for-iOS-macOS-and-visionOS","type":"topic","abstract":[{"type":"text","text":"Learn how new cross-platform APIs in RealityKit can help you build immersive apps for iOS, macOS, and visionOS. Check out the new hover effects, lights and shadows, and portal crossing features, and view them in action through real examples."}],"kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10133-Bring-your-app-to-Siri":{"title":"Bring your app to Siri","abstract":[{"text":"Learn how to use App Intents to expose your appâ€™s functionality to Siri. Understand which intents are already available for your use, and how to create custom intents to integrate actions from your app into the system. Weâ€™ll also cover what metadata to provide, making your entities searchable via Spotlight, annotating onscreen references, and much more.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10133-bring-your-app-to-siri","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10133-Bring-your-app-to-Siri","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10125-Streamline-signin-with-passkey-upgrades-and-credential-managers":{"title":"Streamline sign-in with passkey upgrades and credential managers","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10125-Streamline-signin-with-passkey-upgrades-and-credential-managers","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10125-streamline-signin-with-passkey-upgrades-and-credential-managers","kind":"article","role":"sampleCode","abstract":[{"text":"Learn how to automatically upgrade existing, password-based accounts to use passkeys. Weâ€™ll share why and how to improve account security and ease of sign-in, information about new features available for credential manager apps, and how to make your app information shine in the new Passwords app.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10217-Explore-Swift-performance":{"title":"Explore Swift performance","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10217-Explore-Swift-performance","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10217-explore-swift-performance","kind":"article","role":"sampleCode","abstract":[{"text":"Discover how Swift balances abstraction and performance. Learn what elements of performance to consider and how the Swift optimizer affects them. Explore the different features of Swift and how theyâ€™re implemented to further understand the tradeoffs available that can impact performance.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10186-Optimize-your-3D-assets-for-spatial-computing":{"abstract":[{"type":"text","text":"Dive into an end-to-end workflow for optimized 3D asset creation. Discover best practices for optimizing meshes, materials, and textures in your digital content creation tool. Learn how to harness shader graph, baking, and material instances to enhance your 3D scene while optimizing performance. Take advantage of native tools to work more effectively with your assets and improve your appâ€™s performance."}],"title":"Optimize your 3D assets for spatial computing","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10186-optimize-your-3d-assets-for-spatial-computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10186-Optimize-your-3D-assets-for-spatial-computing","role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression","kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10047-use-core-ml-tools-for-machine-learning-model-compression","type":"topic","title":"Use Core ML Tools for machine learning model compression","abstract":[{"type":"text","text":"Discover how to reduce the footprint of machine learning models in your app with Core ML Tools. Learn how to use techniques like palettization, pruning, and quantization to dramatically reduce model size while still achieving great accuracy. Explore comparisons between compression during the training stages and on fully trained models, and learn how compressed models can run even faster when your app takes full advantage of the Apple Neural Engine."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10138-Create-a-custom-data-store-with-SwiftData":{"url":"\/documentation\/wwdcnotes\/wwdc24-10138-create-a-custom-data-store-with-swiftdata","abstract":[{"text":"Combine the power of SwiftDataâ€™s expressive, declarative modeling API with your own persistence backend. Learn how to build a custom data store and explore how to progressively add persistence features in your app. To get the most out of this session, watch â€œMeet SwiftDataâ€ and â€œModel your schema with SwiftDataâ€ from WWDC23.","type":"text"}],"kind":"article","title":"Create a custom data store with SwiftData","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10138-Create-a-custom-data-store-with-SwiftData","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10093-Bring-your-iOS-or-iPadOS-game-to-visionOS":{"title":"Bring your iOS or iPadOS game to visionOS","url":"\/documentation\/wwdcnotes\/wwdc24-10093-bring-your-ios-or-ipados-game-to-visionos","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10093-Bring-your-iOS-or-iPadOS-game-to-visionOS","type":"topic","abstract":[{"type":"text","text":"Discover how to transform your iOS or iPadOS game into a uniquely visionOS experience. Increase the immersion (and fun factor!) with a 3D frame or an immersive background. And invite players further into your world by adding depth to the window with stereoscopy or head tracking."}],"kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10083-Get-started-with-HealthKit-in-visionOS":{"title":"Get started with HealthKit in visionOS","abstract":[{"type":"text","text":"Discover how to use HealthKit to create experiences that take full advantage of the spatial canvas. Learn the capabilities of HealthKit on the platform, find out how to bring an existing iPadOS app to visionOS, and explore the special considerations governing HealthKit during a Guest User session. Youâ€™ll also learn ways to use SwiftUI, Swift Charts, and Swift concurrency to craft innovative experiences with HealthKit."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10083-get-started-with-healthkit-in-visionos","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10083-Get-started-with-HealthKit-in-visionOS","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10223-Explore-machine-learning-on-Apple-platforms":{"title":"Explore machine learning on Apple platforms","abstract":[{"type":"text","text":"Get started with an overview of machine learning frameworks on Apple platforms. Whether youâ€™re implementing your first ML model, or an ML expert, weâ€™ll offer guidance to help you select the right framework for your appâ€™s needs."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10223-explore-machine-learning-on-apple-platforms","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10223-Explore-machine-learning-on-Apple-platforms","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10122-Use-CloudKit-Console-to-monitor-and-optimize-database-activity":{"title":"Use CloudKit Console to monitor and optimize database activity","abstract":[{"type":"text","text":"Discover the new observability features in CloudKit Console. Learn how to use Telemetry and Logging to troubleshoot and optimize your app. Find out how to set up alerts to monitor your applicationâ€™s behavior and notifications to stay on top of the container events that are most important to you."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10122-use-cloudkit-console-to-monitor-and-optimize-database-activity","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10122-Use-CloudKit-Console-to-monitor-and-optimize-database-activity","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10100-Create-enhanced-spatial-computing-experiences-with-ARKit":{"url":"\/documentation\/wwdcnotes\/wwdc24-10100-create-enhanced-spatial-computing-experiences-with-arkit","abstract":[{"text":"Learn how to create captivating immersive experiences with ARKitâ€™s latest features. Explore ways to use room tracking and object tracking to further engage with your surroundings. Weâ€™ll also share how your app can react to changes in your environmentâ€™s lighting on this platform. Discover improvements in hand tracking and plane detection which can make your spatial experiences more intuitive.","type":"text"}],"kind":"article","title":"Create enhanced spatial computing experiences with ARKit","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10100-Create-enhanced-spatial-computing-experiences-with-ARKit","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10084-Build-custom-swimming-workouts-with-WorkoutKit":{"kind":"article","title":"Build custom swimming workouts with WorkoutKit","abstract":[{"type":"text","text":"Check out the latest in creating, customizing, and scheduling workouts using WorkoutKit. Sprint through the latest in pace and power alerts and expanded support for distance goals. And keep the momentum going with the benefits of custom step names."}],"role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10084-build-custom-swimming-workouts-with-workoutkit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10084-Build-custom-swimming-workouts-with-WorkoutKit"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10073-Catch-up-on-accessibility-in-SwiftUI":{"title":"Catch up on accessibility in SwiftUI","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10073-Catch-up-on-accessibility-in-SwiftUI","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10073-catch-up-on-accessibility-in-swiftui","kind":"article","role":"sampleCode","abstract":[{"text":"SwiftUI makes it easy to build amazing experiences that are accessible to everyone. Weâ€™ll discover how assistive technologies understand and navigate your app through the rich accessibility elements provided by SwiftUI. Weâ€™ll also discuss how you can further customize these experiences by providing more information about your appâ€™s content and interactions by using accessibility modifiers.","type":"text"}]},"https://":{"identifier":"https:\/\/","url":"https:\/\/","type":"link","titleInlineContent":[{"type":"text","text":"Blog"}],"title":"Blog"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10070-Customize-feature-discovery-with-TipKit":{"title":"Customize feature discovery with TipKit","abstract":[{"text":"Focused on feature discovery, the TipKit framework makes it easy to display tips in your app. Now you can group tips so features are discovered in the ideal order, make tips reusable with custom tip identifiers, match the look and feel to your app, and sync tips using CloudKit. Learn how you can use the latest advances in TipKit to help people discover everything your app has to offer.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10070-customize-feature-discovery-with-tipkit","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10070-Customize-feature-discovery-with-TipKit","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-111801-Enhance-your-spatial-computing-app-with-RealityKit-audio":{"title":"Enhance your spatial computing app with RealityKit audio","abstract":[{"type":"text","text":"Elevate your spatial computing experience using RealityKit audio. Discover how spatial audio can make your 3D immersive experiences come to life. From ambient audio, reverb, to real-time procedural audio that can add character to your 3D content, learn how RealityKit audio APIs can help make your app more engaging."}],"url":"\/documentation\/wwdcnotes\/wwdc24-111801-enhance-your-spatial-computing-app-with-realitykit-audio","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-111801-Enhance-your-spatial-computing-app-with-RealityKit-audio","role":"sampleCode"},"https://developer.apple.com/wwdc24/10161":{"identifier":"https:\/\/developer.apple.com\/wwdc24\/10161","url":"https:\/\/developer.apple.com\/wwdc24\/10161","type":"download","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10086-Design-great-visionOS-apps":{"title":"Design great visionOS apps","abstract":[{"type":"text","text":"Find out how to create compelling spatial computing apps by embracing immersion, designing for eyes and hands, and taking advantage of depth, scale, and space. Weâ€™ll share several examples of great visionOS apps and explore how their designers approached creating new experiences for the platform."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10086-design-great-visionos-apps","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10086-Design-great-visionOS-apps","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10171-Demystify-explicitly-built-modules":{"title":"Demystify explicitly built modules","abstract":[{"type":"text","text":"Explore how builds are changing in Xcode 16 with explicitly built modules. Discover how modules are used to build your code, how explicitly built modules improve transparency in compilation tasks, and how you can optimize your build by sharing modules across targets."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10171-demystify-explicitly-built-modules","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10171-Demystify-explicitly-built-modules","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10101-Explore-object-tracking-for-visionOS":{"title":"Explore object tracking for visionOS","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10101-Explore-object-tracking-for-visionOS","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10101-explore-object-tracking-for-visionos","kind":"article","role":"sampleCode","abstract":[{"text":"Find out how you can use object tracking to turn real-world objects into virtual anchors in your visionOS app. Learn how you can build spatial experiences with object tracking from start to finish. Find out how to create a reference object using machine learning in Create ML and attach content relative to your target object in Reality Composer Pro, RealityKit or ARKit APIs.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10195-Go-further-with-Swift-Testing":{"title":"Go further with Swift Testing","abstract":[{"text":"Learn how to write a sweet set of (test) suites using Swift Testingâ€™s baked-in features. Discover how to take the building blocks further and use them to help expand tests to cover more scenarios, organize your tests across different suites, and optimize your tests to run in parallel.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10195-go-further-with-swift-testing","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10195-Go-further-with-Swift-Testing","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10102-Compose-interactive-3D-content-in-Reality-Composer-Pro":{"title":"Compose interactive 3D content in Reality Composer Pro","abstract":[{"type":"text","text":"Discover how the Timeline view in Reality Composer Pro can bring your 3D content to life. Learn how to create an animated story in which characters and objects interact with each other and the world around them using inverse kinematics, blend shapes, and skeletal poses. Weâ€™ll also show you how to use built-in and custom actions, sequence your actions, apply triggers, and implement natural movements."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10102-compose-interactive-3d-content-in-reality-composer-pro","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10102-Compose-interactive-3D-content-in-Reality-Composer-Pro","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10146-Demystify-SwiftUI-containers":{"title":"Demystify SwiftUI containers","abstract":[{"text":"Learn about the capabilities of SwiftUI container views and build a mental model for how subviews are managed by their containers. Leverage new APIs to build your own custom containers, create modifiers to customize container content, and give your containers that extra polish that helps your apps stand out.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10146-demystify-swiftui-containers","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10146-Demystify-SwiftUI-containers","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10113-Discover-media-performance-metrics-in-AVFoundation":{"title":"Discover media performance metrics in AVFoundation","abstract":[{"text":"Discover how you can monitor, analyze, and improve user experience with the new media performance APIs. Explore how to monitor AVPlayer performance for HLS assets using different AVMetricEvents, and learn how to use these metrics to understand and triage player performance issues.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10113-discover-media-performance-metrics-in-avfoundation","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10113-Discover-media-performance-metrics-in-AVFoundation","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10087-Create-custom-environments-for-your-immersive-apps-in-visionOS":{"url":"\/documentation\/wwdcnotes\/wwdc24-10087-create-custom-environments-for-your-immersive-apps-in-visionos","abstract":[{"text":"Discover how to create visually rich and performant customized app environments for Apple Vision Pro. Learn design guidelines, get expert recommendations, and explore techniques you can use in any digital content creation tool to begin building your immersive environment.","type":"text"}],"kind":"article","title":"Create custom environments for your immersive apps in visionOS","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10087-Create-custom-environments-for-your-immersive-apps-in-visionOS","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10210-Bring-your-apps-core-features-to-users-with-App-Intents":{"abstract":[{"text":"Learn the principles of the App Intents framework, like intents, entities, and queries, and how you can harness them to expose your appâ€™s most important functionality right where people need it most. Find out how to build deep integration between your app and the many system features built on top of App Intents, including Siri, controls and widgets, Apple Pencil, Shortcuts, the Action button, and more. Get tips on how to build your App Intents integrations efficiently to create the best experiences in every surface while still sharing code and core functionality.","type":"text"}],"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10210-Bring-your-apps-core-features-to-users-with-App-Intents","kind":"article","type":"topic","title":"Bring your appâ€™s core features to users with App Intents","url":"\/documentation\/wwdcnotes\/wwdc24-10210-bring-your-apps-core-features-to-users-with-app-intents"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10159-Bring-your-machine-learning-and-AI-models-to-Apple-silicon":{"abstract":[{"type":"text","text":"Learn how to optimize your machine learning and AI models to leverage the power of Apple silicon. Review model conversion workflows to prepare your models for on-device deployment. Understand model compression techniques that are compatible with Apple silicon, and at what stages in your model deployment workflow you can apply them. Weâ€™ll also explore the tradeoffs between storage size, latency, power usage and accuracy."}],"title":"Bring your machine learning and AI models to Apple silicon","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10159-bring-your-machine-learning-and-ai-models-to-apple-silicon","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10159-Bring-your-machine-learning-and-AI-models-to-Apple-silicon","role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10207-Migrate-your-TVML-app-to-SwiftUI":{"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10207-migrate-your-tvml-app-to-swiftui","abstract":[{"type":"text","text":"SwiftUI helps you build great apps on all Apple platforms and is the preferred toolkit for bringing your content into the living room with tvOS 18. Learn how to use SwiftUI to create familiar layouts and controls from TVMLKit, and get tips and best practices."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10207-Migrate-your-TVML-app-to-SwiftUI","type":"topic","title":"Migrate your TVML app to SwiftUI","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10157-Extend-your-apps-controls-across-the-system":{"title":"Extend your appâ€™s controls across the system","abstract":[{"type":"text","text":"Bring your appâ€™s controls to Control Center, the Lock Screen, and beyond. Learn how you can use WidgetKit to extend your appâ€™s controls to the system experience. Weâ€™ll cover how you can to build a control, tailor its appearance, and make it configurable."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10157-extend-your-apps-controls-across-the-system","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10157-Extend-your-apps-controls-across-the-system","role":"sampleCode"},"https://avatars.githubusercontent.com/u/20319411?v=4":{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/20319411?v=4","type":"image","alt":"Profile image of Ramit Sharma","variants":[{"traits":["1x","light"],"url":"https:\/\/avatars.githubusercontent.com\/u\/20319411?v=4"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10170-Consume-noncopyable-types-in-Swift":{"title":"Consume noncopyable types in Swift","abstract":[{"text":"Get started with noncopyable types in Swift. Discover what copying means in Swift, when you might want to use a noncopyable type, and how value ownership lets you state your intentions clearly.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10170-consume-noncopyable-types-in-swift","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10170-Consume-noncopyable-types-in-Swift","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","url":"\/documentation\/wwdcnotes","role":"collection","title":"WWDC Notes","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10200-Extend-your-Xcode-Cloud-workflows":{"title":"Extend your Xcode Cloud workflows","abstract":[{"text":"Discover how Xcode Cloud can adapt to your development needs. Weâ€™ll show you how to streamline your workflows, automate testing and distribution with start conditions, custom aliases, custom scripts, webhooks, and the App Store Connect API.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10200-extend-your-xcode-cloud-workflows","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10200-Extend-your-Xcode-Cloud-workflows","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10150-SwiftUI-essentials":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10150-SwiftUI-essentials","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10150-swiftui-essentials","abstract":[{"text":"Join us on a tour of SwiftUI, Appleâ€™s declarative user interface framework. Learn essential concepts for building apps in SwiftUI, like views, state variables, and layout. Discover the breadth of APIs for building fully featured experiences and crafting unique custom components. Whether youâ€™re brand new to SwiftUI or an experienced developer, youâ€™ll learn how to take advantage of what SwiftUI has to offer when building great apps.","type":"text"}],"role":"sampleCode","title":"SwiftUI essentials"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10163-Discover-Swift-enhancements-in-the-Vision-framework":{"role":"sampleCode","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10163-discover-swift-enhancements-in-the-vision-framework","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10163-Discover-Swift-enhancements-in-the-Vision-framework","abstract":[{"type":"text","text":"The Vision Framework API has been redesigned to leverage modern Swift features like concurrency, making it easier and faster to integrate a wide array of Vision algorithms into your app. Weâ€™ll tour the updated API and share sample code, along with best practices, to help you get the benefits of this framework with less coding effort. Weâ€™ll also demonstrate two new features: image aesthetics and holistic body pose."}],"title":"Discover Swift enhancements in the Vision framework"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10115-Enhance-the-immersion-of-media-viewing-in-custom-environments":{"abstract":[{"type":"text","text":"Extend your media viewing experience using Reality Composer Pro components like Docking Region, Reverb, and Virtual Environment Probe. Find out how to further enhance immersion using Reflections, Tint Surroundings Effect, SharePlay, and the Immersive Environment Picker."}],"title":"Enhance the immersion of media viewing in custom environments","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10115-enhance-the-immersion-of-media-viewing-in-custom-environments","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10115-Enhance-the-immersion-of-media-viewing-in-custom-environments","role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10067-Bring-context-to-todays-weather":{"title":"Bring context to todayâ€™s weather","abstract":[{"type":"text","text":"Harness the power of WeatherKit to get detailed weather forecast data such as precipitation amounts by type, cloud cover by altitude, or maximum wind speed. Find out how you can summarize weather by different parts of the day and highlight significant upcoming changes to temperature or precipitation. Understand how you can compare current weather to the past through our Historical Comparisons dataset and dive into historical weather statistics for any location in the world. Weâ€™ll also explore how you can do all of this faster with our Swift and REST APIs."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10067-bring-context-to-todays-weather","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10067-Bring-context-to-todays-weather","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10075-Track-model-changes-with-SwiftData-history":{"url":"\/documentation\/wwdcnotes\/wwdc24-10075-track-model-changes-with-swiftdata-history","abstract":[{"text":"Reveal the history of your modelâ€™s changes with SwiftData! Use the history API to understand when data store changes occurred, and learn how to use this information to build features like remote server sync and out-of-process change handing in your app. Weâ€™ll also cover how you can build support for the history API into a custom data store.","type":"text"}],"kind":"article","title":"Track model changes with SwiftData history","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10075-Track-model-changes-with-SwiftData-history","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10176-Design-App-Intents-for-system-experiences":{"title":"Design App Intents for system experiences","type":"topic","abstract":[{"type":"text","text":"App Intents power system experiences in controls, Spotlight, Siri, and more. Find out how to identify the functionality thatâ€™s best for App Intents, and how to use parameters to make these intents flexible. Learn how to use App Intents to allow people to take action outside your app, and see examples of when to navigate into your app to show contextual information."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10176-design-app-intents-for-system-experiences","kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10176-Design-App-Intents-for-system-experiences"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10152-Create-custom-hover-effects-in-visionOS":{"abstract":[{"type":"text","text":"Learn how to develop custom hover effects that update views when people look at them. Find out how to build an expanding button effect that combines opacity, scale, and clip effects. Discover best practices for creating effects that are comfortable and respect peopleâ€™s accessibility needs."}],"title":"Create custom hover effects in visionOS","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10152-create-custom-hover-effects-in-visionos","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10152-Create-custom-hover-effects-in-visionOS","role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10151-Create-custom-visual-effects-with-SwiftUI":{"title":"Create custom visual effects with SwiftUI","abstract":[{"type":"text","text":"Discover how to create stunning visual effects in SwiftUI. Learn to build unique scroll effects, rich color treatments, and custom transitions. Weâ€™ll also explore advanced graphic effects using Metal shaders and custom text rendering."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10151-create-custom-visual-effects-with-swiftui","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10151-Create-custom-visual-effects-with-SwiftUI","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10209-Enhanced-suggestions-for-your-journaling-app":{"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10209-Enhanced-suggestions-for-your-journaling-app","url":"\/documentation\/wwdcnotes\/wwdc24-10209-enhanced-suggestions-for-your-journaling-app","kind":"article","title":"Enhanced suggestions for your journaling app","abstract":[{"text":"Find out how your journaling app can display journaling suggestions with richer content from the system. Explore new types of available content like state of mind data, reflection prompts, and support for third-party media content and motion-based activities.","type":"text"}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10160-Train-your-machine-learning-and-AI-models-on-Apple-GPUs":{"role":"sampleCode","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10160-train-your-machine-learning-and-ai-models-on-apple-gpus","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10160-Train-your-machine-learning-and-AI-models-on-Apple-GPUs","abstract":[{"type":"text","text":"Learn how to train your models on Apple Silicon with Metal for PyTorch, JAX and TensorFlow. Take advantage of new attention operations and quantization support for improved transformer model performance on your devices."}],"title":"Train your machine learning and AI models on Apple GPUs"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10162-Keep-colors-consistent-across-captures":{"title":"Keep colors consistent across captures","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10162-Keep-colors-consistent-across-captures","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10162-keep-colors-consistent-across-captures","kind":"article","role":"sampleCode","abstract":[{"text":"Meet the Constant Color API and find out how it can help people use your app to determine precise colors. Youâ€™ll learn how to adopt the API, explore its scientific and marketing potential, and discover best practices for making the most of the technology.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10062-Explore-App-Store-server-APIs-for-InApp-Purchase":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10062-explore-app-store-server-apis-for-inapp-purchase","abstract":[{"text":"Learn how to leverage your server to build great In-App Purchase experiences with the latest updates to the App Store Server API, App Store Server Notifications, and the open source App Store Server Library. After a recap of current APIs, weâ€™ll introduce updated endpoint functionality, new transaction fields, and a new notification type. Weâ€™ll also discuss best practices for the purchase lifecycle, delivering content, and targeting offers, so you can become a server power user.","type":"text"}],"title":"Explore App Store server APIs for In-App Purchase","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10062-Explore-App-Store-server-APIs-for-InApp-Purchase"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10132-Evolve-your-document-launch-experience":{"title":"Evolve your document launch experience","abstract":[{"type":"text","text":"Make your document-based app stand out, and bring its unique identity into focus with the new document launch experience. Learn how to leverage the new API to customize the first screen people see when they launch your app. Utilize the new system-provided design, and amend it with custom actions, delightful decorative views, and impressive animations."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10132-evolve-your-document-launch-experience","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10132-Evolve-your-document-launch-experience","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10198-Run-Break-Inspect-Explore-effective-debugging-in-LLDB":{"title":"Run, Break, Inspect: Explore effective debugging in LLDB","type":"topic","abstract":[{"type":"text","text":"Learn how to use LLDB to explore and debug codebases. Weâ€™ll show you how to make the most of crashlogs and backtraces, and how to supercharge breakpoints with actions and complex stop conditions. Weâ€™ll also explore how the â€œpâ€ command and the latest features in Swift 6 can enhance your debugging experience."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10198-run-break-inspect-explore-effective-debugging-in-lldb","kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10198-Run-Break-Inspect-Explore-effective-debugging-in-LLDB"},"doc://WWDCNotes/documentation/WWDCNotes/RamitSharma991":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/RamitSharma991","role":"sampleCode","title":"Ramit Sharma (14 notes)","url":"\/documentation\/wwdcnotes\/ramitsharma991","abstract":[{"text":"Indie iOS Dev. Swift, SwiftUI, Obj-C, UX and related.","type":"text"}],"type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10088-Capture-HDR-content-with-ScreenCaptureKit":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10088-Capture-HDR-content-with-ScreenCaptureKit","title":"Capture HDR content with ScreenCaptureKit","abstract":[{"type":"text","text":"Learn how to capture high dynamic colors using ScreenCaptureKit, and explore new features like HDR support, microphone capture, and straight-to-file recording."}],"kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc24-10088-capture-hdr-content-with-screencapturekit"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10172-Break-into-the-RealityKit-debugger":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10172-break-into-the-realitykit-debugger","abstract":[{"text":"Meet the RealityKit debugger and discover how this new tool lets you inspect the entity hierarchy of spatial apps, debug rogue transformations, find missing entities, and detect which parts of your code are causing problems for your systems.","type":"text"}],"title":"Break into the RealityKit debugger","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10172-Break-into-the-RealityKit-debugger"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10220-Bring-expression-to-your-app-with-Genmoji":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10220-bring-expression-to-your-app-with-genmoji","abstract":[{"text":"Discover how to bring Genmoji to life in your app. Weâ€™ll go over how to render, store, and communicate text that includes Genmoji. If your app features a custom text engine, weâ€™ll also cover techniques for adding support for Genmoji.","type":"text"}],"title":"Bring expression to your app with Genmoji","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10220-Bring-expression-to-your-app-with-Genmoji"},"https://github.com/RamitSharma991":{"identifier":"https:\/\/github.com\/RamitSharma991","url":"https:\/\/github.com\/RamitSharma991","type":"link","titleInlineContent":[{"type":"text","text":"GitHub"}],"title":"GitHub"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10089-Port-advanced-games-to-Apple-platforms":{"title":"Port advanced games to Apple platforms","abstract":[{"text":"Discover how simple it can be to reach players on Apple platforms worldwide. Weâ€™ll show you how to evaluate your Windows executable on Apple silicon, start your game port with code samples, convert your shader code to Metal, and bring your game to Mac, iPhone, and iPad. Explore enhanced Metal tools that understand HLSL shaders to validate, debug, and profile your ported shaders on Metal.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc24-10089-port-advanced-games-to-apple-platforms","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10089-Port-advanced-games-to-Apple-platforms","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10201-Customize-spatial-Persona-templates-in-SharePlay":{"url":"\/documentation\/wwdcnotes\/wwdc24-10201-customize-spatial-persona-templates-in-shareplay","abstract":[{"text":"Learn how to use custom spatial Persona templates in your visionOS SharePlay experience to fine-tune the placement of Personas relative to your app. Weâ€™ll show you how to adopt custom spatial Persona templates in a sample app with SharePlay, move participants between seats, and test your changes in Simulator. Weâ€™ll also share best practices for designing custom spatial templates that will make your experience shine.","type":"text"}],"kind":"article","title":"Customize spatial Persona templates in SharePlay","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10201-Customize-spatial-Persona-templates-in-SharePlay","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10116-Explore-multiview-video-playback-in-visionOS":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10116-explore-multiview-video-playback-in-visionos","abstract":[{"text":"Learn how AVExperienceController can enable playback of multiple videos on Apple Vision Pro. Review best practices for adoption and explore great use cases, like viewing a sports broadcast from different angles or watching multiple games simultaneously. And discover how to design a compelling and intuitive multiview experience in your app.","type":"text"}],"title":"Explore multiview video playback in visionOS","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10116-Explore-multiview-video-playback-in-visionOS"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10114-Enhance-ad-experiences-with-HLS-interstitials":{"title":"Enhance ad experiences with HLS interstitials","abstract":[{"type":"text","text":"Explore how HLS Interstitials can help you seamlessly insert advertisements into your HLS content. Weâ€™ll also show you how to use integrated timeline to tune your UI experience and build SharePlay for interstitials."}],"url":"\/documentation\/wwdcnotes\/wwdc24-10114-enhance-ad-experiences-with-hls-interstitials","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10114-Enhance-ad-experiences-with-HLS-interstitials","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10185-Build-multilingualready-apps":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10185-Build-multilingualready-apps","role":"sampleCode","type":"topic","abstract":[{"type":"text","text":"Ensure your app works properly and effectively for multilingual users. Learn best practices for text input, display, search, and formatting. Get details on typing in multiple languages without switching between keyboards. And find out how the latest advances in the String Catalog can make localization even easier."}],"title":"Build multilingual-ready apps","url":"\/documentation\/wwdcnotes\/wwdc24-10185-build-multilingualready-apps"}}}