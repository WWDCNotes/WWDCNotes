{"schemaVersion":{"major":0,"minor":3,"patch":0},"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10048-Whats-new-in-VisionKit"},"sections":[],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10048-whats-new-in-visionkit"]}],"metadata":{"title":"What’s new in VisionKit","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC23","role":"sampleCode"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"abstract":[{"text":"Discover how VisionKit can help people quickly lift subjects from images in your app and learn more about the content of an image with Visual Look Up. We’ll also take a tour of the latest updates to VisionKit for Live Text interaction, data scanning, and expanded support for macOS apps.","type":"text"}],"primaryContentSections":[{"content":[{"text":"Overview","level":2,"type":"heading","anchor":"overview"},{"inlineContent":[{"type":"text","text":"To recap, last year Live Text support was added in VisionKit, enabling interactions such as text selection, translation, QR support, and more for images in your apps. VisionKit also introduced the DataScannerViewController, The Data scanner uses a live camera feed provide a simple and full featured way to capture specific text types, as well as many variants of machine readable codes. Information on those APIs are included in these WWDC22 sessions."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-recap2022"}],"type":"paragraph"},{"inlineContent":[{"text":"You can get the links to the talks below at the end of this post.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This year VisionKit is adding support for:"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Subject lifting"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Visual Look Up"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Data Scanner and Live Text. A new Live Text API for text selection.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Expanded platform support for Catalyst, and context menu integration for native macOS apps.","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"text":"Subject Lifting","level":1,"type":"heading","anchor":"Subject-Lifting"},{"inlineContent":[{"identifier":"WWDC23-10048-subjectLifting","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"With a simple long press on the subject of an image, it lifts it from its surroundings and becomes highlighted with this beautiful animated glow, and then I am presented with several options to share it, or invoke Visual Look Up.","type":"text"}],"type":"paragraph"},{"text":"New for iOS 17","level":2,"type":"heading","anchor":"New-for-iOS-17"},{"inlineContent":[{"identifier":"WWDC23-10048-sticker1","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"You can now use any lifted subject to create a sticker, with fun effects such as shiny, puffy, and more to share with your friends and family."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Now the good news is, integrating Subject Lifting is very simple."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Here is the same code snippet from the last year’s video, it supports Subject Lifting, without any code changed."}],"type":"paragraph"},{"code":["func analyzeCurrentImage() {","","    if let image = image {","        Task {","            do {","                let configuration =ImageAnalyzer.Configuration([.text, .machineReadableCode])","                let analysis = try awaitanalyzer.analyze(image,configuration:configuration)","                if image == self.image {","                    interaction.analysis =analysis;","                    interactio.preferredInteractionypes = [.automatic]","                }","            }","            catch {","                \/\/ Handle error...","            }","        }","    }","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"Let’s explore further. Notice that I’m not passing anything special into the analyzer configuration. This is because in order to preserve power and performance, Subject Lifting analysis is handled separately by the interaction after the initial analysis is complete. For iOS this process occurs after it’s been on screen for a few seconds, and for macOS it will occur the first time the menu appears.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"This means you don’t need to handle the case of the user swiping though many photos. The interaction will handle this for you. All you need to do is ensure you have an appropriate interaction type set– in this case, automatic– and the rest is handled by the interaction.","type":"text"}],"type":"paragraph"},{"text":"Subject lifting interaction types","level":1,"type":"heading","anchor":"Subject-lifting-interaction-types"},{"inlineContent":[{"text":"Let’s examine Subject Lifting compatible interaction types a bit closer. Automatic gives the default out of the box experience, combining text interaction, Subject Lifting, and more.","type":"text"},{"text":"\n","type":"text"},{"text":"If you only want Subject Lifting, and not text selection or data detectors, you can set the interaction type to .imageSegmentation, or combine it with other types.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"And finally, if Subject Lifting just does not make sense for your app, but you want the previous automatic behavior from iOS 16, no problem, you can use a new type, .automaticTextOnly. This provides features such as text selection and data detectors, but not Subject Lifting."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-interactionTypes","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"We have a detailed session specifically on Subject Lifting available if you would like to learn advanced topics about this amazing new technology in both VisionKit, and Vision.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"Lift subjects from images in your app","type":"text"}],"type":"paragraph"},{"text":"Visual Look Up","level":1,"type":"heading","anchor":"Visual-Look-Up"},{"inlineContent":[{"text":"This year VisionKit also supports Visual Look Up. Visual Look Up allows users to easily identify and learn about pets, nature, landmarks, art, and media.","type":"text"},{"text":"\n","type":"text"},{"text":"And In iOS 17, Visual Look Up will support additional domains, including food, products, and signs and symbols.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-visualLookUp","type":"image"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-visualLookUp2","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Now, finally, it’s easy to look up what those symbols on your laundry tags mean. I mean, that’s pretty cool!"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Visual Look Up availability is based on language, and is available for these languages:"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"English"}]}]},{"content":[{"inlineContent":[{"text":"Italian","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"French"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Japanese","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"German","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Spanish","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"Let’s take a quick peek under the hood, and explore how Visual Look Up works. It’s actually a two-part process. Initial processing is accomplished entirely on device at analysis time."},{"type":"text","text":"\n"},{"type":"text","text":"If the .visualLookUp type is present in the analyzer configuration, Visual Look Up will locate the bounding box of the results, and their top level domain. For example, if it’s a cat, book, or plant. This step also includes feature extraction."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-visualLookUp3","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Once the user requests to look up an object, then, and only then, are the domain and image embeddings from feature extraction sent to the server for additional processing."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-visualLookUp4"}],"type":"paragraph"},{"inlineContent":[{"text":"Now you know how Visual Look Up works, let’s quickly explore how to use it, and what actions you need to take to add it to your app. Visual Look Up can be invoked in two different ways. The first is in conjunction with Subject Lifting, if the current lifted subject contains one, and only one, correlated Visual Look Up result, the Look Up option will be offered in the menu,","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-selection","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"and selecting it will show the full Look Up result.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-selection2","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"VisionKit handles this interaction for you automatically. As an adopter all you need to do is have ","type":"text"},{"code":".visualLookUp","type":"codeVoice"},{"text":" added to your analyzer configuration at analysis time.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Second, there is a modal interaction available where badges are placed over each of the visual search results. Notice how the badges move to the corner if they leave the viewport, Users can tap on these badges to show the Look Up result. This is the same interaction as clicking on the info button in the Photos app, or Quick Look, for example."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-badges","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This mode is invoked by setting "},{"type":"codeVoice","code":".visualLookUp"},{"type":"text","text":" as the "},{"type":"codeVoice","code":"preferredInteractionType"},{"type":"text","text":" on your interaction."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-badges2","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Please note: this type will have precedence over the other interaction types. For example, you cannot select text or data detectors at the same time that the visualLookup Mode is set. As such, this is normally used in conjunction with a button, or some other bespoke way to get in and out of this mode. For example, Quick Look uses the info button to enter Visual Look Up mode.","type":"text"}],"type":"paragraph"},{"text":"Data Scanner and Live Text","level":1,"type":"heading","anchor":"Data-Scanner-and-Live-Text"},{"inlineContent":[{"type":"text","text":"Introduced In iOS 16, the DataScannerViewController was designed to be the easiest way to use OCR with a live camera viewfinder."}],"type":"paragraph"},{"inlineContent":[{"text":"New in iOS 17, it’s been enhanced with:","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"optical flow tracking"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"currency support","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"text":"Optical flow tracking can enhance text tracking for live-camera experiences.","type":"text"},{"text":"\n","type":"text"},{"text":"In iOS 16 I’m scanning for text with highFrameRateTracking enabled.","type":"text"},{"text":"\n","type":"text"},{"text":"But new in iOS17 with optical flow tracking the highlights feel much more stable and grounded than before.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-tracking","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Optical flow tracking comes for free whenever you use the DataScannerViewController, however, it is only available when recognizing text, and not machine readable codes. And you’re also required to scan for text without a specific text content type set. And finally, once again, ensure high frame-rate tracking is enabled. Which is, conveniently, the default."}],"type":"paragraph"},{"code":["\/\/ Recognize URLs","let recognizedDataTypes: Set<DataScannerViewController.RecognizedDataType> = [","    .text(textContentType: .URL)","]","","\/\/ Present the data scanner configured without high frame-rate tracking","let dataScanner = DataScannerViewController(recognizedDataTypes: recognizedDataTypes,","                                            isHighFrameRateTrackingEnabled: false)","present(dataScanner, animated: true) {","    try? dataScanner.startScanning()","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"No matter how you configure it, the data scanner provides great text tracking; but if your use case allows for this configuration, the new optical flow tracking can enhance it even further.","type":"text"}],"type":"paragraph"},{"text":"Currency","level":1,"type":"heading","anchor":"Currency"},{"inlineContent":[{"type":"text","text":"Next, the data scanner has a new option allowing users to find and interact with monetary values. It’s incredibly simple to enable. Just set the text content type to currency when specifying text recognition in the data scanner’s initializer, just as you would other content types like email addresses or telephone numbers."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-currency","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Now I’m going to explore this new type in more detail with a quick example. When the data scanner recognizes currency in text, It contains both a bounds and a transcript.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-transcript","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The transcript has both the currency symbol and the amount."}],"type":"paragraph"},{"code":["\/\/ Print the total of values in the current locale's currency","let formatter = NumberFormatter()","formatter.numberStyle = .currency","formatter.locale = Locale.current","guard let currencySymbol = formatter.currencySymbol else { return }","","var total: Double = 0.0","for await allItems: [RecognizedItem] in dataScannerViewController.recognizedItems {","    for recognizedItem in allItems {","        if case .text(let text) = recognizedItem {","            let transcript = text.transcript","            if transcript.contains (currencySymbol),","               let value = formatter.number(from: transcript) {","                total = total + value.doubleValue","            }","        }","    }","    print(\"total: \\(total)\")","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"type":"text","text":"Here’s an example where I find the total of all the values on something like a receipt. First, I get the currency symbol using the current locale."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"While awaiting the data scanner’s results in the recognizedItems stream, I can loop through each of the recognized items and grab its transcript. If the transcript contains the currency symbol I’m interested in, I’ll go ahead and update the total value. And just like that, now you’ll have the sum of all values. This is just a simple example, but this can very powerful."}],"type":"paragraph"},{"text":"Live Text enhancements","level":1,"type":"heading","anchor":"Live-Text-enhancements"},{"inlineContent":[{"text":"First off, Live Text is coming to more regions by expanding our supported languages to include Thai and Vietnamese.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-languages","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Live Text includes enhancements for document structure detection as well this year. As an example, in iOS 16 Live Text supported list detection. This allows you to easily copy and paste a list into an app that understands lists, such as Notes, and the list formatting will be maintained."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-listDetection","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Live Text handles several list styles, such as numbers or bullets."}],"type":"paragraph"},{"inlineContent":[{"text":"And now, Live Text is offering the same support for Tables, making it far easier to get structured table data from an image into applications like Notes or Numbers. Now I can select, copy, and paste this table into Numbers, and the structure is maintained. Notice how it merges cells automatically if necessary.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-tables"}],"type":"paragraph"},{"text":"Context Aware Data Detectors","level":2,"type":"heading","anchor":"Context-Aware-Data-Detectors"},{"inlineContent":[{"type":"text","text":"We are also adding Context Aware Data Detectors in Live Text. For this feature, data detectors and their visual relationships are used when adding contacts. Notice how when I add this contact from an email address, additional information from surrounding data detectors are now included, allowing me to easily add all this information at once. Adding a contact from a business card or flyer has never been easier."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-contacts","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-contacts2"}],"type":"paragraph"},{"inlineContent":[{"text":"In addition to these great features you are also getting for free, VisionKit also has some new APIs specifically for text. Last year, you could get the entire text contents by accessing the transcript property on the image analysis.","type":"text"}],"type":"paragraph"},{"code":["ImageAnalysis","    .transcript"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"You now have full access to plain and attributed text, selected ranges, and easy access to selected text. There is also a new delegate method so you can be aware when the text selection changes and update your UI as appropriate.","type":"text"}],"type":"paragraph"},{"code":["ImageAnalysisInteraction","    .text","    .selectedText ","    .selectedRanges ","    .selectedAttributedText","    ","ImageAnalysisInteractionDelegate","    textSelectionDidChange(_ interaction: ImageAnalysisInteraction)"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"It is now easy to add features that rely on what the user has selected. For example, using the menu builder API, you could insert a menu item that creates a reminder based on the current text selection.","type":"text"}],"type":"paragraph"},{"code":["override func buildMenu(with builder: UIMenuBuilder) {","    ","    let text = interaction.selectedText","    if text.length > 0, builder.system == UIMenuSystem.context {","        let command = UICommand(title:\"Create Reminder\", action:","                                    #selector(handleCreateReminder))","        let menu = UIMenu(options: .displayInline, children: [command])","        builder.insertSibling(menu, afterMenu: .share)","    }","    ","    super.buildMenu(with: builder)","}","@obic func handleCreateReminder() {","    createReminder(interaction.selectedText)","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"type":"text","text":"Start in the view controller that owns your image analysis interaction. First grab the selected text, and ensure it isn’t empty. Then create a command that calls our handler when chosen, Now create a menu object that holds the command. And finally, insert that menu as a sibling after the share menu option. Now you have a custom menu alongside system items like copy and share."}],"type":"paragraph"},{"text":"Expanded platform support","level":2,"type":"heading","anchor":"Expanded-platform-support"},{"inlineContent":[{"type":"text","text":"And this year, it’s all about the Mac. We are rolling out Catalyst support to easily bring Live Text from your iOS apps over to the Mac. And if you’re new to the native macOS API and the ImageAnalysisOverlayView, stay tuned, because I’m going to go over some specifics, as well as some tips on adopting them."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Finally, I am going to talk about a new system for menus, offering simple and seamless integration of VisionKit into your contextual menus."}],"type":"paragraph"},{"text":"Catalyst","level":3,"type":"heading","anchor":"Catalyst"},{"inlineContent":[{"type":"text","text":"Catalyst adoption is very straightforward. It should be a simple recompile to get the image analysis interaction working in Catalyst. We support, Live Text, Subject Lifting and Visual Look Up, but unfortunately QR code support is unavailable in either the Catalyst environment or native macOS API for VisionKit. However, I wanted to let you know that if you have a shared implementation, leaving the "},{"code":".machineReadableCodes","type":"codeVoice"},{"type":"text","text":" in your analyzer configuration for Catalyst is perfectly safe, and just becomes a no-op. Also, please note that QR detection support is available in the Vision Framework if you need this functionality on the Mac."}],"type":"paragraph"},{"inlineContent":[{"text":"Please see also: ","type":"text"},{"identifier":"https:\/\/developer.apple.com\/wwdc21\/10041","type":"reference","isActive":true}],"type":"paragraph"},{"text":"The native macOS API","level":3,"type":"heading","anchor":"The-native-macOS-API"},{"inlineContent":[{"type":"text","text":"As with iOS, there are two major classes you need to be aware of when adopting VisionKit:"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"The ImageAnalyzer"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The ImageAnalysisOverlayView.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"First, the easy part. The Image Analyzer and analysis process for the Mac is identical to iOS. With the exception of machine readable codes being a no-op, as I mentioned earlier, everything is the same and is used in the same way. The main difference between the iOS ImageAnalysisInteraction, and macOS’s ImageAnalysisOverlayView is how the interaction is added to your application."}],"type":"paragraph"},{"inlineContent":[{"text":"For iOS, the ImageAnalysisInteraction is a UIInteraction that is added to a view, already existing in your apps view hierarchy.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-ImageAnalysisInteraction"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"But UIInteraction does not exist on the Mac. So what do you do? In this case, as the name suggests, the "},{"type":"codeVoice","code":"ImageAnalysisOverlayView"},{"type":"text","text":" is a subclass of NSView. I simply need to add the overlay view in my view hierarchy above my image content."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-ImageAnalysisOverlayView"}],"type":"paragraph"},{"inlineContent":[{"text":"The simplest way is to add it as a sub view of my content view. Any way you choose is perfectly fine, but I’ve found adding it as a subView is generally simpler, and easier to manage as you don’t have to handle repositioning the overlay view when the contents view position changes.","type":"text"}],"type":"paragraph"},{"text":"Contents rect","level":3,"type":"heading","anchor":"Contents-rect"},{"inlineContent":[{"text":"Since the OverlayView doesn’t host or render your content, it needs to know exactly where the content exists in relation to its bounds. This is described by the contentsRect, which is in unit coordinate space with the origin at the top left.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Since the overlay view is placed directly over the imageView, they have same bounds. I’ll show the bounds with this rectangle. And I’ll also add its matching contents rect. Easiest case is if the content matches the bounds. Here it’s simply the unit rectangle."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-contentsRect","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Now, here is an aspect fit."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-contentsRect2"}],"type":"paragraph"},{"inlineContent":[{"text":"Notice this portion of the imageView now has no content underneath it. And is reflected in the contents rect.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"And here is an aspect fill. This portion of the image is no longer visible to the user, Notice how the contents rect changes here."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-contentsRect3","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Just as with UIImageView on iOS, if you are using NSImageView, you can simply set the trackingImageView property on the overlay view, and it will calculate all of this for you automatically.","type":"text"}],"type":"paragraph"},{"code":["override func viewDidLoad() {","","    super.viewDidLoad()","    overlayView = ImageAnalysisOverlayView(frame: imageView.bounds)","    overlayView.autoresizingMask = [.width, .height]","    overlayView.delegate = self;","    ","    imageView.addSubview(overlayView)","    overlay.trackingImageView = imageView","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"type":"text","text":"If you are not using NSImageView, no worries. You can provide the contents rect by implementing the delegate method "},{"type":"codeVoice","code":"contentsRect(for overlayView:)"}],"type":"paragraph"},{"inlineContent":[{"text":"The overlay view will ask for this during layout when its bounds change. However, you can manually request this be updated by calling ","type":"text"},{"type":"codeVoice","code":"setContentsRectNeedsUpdate"},{"text":" on the overlayView.","type":"text"}],"type":"paragraph"},{"code":["func contentsRect(for overlayView: ImageAnalysisOverlayView) -> CGRect {","    return calculateOverlayContentsRect()","}","","setContentsRectNeedsUpdate()"],"syntax":"swift","type":"codeListing"},{"text":"Contextual menus","level":3,"type":"heading","anchor":"Contextual-menus"},{"inlineContent":[{"text":"Contextual menus are a huge part of the Mac experience. Now Its now easy to add VisionKit provided functionality directly into your menus, for features such as Live Text, Look Up, and Subject Lifting, and more. One question you may have is, why? Let’s examine the macOS Photos app. If I were to right click on the text for this iconic road sign, I would just be presented with only the VisionKit text menu.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-contextualMenus"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"If it were not over text, I would be offered the app menu instead, without any text items. This is not ideal. Now in macOS Sonoma, items can be combined into the same menu. You can easily get to both text and image functionality, no matter where the menu event was initiated."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-contextualMenus2","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This is a far better experience for the user, and is simple to implement. Let’s explore how this can be accomplished in your own app. You now have a new delegate method available, "},{"type":"codeVoice","code":"overlayview:updatedmenu:forevent:atpoint"},{"type":"text","text":"."}],"type":"paragraph"},{"code":["func overlayView(","    _ overlayView: ImageAnalysisOverlayView, ","    updatedMenuFor menu: NSMenu, ","    for event: NSEvent, ","    at point: CGPoint","    ) -> NSMenu { return menu }"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"type":"text","text":"The arguments include the event that triggered the menu, and the point in the overlay view bounds coordinate space, so you can create any menus you need. From there, you simply need to return the menu that you would like to display. The default implementation returns the VisionKit menu. However, you may wish to add your own items to that menu, or take items from that menu and add it to yours."}],"type":"paragraph"},{"text":"Menu Tags","level":3,"type":"heading","anchor":"Menu-Tags"},{"inlineContent":[{"text":"VisionKit menu items are identified by tags, and there is a struct available that contains these tags. We have several items available for copying and sharing the image and subjects, and one for Look Up. We also have a special item you can use to find the suggested index to add items to the VisionKit provided menu, but more on that later.","type":"text"}],"type":"paragraph"},{"code":["ImageAnalysisOverlayView.MenuTag","    .copyImage","    .shareImage","    .copySubject","    .shareSubject ","    .lookupItem ","    .recommendedAppItems"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"Here are some examples of how this is used. If I had an existing menu and all I were interested in was adding the copySubject item, it could be easily added like this. First, get your apps menu. Then get the item you are interested in. In this case, copySubject. And insert it in your menu. Now, it’s important to remember that Items will only be available if they are actually valid. For example, if there’s no subject interaction capable type present, the copySubject item will not be in the menu. Also, for system provided text items, they are included if applicable, but not all are identifiable by tag. You can even customize these items however you would wish. For example, I’ve changed the item from copy image to copy photo. And worry not about changing these properties. These items are re-created each time and you can change them however you’d like.","type":"text"}],"type":"paragraph"},{"code":["func overlayView(_ overlayView: ImageAnalysisOverlayView, ","    updatedMenuFor menu: NSMenu, ","    for event: NSEvent, ","    at point: CGPoint) -> NSMenu {","        let myMenu = self.menu(for: event)","        if let item = menu.item(withTag: ImageAnalysisOverlayView.MenuTag.copySubject) {","                item.title = NSLocalizedString(\"Copy Photo\", ","                                \"Copy Photo menu title\")","                myMenu.insertItem(item, at: (someIndex))","            }","        return myMenu","    }"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"Now that I’ve covered adding items to your existing menu, I’m going to explore an example of how to add items to the VisionKit menu.","type":"text"}],"type":"paragraph"},{"code":["func overlayView(_ overlayView: ImageAnalysisOverlayView, ","    updatedMenuFor menu: NSMenu, ","    for event: NSEvent, ","    at point: CGPoint) -> NSMenu {","        let recommendedIndex = menu.indexOfItem(withTag: .recommendedAppItems)","        menu. insertItem(myItem, at: recommendedIndex) ","    return menu","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"As mentioned earlier, the overlayView will provide an item with a tag at the recommended index to insert your items called recommendedAppItems. You simply ask for the index of this item and insert your items at that index. Using this index is optional and not required. However, it is a good way to keep things consistent for your users.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"You will notice that some of these menu items have special properties. For example, when the subject related menu item are highlighted, the area surrounding my cat KiKi here dims and the glow animation begins, indicating the subject to the user before it is copied or shared."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-contextualMenus3","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"VisionKit uses the menu appearing as a trigger to begin subject analysis, if it hasn’t began already. This is all handled for you automatically. In order to provide these features, VisionKit will set itself as a delegate for any menu you return from the update menu method. If you were previously relying on these NSMenuDelegate callbacks, VisionKit now provides its own Delegate callbacks allowing you to retain functionality with your menu items if you were using that previously.","type":"text"}],"type":"paragraph"},{"code":["NSMenuDelgate","    menuNeedsUpdate (NSMenu)","    menuWill0pen (NSMenu)","    menuDidClose (NSMenu)","    menu(NSMenu, willHighlight: NSMenuItem?)","","ImageAnalysisOverlayViewDelegate","    overlayView(ImageAnalysisOverlayView, needsUpdate: NSMenu)","    overlayView(ImageAnalysisOverlayView, willOpen: NSMenu)","    overlayView(ImageAnalysisOverlayView, didClose: NSMenu)","    overlayView(ImageAnalysisOverlayView,menu: NSMenu, willHighlight: NSMenuItem?)"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"And here’s a quick tip. If you are in this situation, depending on where the menu was initiated from, it may not come from VisionKit. So you’ll likely want to keep your existing implementation around.","type":"text"}],"type":"paragraph"},{"code":["func menuWillOpen(_ menu: NSMenu) {","    \/\/ Your fancy code.","}","","func overlayView(_ overlayView: ImageAnalysisOverlayView, willOpen menu: NSMenu){","        menuWillOpen(menu)","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"type":"text","text":"Generally, the simplest way to keep this all in sync is to have your OverlayViewDelegate implementation call your matching NSMenuDelegate implementation, adjusting as necessary. Of course, ensure this makes sense for your app, but in general, this usually does the trick. And that’s a quick overview of what’s new in VisionKit."}],"type":"paragraph"},{"text":"Check out also","level":1,"type":"heading","anchor":"Check-out-also"},{"inlineContent":[{"isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc22\/10026","type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc22\/10025","type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc23\/10176","type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"overridingTitle":"Extract document data using Vision -  WWDC21","identifier":"https:\/\/developer.apple.com\/wwdc21\/10041","type":"reference","overridingTitleInlineContent":[{"text":"Extract document data using Vision -  WWDC21","type":"text"}]}],"type":"paragraph"},{"text":"Written By","level":2,"type":"heading","anchor":"Written-By"},{"numberOfColumns":5,"columns":[{"size":1,"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"multitudes"}]}]},{"size":4,"content":[{"text":"laurent b","type":"heading","level":3,"anchor":"laurent-b"},{"inlineContent":[{"isActive":true,"overridingTitle":"Contributed Notes","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/multitudes"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/laurentbrusa.hashnode.dev\/"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/x.com\/wrmultitudes"}],"type":"paragraph"}]}],"type":"row"},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true,"type":"reference"}],"type":"paragraph"},{"text":"Related Sessions","level":2,"type":"heading","anchor":"Related-Sessions"},{"style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10026-Add-Live-Text-interaction-to-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10276-Use-the-camera-for-keyboard-input-in-your-app"],"type":"links"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}],"kind":"content"}],"kind":"article","sampleCodeDownload":{"action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc23\/10048","isActive":true,"overridingTitle":"Watch Video (19 min)"},"kind":"sampleDownload"},"references":{"WWDC23-10048-ImageAnalysisInteraction":{"alt":"ImageAnalysisInteraction","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-ImageAnalysisInteraction.jpg"}],"identifier":"WWDC23-10048-ImageAnalysisInteraction","type":"image"},"WWDC23-10048-contacts2":{"alt":"Contacts detection","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contacts2.jpg"}],"identifier":"WWDC23-10048-contacts2","type":"image"},"https://developer.apple.com/wwdc22/10025":{"title":"Capture machine-readable codes and text with VisionKit -  WWDC22","identifier":"https:\/\/developer.apple.com\/wwdc22\/10025","titleInlineContent":[{"text":"Capture machine-readable codes and text with VisionKit -  WWDC22","type":"text"}],"type":"link","url":"https:\/\/developer.apple.com\/wwdc22\/10025"},"WWDC23-10048-selection2":{"alt":"Selection Look Up","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-selection2.jpg"}],"identifier":"WWDC23-10048-selection2","type":"image"},"WWDC23-10048-tracking":{"alt":"New Tracking","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-tracking.jpg"}],"identifier":"WWDC23-10048-tracking","type":"image"},"https://github.com/multitudes":{"title":"GitHub","identifier":"https:\/\/github.com\/multitudes","titleInlineContent":[{"text":"GitHub","type":"text"}],"type":"link","url":"https:\/\/github.com\/multitudes"},"WWDC23-10048-badges2":{"alt":"Badges","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-badges2.jpg"}],"identifier":"WWDC23-10048-badges2","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"images":[{"identifier":"WWDC23-Icon.png","type":"icon"},{"identifier":"WWDC23.jpeg","type":"card"}],"title":"WWDC23","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}],"role":"collectionGroup","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"},"WWDC23-10048-visualLookUp":{"alt":"Visual Look Up","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-visualLookUp.jpg"}],"identifier":"WWDC23-10048-visualLookUp","type":"image"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Contributions are welcome!","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"type":"link","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"WWDC23-10048-contentsRect3":{"alt":"Contents rect","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contentsRect3.jpg"}],"identifier":"WWDC23-10048-contentsRect3","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10176-Lift-subjects-from-images-in-your-app":{"title":"Lift subjects from images in your app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","abstract":[{"text":"Discover how you can easily pull the subject of an image from its background in your apps. Learn how to lift the primary subject or to access the subject at a given point with VisionKit. We’ll also share how you can lift subjects using Vision and combine that with lower-level frameworks like Core Image to create fun image effects and more complex compositing pipelines.","type":"text"}],"role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10176-lift-subjects-from-images-in-your-app","kind":"article"},"WWDC23-10048-badges":{"alt":"Badges","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-badges.jpg"}],"identifier":"WWDC23-10048-badges","type":"image"},"multitudes":{"alt":"Profile image of laurent b","variants":[{"traits":["1x","light"],"url":"\/images\/multitudes.jpeg"}],"identifier":"multitudes","type":"image"},"WWDC23-10048-visualLookUp3":{"alt":"Visual Look Up","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-visualLookUp3.jpg"}],"identifier":"WWDC23-10048-visualLookUp3","type":"image"},"https://laurentbrusa.hashnode.dev/":{"title":"Blog","identifier":"https:\/\/laurentbrusa.hashnode.dev\/","titleInlineContent":[{"text":"Blog","type":"text"}],"type":"link","url":"https:\/\/laurentbrusa.hashnode.dev\/"},"WWDC23-10048-subjectLifting":{"alt":"Subject Lifting","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-subjectLifting.jpg"}],"identifier":"WWDC23-10048-subjectLifting","type":"image"},"WWDC23-10048-tables":{"alt":"Tables detection","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-tables.jpg"}],"identifier":"WWDC23-10048-tables","type":"image"},"WWDCNotes.png":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"identifier":"WWDCNotes.png","type":"image"},"https://developer.apple.com/wwdc21/10041":{"title":"Extract document data using Vision -  WWDC21","identifier":"https:\/\/developer.apple.com\/wwdc21\/10041","titleInlineContent":[{"text":"Extract document data using Vision -  WWDC21","type":"text"}],"type":"link","url":"https:\/\/developer.apple.com\/wwdc21\/10041"},"WWDC23-10048-languages":{"alt":"Live Text enhancements","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-languages.jpg"}],"identifier":"WWDC23-10048-languages","type":"image"},"https://developer.apple.com/wwdc23/10048":{"checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc23\/10048","type":"download","url":"https:\/\/developer.apple.com\/wwdc23\/10048"},"WWDC23-10048-contextualMenus2":{"alt":"Contextual Menus","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contextualMenus2.jpg"}],"identifier":"WWDC23-10048-contextualMenus2","type":"image"},"WWDC23-10048-interactionTypes":{"alt":"Interaction Types","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-interactionTypes.jpg"}],"identifier":"WWDC23-10048-interactionTypes","type":"image"},"WWDC23-Icon.png":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-Icon.png"}],"identifier":"WWDC23-Icon.png","type":"image"},"WWDC23-10048-selection":{"alt":"Selection","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-selection.jpg"}],"identifier":"WWDC23-10048-selection","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","role":"collection","title":"WWDC Notes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","url":"\/documentation\/wwdcnotes"},"https://developer.apple.com/wwdc22/10026":{"title":"Add Live Text Interaction to Your App - WWDC22","identifier":"https:\/\/developer.apple.com\/wwdc22\/10026","titleInlineContent":[{"text":"Add Live Text Interaction to Your App - WWDC22","type":"text"}],"type":"link","url":"https:\/\/developer.apple.com\/wwdc22\/10026"},"WWDC23-10048-contextualMenus3":{"alt":"Contextual Menus","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contextualMenus3.jpg"}],"identifier":"WWDC23-10048-contextualMenus3","type":"image"},"WWDC23-10048-listDetection":{"alt":"List detection","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-listDetection.jpg"}],"identifier":"WWDC23-10048-listDetection","type":"image"},"WWDC23-10048-contentsRect2":{"alt":"Contents rect","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contentsRect2.jpg"}],"identifier":"WWDC23-10048-contentsRect2","type":"image"},"WWDC23-10048-contacts":{"alt":"Contacts detection","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contacts.jpg"}],"identifier":"WWDC23-10048-contacts","type":"image"},"WWDC23-10048-contextualMenus":{"alt":"Contextual Menus","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contextualMenus.jpg"}],"identifier":"WWDC23-10048-contextualMenus","type":"image"},"WWDC23-10048-contentsRect":{"alt":"Contents rect","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-contentsRect.jpg"}],"identifier":"WWDC23-10048-contentsRect","type":"image"},"WWDC23-10048-transcript":{"alt":"New currency tracker","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-transcript.jpg"}],"identifier":"WWDC23-10048-transcript","type":"image"},"https://developer.apple.com/wwdc23/10176":{"title":"Lift subjects from images in your app -  WWDC23","identifier":"https:\/\/developer.apple.com\/wwdc23\/10176","titleInlineContent":[{"text":"Lift subjects from images in your app -  WWDC23","type":"text"}],"type":"link","url":"https:\/\/developer.apple.com\/wwdc23\/10176"},"https://x.com/wrmultitudes":{"title":"X\/Twitter","identifier":"https:\/\/x.com\/wrmultitudes","titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"type":"link","url":"https:\/\/x.com\/wrmultitudes"},"WWDC23-10048-recap2022":{"alt":"Recap of WWDC 2022","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-recap2022.jpg"}],"identifier":"WWDC23-10048-recap2022","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10276-Use-the-camera-for-keyboard-input-in-your-app":{"title":"Use the camera for keyboard input in your app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10276-Use-the-camera-for-keyboard-input-in-your-app","abstract":[{"text":"Learn how you can support Live Text and intelligently pull information from the camera to fill out forms and text fields in your app. We’ll show you how to apply content filtering to capture the correct information when someone uses the camera as keyboard input and apply it to a relevant UITextField, helping your app input data like phone numbers, addresses, and flight information. And we’ll explore how you can create a custom interface, extend other controls like UIImageViews to support this capability, and more.","type":"text"}],"role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc21-10276-use-the-camera-for-keyboard-input-in-your-app","kind":"article"},"multitudes.jpeg":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/multitudes.jpeg"}],"identifier":"multitudes.jpeg","type":"image"},"WWDC23-10048-visualLookUp4":{"alt":"Visual Look Up","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-visualLookUp4.jpg"}],"identifier":"WWDC23-10048-visualLookUp4","type":"image"},"WWDC23-10048-ImageAnalysisOverlayView":{"alt":"ImageAnalysisOverlayView","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-ImageAnalysisOverlayView.jpg"}],"identifier":"WWDC23-10048-ImageAnalysisOverlayView","type":"image"},"WWDC23-10048-visualLookUp2":{"alt":"Visual Look Up","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-visualLookUp2.jpg"}],"identifier":"WWDC23-10048-visualLookUp2","type":"image"},"WWDC23-10048-sticker1":{"alt":"Stickers","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-sticker1.jpg"}],"identifier":"WWDC23-10048-sticker1","type":"image"},"WWDC23-10048-currency":{"alt":"New currency tracker","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10048-currency.jpg"}],"identifier":"WWDC23-10048-currency","type":"image"},"WWDC23.jpeg":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23.jpeg"}],"identifier":"WWDC23.jpeg","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10026-Add-Live-Text-interaction-to-your-app":{"title":"Add Live Text interaction to your app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10026-Add-Live-Text-interaction-to-your-app","abstract":[{"text":"Learn how you can bring Live Text support for still photos or paused video frames to your app. We’ll share how you can easily enable text interactions, translation, data detection, and QR code scanning within any image view on iOS, iPadOS, or macOS. We’ll also go over how to control interaction types, manage the supplementary interface, and resolve potential gesture conflicts.","type":"text"}],"role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc22-10026-add-live-text-interaction-to-your-app","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/multitudes":{"abstract":[{"type":"text","text":"student at 42Berlin 🐬 | 🍎 Swift(UI) app dev  | speciality coffee ☕️ & cycling 🚴🏻‍♂️"}],"role":"sampleCode","kind":"article","title":"laurent b (33 notes)","type":"topic","url":"\/documentation\/wwdcnotes\/multitudes","images":[{"type":"card","identifier":"multitudes.jpeg"},{"type":"icon","identifier":"multitudes.jpeg"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes"}}}