{"schemaVersion":{"major":0,"minor":3,"patch":0},"abstract":[{"text":"Discover how VisionKit can help people quickly lift subjects from images in your app and learn more about the content of an image with Visual Look Up. We’ll also take a tour of the latest updates to VisionKit for Live Text interaction, data scanning, and expanded support for macOS apps.","type":"text"}],"sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10048-Whats-new-in-VisionKit"},"seeAlsoSections":[{"title":"Updated Tools & Frameworks","generated":true,"identifiers":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10178-Whats-new-in-App-Clips","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10117-Whats-new-in-App-Store-Connect","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10015-Whats-new-in-App-Store-preorders","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10014-Whats-new-in-App-Store-pricing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10141-Whats-new-in-App-Store-server-APIs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10054-Whats-new-in-AppKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10108-Whats-new-in-Background-Assets","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10121-Whats-new-in-CSS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10186-Whats-new-in-Core-Data","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10179-Whats-new-in-Core-Motion","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10197-Whats-new-in-SF-Symbols-5","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10119-Whats-new-in-Safari-extensions","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10136-Whats-new-in-ScreenCaptureKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10140-Whats-new-in-StoreKit-2-and-StoreKit-Testing-in-Xcode","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10164-Whats-new-in-Swift","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10148-Whats-new-in-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10055-Whats-new-in-UIKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10114-Whats-new-in-Wallet-and-Apple-Pay","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10118-Whats-new-in-Web-Inspector","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10165-Whats-new-in-Xcode-15","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10040-Whats-new-in-managing-Apple-devices","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10053-Whats-new-in-privacy","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10235-Whats-new-in-voice-processing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10120-Whats-new-in-web-apps"]}],"primaryContentSections":[{"kind":"content","content":[{"anchor":"overview","type":"heading","text":"Overview","level":2},{"inlineContent":[{"type":"text","text":"To recap, last year Live Text support was added in VisionKit, enabling interactions such as text selection, translation, QR support, and more for images in your apps. VisionKit also introduced the DataScannerViewController, The Data scanner uses a live camera feed provide a simple and full featured way to capture specific text types, as well as many variants of machine readable codes. Information on those APIs are included in these WWDC22 sessions."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-recap2022"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"You can get the links to the talks below at the end of this post."}],"type":"paragraph"},{"inlineContent":[{"text":"This year VisionKit is adding support for:","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Subject lifting","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Visual Look Up","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Data Scanner and Live Text. A new Live Text API for text selection.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Expanded platform support for Catalyst, and context menu integration for native macOS apps.","type":"text"}]}]}],"type":"unorderedList"},{"anchor":"Subject-Lifting","type":"heading","text":"Subject Lifting","level":1},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-subjectLifting"}],"type":"paragraph"},{"inlineContent":[{"text":"With a simple long press on the subject of an image, it lifts it from its surroundings and becomes highlighted with this beautiful animated glow, and then I am presented with several options to share it, or invoke Visual Look Up.","type":"text"}],"type":"paragraph"},{"anchor":"New-for-iOS-17","type":"heading","text":"New for iOS 17","level":2},{"inlineContent":[{"identifier":"WWDC23-10048-sticker1","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"You can now use any lifted subject to create a sticker, with fun effects such as shiny, puffy, and more to share with your friends and family.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Now the good news is, integrating Subject Lifting is very simple."}],"type":"paragraph"},{"inlineContent":[{"text":"Here is the same code snippet from the last year’s video, it supports Subject Lifting, without any code changed.","type":"text"}],"type":"paragraph"},{"type":"codeListing","code":["func analyzeCurrentImage() {","","    if let image = image {","        Task {","            do {","                let configuration =ImageAnalyzer.Configuration([.text, .machineReadableCode])","                let analysis = try awaitanalyzer.analyze(image,configuration:configuration)","                if image == self.image {","                    interaction.analysis =analysis;","                    interactio.preferredInteractionypes = [.automatic]","                }","            }","            catch {","                \/\/ Handle error...","            }","        }","    }","}"],"syntax":"swift"},{"inlineContent":[{"text":"Let’s explore further. Notice that I’m not passing anything special into the analyzer configuration. This is because in order to preserve power and performance, Subject Lifting analysis is handled separately by the interaction after the initial analysis is complete. For iOS this process occurs after it’s been on screen for a few seconds, and for macOS it will occur the first time the menu appears.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"This means you don’t need to handle the case of the user swiping though many photos. The interaction will handle this for you. All you need to do is ensure you have an appropriate interaction type set– in this case, automatic– and the rest is handled by the interaction.","type":"text"}],"type":"paragraph"},{"anchor":"Subject-lifting-interaction-types","type":"heading","text":"Subject lifting interaction types","level":1},{"inlineContent":[{"text":"Let’s examine Subject Lifting compatible interaction types a bit closer. Automatic gives the default out of the box experience, combining text interaction, Subject Lifting, and more.","type":"text"},{"text":"\n","type":"text"},{"text":"If you only want Subject Lifting, and not text selection or data detectors, you can set the interaction type to .imageSegmentation, or combine it with other types.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"And finally, if Subject Lifting just does not make sense for your app, but you want the previous automatic behavior from iOS 16, no problem, you can use a new type, .automaticTextOnly. This provides features such as text selection and data detectors, but not Subject Lifting.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-interactionTypes"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We have a detailed session specifically on Subject Lifting available if you would like to learn advanced topics about this amazing new technology in both VisionKit, and Vision."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Lift subjects from images in your app"}],"type":"paragraph"},{"anchor":"Visual-Look-Up","type":"heading","text":"Visual Look Up","level":1},{"inlineContent":[{"type":"text","text":"This year VisionKit also supports Visual Look Up. Visual Look Up allows users to easily identify and learn about pets, nature, landmarks, art, and media."},{"type":"text","text":"\n"},{"type":"text","text":"And In iOS 17, Visual Look Up will support additional domains, including food, products, and signs and symbols."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-visualLookUp"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-visualLookUp2"}],"type":"paragraph"},{"inlineContent":[{"text":"Now, finally, it’s easy to look up what those symbols on your laundry tags mean. I mean, that’s pretty cool!","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"Visual Look Up availability is based on language, and is available for these languages:","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"English","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Italian","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"French","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Japanese","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"German","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Spanish"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"Let’s take a quick peek under the hood, and explore how Visual Look Up works. It’s actually a two-part process. Initial processing is accomplished entirely on device at analysis time."},{"type":"text","text":"\n"},{"type":"text","text":"If the .visualLookUp type is present in the analyzer configuration, Visual Look Up will locate the bounding box of the results, and their top level domain. For example, if it’s a cat, book, or plant. This step also includes feature extraction."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-visualLookUp3"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Once the user requests to look up an object, then, and only then, are the domain and image embeddings from feature extraction sent to the server for additional processing."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-visualLookUp4","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Now you know how Visual Look Up works, let’s quickly explore how to use it, and what actions you need to take to add it to your app. Visual Look Up can be invoked in two different ways. The first is in conjunction with Subject Lifting, if the current lifted subject contains one, and only one, correlated Visual Look Up result, the Look Up option will be offered in the menu,","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-selection"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"and selecting it will show the full Look Up result."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-selection2","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"VisionKit handles this interaction for you automatically. As an adopter all you need to do is have "},{"code":".visualLookUp","type":"codeVoice"},{"type":"text","text":" added to your analyzer configuration at analysis time."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Second, there is a modal interaction available where badges are placed over each of the visual search results. Notice how the badges move to the corner if they leave the viewport, Users can tap on these badges to show the Look Up result. This is the same interaction as clicking on the info button in the Photos app, or Quick Look, for example."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10048-badges","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This mode is invoked by setting "},{"code":".visualLookUp","type":"codeVoice"},{"type":"text","text":" as the "},{"code":"preferredInteractionType","type":"codeVoice"},{"type":"text","text":" on your interaction."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10048-badges2"}],"type":"paragraph"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Please note: this type will have precedence over the other interaction types. For example, you cannot select text or data detectors at the same time that the visualLookup Mode is set. As such, this is normally used in conjunction with a button, or some other bespoke way to get in and out of this mode. For example, Quick Look uses the info button to enter Visual Look Up mode."}]},{"level":1,"type":"heading","anchor":"Data-Scanner-and-Live-Text","text":"Data Scanner and Live Text"},{"type":"paragraph","inlineContent":[{"text":"Introduced In iOS 16, the DataScannerViewController was designed to be the easiest way to use OCR with a live camera viewfinder.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"New in iOS 17, it’s been enhanced with:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"optical flow tracking"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"currency support"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Optical flow tracking can enhance text tracking for live-camera experiences."},{"type":"text","text":"\n"},{"type":"text","text":"In iOS 16 I’m scanning for text with highFrameRateTracking enabled."},{"type":"text","text":"\n"},{"type":"text","text":"But new in iOS17 with optical flow tracking the highlights feel much more stable and grounded than before."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-tracking"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Optical flow tracking comes for free whenever you use the DataScannerViewController, however, it is only available when recognizing text, and not machine readable codes. And you’re also required to scan for text without a specific text content type set. And finally, once again, ensure high frame-rate tracking is enabled. Which is, conveniently, the default."}]},{"code":["\/\/ Recognize URLs","let recognizedDataTypes: Set<DataScannerViewController.RecognizedDataType> = [","    .text(textContentType: .URL)","]","","\/\/ Present the data scanner configured without high frame-rate tracking","let dataScanner = DataScannerViewController(recognizedDataTypes: recognizedDataTypes,","                                            isHighFrameRateTrackingEnabled: false)","present(dataScanner, animated: true) {","    try? dataScanner.startScanning()","}"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"No matter how you configure it, the data scanner provides great text tracking; but if your use case allows for this configuration, the new optical flow tracking can enhance it even further.","type":"text"}]},{"level":1,"type":"heading","anchor":"Currency","text":"Currency"},{"type":"paragraph","inlineContent":[{"text":"Next, the data scanner has a new option allowing users to find and interact with monetary values. It’s incredibly simple to enable. Just set the text content type to currency when specifying text recognition in the data scanner’s initializer, just as you would other content types like email addresses or telephone numbers.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10048-currency","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"Now I’m going to explore this new type in more detail with a quick example. When the data scanner recognizes currency in text, It contains both a bounds and a transcript.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-transcript"}]},{"type":"paragraph","inlineContent":[{"text":"The transcript has both the currency symbol and the amount.","type":"text"}]},{"code":["\/\/ Print the total of values in the current locale's currency","let formatter = NumberFormatter()","formatter.numberStyle = .currency","formatter.locale = Locale.current","guard let currencySymbol = formatter.currencySymbol else { return }","","var total: Double = 0.0","for await allItems: [RecognizedItem] in dataScannerViewController.recognizedItems {","    for recognizedItem in allItems {","        if case .text(let text) = recognizedItem {","            let transcript = text.transcript","            if transcript.contains (currencySymbol),","               let value = formatter.number(from: transcript) {","                total = total + value.doubleValue","            }","        }","    }","    print(\"total: \\(total)\")","}"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"Here’s an example where I find the total of all the values on something like a receipt. First, I get the currency symbol using the current locale.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"While awaiting the data scanner’s results in the recognizedItems stream, I can loop through each of the recognized items and grab its transcript. If the transcript contains the currency symbol I’m interested in, I’ll go ahead and update the total value. And just like that, now you’ll have the sum of all values. This is just a simple example, but this can very powerful."}]},{"level":1,"type":"heading","anchor":"Live-Text-enhancements","text":"Live Text enhancements"},{"type":"paragraph","inlineContent":[{"type":"text","text":"First off, Live Text is coming to more regions by expanding our supported languages to include Thai and Vietnamese."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-languages"}]},{"type":"paragraph","inlineContent":[{"text":"Live Text includes enhancements for document structure detection as well this year. As an example, in iOS 16 Live Text supported list detection. This allows you to easily copy and paste a list into an app that understands lists, such as Notes, and the list formatting will be maintained.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-listDetection"}]},{"type":"paragraph","inlineContent":[{"text":"Live Text handles several list styles, such as numbers or bullets.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"And now, Live Text is offering the same support for Tables, making it far easier to get structured table data from an image into applications like Notes or Numbers. Now I can select, copy, and paste this table into Numbers, and the structure is maintained. Notice how it merges cells automatically if necessary.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10048-tables","type":"image"}]},{"level":2,"type":"heading","anchor":"Context-Aware-Data-Detectors","text":"Context Aware Data Detectors"},{"type":"paragraph","inlineContent":[{"type":"text","text":"We are also adding Context Aware Data Detectors in Live Text. For this feature, data detectors and their visual relationships are used when adding contacts. Notice how when I add this contact from an email address, additional information from surrounding data detectors are now included, allowing me to easily add all this information at once. Adding a contact from a business card or flyer has never been easier."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10048-contacts","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-contacts2"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"In addition to these great features you are also getting for free, VisionKit also has some new APIs specifically for text. Last year, you could get the entire text contents by accessing the transcript property on the image analysis."}]},{"code":["ImageAnalysis","    .transcript"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"You now have full access to plain and attributed text, selected ranges, and easy access to selected text. There is also a new delegate method so you can be aware when the text selection changes and update your UI as appropriate."}]},{"code":["ImageAnalysisInteraction","    .text","    .selectedText ","    .selectedRanges ","    .selectedAttributedText","    ","ImageAnalysisInteractionDelegate","    textSelectionDidChange(_ interaction: ImageAnalysisInteraction)"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"It is now easy to add features that rely on what the user has selected. For example, using the menu builder API, you could insert a menu item that creates a reminder based on the current text selection."}]},{"code":["override func buildMenu(with builder: UIMenuBuilder) {","    ","    let text = interaction.selectedText","    if text.length > 0, builder.system == UIMenuSystem.context {","        let command = UICommand(title:\"Create Reminder\", action:","                                    #selector(handleCreateReminder))","        let menu = UIMenu(options: .displayInline, children: [command])","        builder.insertSibling(menu, afterMenu: .share)","    }","    ","    super.buildMenu(with: builder)","}","@obic func handleCreateReminder() {","    createReminder(interaction.selectedText)","}"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"Start in the view controller that owns your image analysis interaction. First grab the selected text, and ensure it isn’t empty. Then create a command that calls our handler when chosen, Now create a menu object that holds the command. And finally, insert that menu as a sibling after the share menu option. Now you have a custom menu alongside system items like copy and share.","type":"text"}]},{"level":2,"type":"heading","anchor":"Expanded-platform-support","text":"Expanded platform support"},{"type":"paragraph","inlineContent":[{"text":"And this year, it’s all about the Mac. We are rolling out Catalyst support to easily bring Live Text from your iOS apps over to the Mac. And if you’re new to the native macOS API and the ImageAnalysisOverlayView, stay tuned, because I’m going to go over some specifics, as well as some tips on adopting them.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Finally, I am going to talk about a new system for menus, offering simple and seamless integration of VisionKit into your contextual menus."}]},{"level":3,"type":"heading","anchor":"Catalyst","text":"Catalyst"},{"type":"paragraph","inlineContent":[{"text":"Catalyst adoption is very straightforward. It should be a simple recompile to get the image analysis interaction working in Catalyst. We support, Live Text, Subject Lifting and Visual Look Up, but unfortunately QR code support is unavailable in either the Catalyst environment or native macOS API for VisionKit. However, I wanted to let you know that if you have a shared implementation, leaving the ","type":"text"},{"code":".machineReadableCodes","type":"codeVoice"},{"text":" in your analyzer configuration for Catalyst is perfectly safe, and just becomes a no-op. Also, please note that QR detection support is available in the Vision Framework if you need this functionality on the Mac.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Please see also: ","type":"text"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc21\/10041","type":"reference"}]},{"level":3,"type":"heading","anchor":"The-native-macOS-API","text":"The native macOS API"},{"type":"paragraph","inlineContent":[{"type":"text","text":"As with iOS, there are two major classes you need to be aware of when adopting VisionKit:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"The ImageAnalyzer"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The ImageAnalysisOverlayView."}]}]}]},{"type":"paragraph","inlineContent":[{"text":"First, the easy part. The Image Analyzer and analysis process for the Mac is identical to iOS. With the exception of machine readable codes being a no-op, as I mentioned earlier, everything is the same and is used in the same way. The main difference between the iOS ImageAnalysisInteraction, and macOS’s ImageAnalysisOverlayView is how the interaction is added to your application.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"For iOS, the ImageAnalysisInteraction is a UIInteraction that is added to a view, already existing in your apps view hierarchy.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-ImageAnalysisInteraction"}]},{"type":"paragraph","inlineContent":[{"text":"But UIInteraction does not exist on the Mac. So what do you do? In this case, as the name suggests, the ","type":"text"},{"code":"ImageAnalysisOverlayView","type":"codeVoice"},{"text":" is a subclass of NSView. I simply need to add the overlay view in my view hierarchy above my image content.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-ImageAnalysisOverlayView"}]},{"type":"paragraph","inlineContent":[{"text":"The simplest way is to add it as a sub view of my content view. Any way you choose is perfectly fine, but I’ve found adding it as a subView is generally simpler, and easier to manage as you don’t have to handle repositioning the overlay view when the contents view position changes.","type":"text"}]},{"level":3,"type":"heading","anchor":"Contents-rect","text":"Contents rect"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Since the OverlayView doesn’t host or render your content, it needs to know exactly where the content exists in relation to its bounds. This is described by the contentsRect, which is in unit coordinate space with the origin at the top left."}]},{"type":"paragraph","inlineContent":[{"text":"Since the overlay view is placed directly over the imageView, they have same bounds. I’ll show the bounds with this rectangle. And I’ll also add its matching contents rect. Easiest case is if the content matches the bounds. Here it’s simply the unit rectangle.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-contentsRect"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now, here is an aspect fit."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10048-contentsRect2","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Notice this portion of the imageView now has no content underneath it. And is reflected in the contents rect."}]},{"type":"paragraph","inlineContent":[{"text":"And here is an aspect fill. This portion of the image is no longer visible to the user, Notice how the contents rect changes here.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-contentsRect3"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Just as with UIImageView on iOS, if you are using NSImageView, you can simply set the trackingImageView property on the overlay view, and it will calculate all of this for you automatically."}]},{"code":["override func viewDidLoad() {","","    super.viewDidLoad()","    overlayView = ImageAnalysisOverlayView(frame: imageView.bounds)","    overlayView.autoresizingMask = [.width, .height]","    overlayView.delegate = self;","    ","    imageView.addSubview(overlayView)","    overlay.trackingImageView = imageView","}"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"If you are not using NSImageView, no worries. You can provide the contents rect by implementing the delegate method "},{"code":"contentsRect(for overlayView:)","type":"codeVoice"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The overlay view will ask for this during layout when its bounds change. However, you can manually request this be updated by calling "},{"code":"setContentsRectNeedsUpdate","type":"codeVoice"},{"type":"text","text":" on the overlayView."}]},{"code":["func contentsRect(for overlayView: ImageAnalysisOverlayView) -> CGRect {","    return calculateOverlayContentsRect()","}","","setContentsRectNeedsUpdate()"],"syntax":"swift","type":"codeListing"},{"level":3,"type":"heading","anchor":"Contextual-menus","text":"Contextual menus"},{"type":"paragraph","inlineContent":[{"text":"Contextual menus are a huge part of the Mac experience. Now Its now easy to add VisionKit provided functionality directly into your menus, for features such as Live Text, Look Up, and Subject Lifting, and more. One question you may have is, why? Let’s examine the macOS Photos app. If I were to right click on the text for this iconic road sign, I would just be presented with only the VisionKit text menu.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10048-contextualMenus","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"If it were not over text, I would be offered the app menu instead, without any text items. This is not ideal. Now in macOS Sonoma, items can be combined into the same menu. You can easily get to both text and image functionality, no matter where the menu event was initiated."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10048-contextualMenus2","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"This is a far better experience for the user, and is simple to implement. Let’s explore how this can be accomplished in your own app. You now have a new delegate method available, ","type":"text"},{"code":"overlayview:updatedmenu:forevent:atpoint","type":"codeVoice"},{"text":".","type":"text"}]},{"code":["func overlayView(","    _ overlayView: ImageAnalysisOverlayView, ","    updatedMenuFor menu: NSMenu, ","    for event: NSEvent, ","    at point: CGPoint","    ) -> NSMenu { return menu }"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The arguments include the event that triggered the menu, and the point in the overlay view bounds coordinate space, so you can create any menus you need. From there, you simply need to return the menu that you would like to display. The default implementation returns the VisionKit menu. However, you may wish to add your own items to that menu, or take items from that menu and add it to yours."}]},{"level":3,"type":"heading","anchor":"Menu-Tags","text":"Menu Tags"},{"type":"paragraph","inlineContent":[{"text":"VisionKit menu items are identified by tags, and there is a struct available that contains these tags. We have several items available for copying and sharing the image and subjects, and one for Look Up. We also have a special item you can use to find the suggested index to add items to the VisionKit provided menu, but more on that later.","type":"text"}]},{"code":["ImageAnalysisOverlayView.MenuTag","    .copyImage","    .shareImage","    .copySubject","    .shareSubject ","    .lookupItem ","    .recommendedAppItems"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Here are some examples of how this is used. If I had an existing menu and all I were interested in was adding the copySubject item, it could be easily added like this. First, get your apps menu. Then get the item you are interested in. In this case, copySubject. And insert it in your menu. Now, it’s important to remember that Items will only be available if they are actually valid. For example, if there’s no subject interaction capable type present, the copySubject item will not be in the menu. Also, for system provided text items, they are included if applicable, but not all are identifiable by tag. You can even customize these items however you would wish. For example, I’ve changed the item from copy image to copy photo. And worry not about changing these properties. These items are re-created each time and you can change them however you’d like."}]},{"code":["func overlayView(_ overlayView: ImageAnalysisOverlayView, ","    updatedMenuFor menu: NSMenu, ","    for event: NSEvent, ","    at point: CGPoint) -> NSMenu {","        let myMenu = self.menu(for: event)","        if let item = menu.item(withTag: ImageAnalysisOverlayView.MenuTag.copySubject) {","                item.title = NSLocalizedString(\"Copy Photo\", ","                                \"Copy Photo menu title\")","                myMenu.insertItem(item, at: (someIndex))","            }","        return myMenu","    }"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now that I’ve covered adding items to your existing menu, I’m going to explore an example of how to add items to the VisionKit menu."}]},{"code":["func overlayView(_ overlayView: ImageAnalysisOverlayView, ","    updatedMenuFor menu: NSMenu, ","    for event: NSEvent, ","    at point: CGPoint) -> NSMenu {","        let recommendedIndex = menu.indexOfItem(withTag: .recommendedAppItems)","        menu. insertItem(myItem, at: recommendedIndex) ","    return menu","}"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"As mentioned earlier, the overlayView will provide an item with a tag at the recommended index to insert your items called recommendedAppItems. You simply ask for the index of this item and insert your items at that index. Using this index is optional and not required. However, it is a good way to keep things consistent for your users.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"You will notice that some of these menu items have special properties. For example, when the subject related menu item are highlighted, the area surrounding my cat KiKi here dims and the glow animation begins, indicating the subject to the user before it is copied or shared.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10048-contextualMenus3"}]},{"type":"paragraph","inlineContent":[{"text":"VisionKit uses the menu appearing as a trigger to begin subject analysis, if it hasn’t began already. This is all handled for you automatically. In order to provide these features, VisionKit will set itself as a delegate for any menu you return from the update menu method. If you were previously relying on these NSMenuDelegate callbacks, VisionKit now provides its own Delegate callbacks allowing you to retain functionality with your menu items if you were using that previously.","type":"text"}]},{"code":["NSMenuDelgate","    menuNeedsUpdate (NSMenu)","    menuWill0pen (NSMenu)","    menuDidClose (NSMenu)","    menu(NSMenu, willHighlight: NSMenuItem?)","","ImageAnalysisOverlayViewDelegate","    overlayView(ImageAnalysisOverlayView, needsUpdate: NSMenu)","    overlayView(ImageAnalysisOverlayView, willOpen: NSMenu)","    overlayView(ImageAnalysisOverlayView, didClose: NSMenu)","    overlayView(ImageAnalysisOverlayView,menu: NSMenu, willHighlight: NSMenuItem?)"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"And here’s a quick tip. If you are in this situation, depending on where the menu was initiated from, it may not come from VisionKit. So you’ll likely want to keep your existing implementation around."}]},{"code":["func menuWillOpen(_ menu: NSMenu) {","    \/\/ Your fancy code.","}","","func overlayView(_ overlayView: ImageAnalysisOverlayView, willOpen menu: NSMenu){","        menuWillOpen(menu)","}"],"syntax":"swift","type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"Generally, the simplest way to keep this all in sync is to have your OverlayViewDelegate implementation call your matching NSMenuDelegate implementation, adjusting as necessary. Of course, ensure this makes sense for your app, but in general, this usually does the trick. And that’s a quick overview of what’s new in VisionKit.","type":"text"}]},{"level":1,"type":"heading","anchor":"Check-out-also","text":"Check out also"},{"type":"paragraph","inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc22\/10026"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc22\/10025"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc23\/10176"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"overridingTitleInlineContent":[{"text":"Extract document data using Vision -  WWDC21","type":"text"}],"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc21\/10041","overridingTitle":"Extract document data using Vision -  WWDC21"}]},{"level":2,"type":"heading","anchor":"Written-By","text":"Written By"},{"numberOfColumns":5,"type":"row","columns":[{"size":1,"content":[{"inlineContent":[{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4","type":"image"}],"type":"paragraph"}]},{"size":4,"content":[{"type":"heading","level":3,"anchor":"laurent-b","text":"laurent b"},{"type":"paragraph","inlineContent":[{"type":"reference","isActive":true,"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes","overridingTitle":"Contributed Notes","overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}]},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"type":"reference","isActive":true,"identifier":"https:\/\/github.com\/multitudes"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"type":"reference","isActive":true,"identifier":"https:\/\/x.com\/wrmultitudes"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"type":"reference","isActive":true,"identifier":"https:\/\/laurentbrusa.hashnode.dev\/"}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"reference","isActive":true,"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing"}]},{"level":2,"type":"heading","anchor":"Related-Sessions","text":"Related Sessions"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10026-Add-Live-Text-interaction-to-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10276-Use-the-camera-for-keyboard-input-in-your-app"]},{"type":"small","inlineContent":[{"inlineContent":[{"type":"text","text":"Legal Notice"}],"type":"strong"}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}]}],"sampleCodeDownload":{"action":{"overridingTitle":"Watch Video (19 min)","type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc23\/10048"},"kind":"sampleDownload"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc23-10048-whats-new-in-visionkit"],"traits":[{"interfaceLanguage":"swift"}]}],"kind":"article","metadata":{"role":"sampleCode","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC23","title":"What’s new in VisionKit"},"references":{"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"link","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"title":"Contributions are welcome!","url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10186-Whats-new-in-Core-Data":{"kind":"article","title":"What’s new in Core Data","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10186-whats-new-in-core-data","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10186-Whats-new-in-Core-Data","abstract":[{"text":"Elevate your app’s data persistence with improvements in Core Data. Learn how you can use composite attributes to create more intuitive data models. We’ll also show you how to migrate your schema through disruptive changes, when to defer intense migrations, and how to avoid overhead on a person’s device.","type":"text"}],"type":"topic"},"WWDC23-10048-contextualMenus2":{"variants":[{"url":"\/images\/WWDC23-10048-contextualMenus2.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contextualMenus2","type":"image","alt":"Contextual Menus"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10026-Add-Live-Text-interaction-to-your-app":{"kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc22-10026-add-live-text-interaction-to-your-app","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10026-Add-Live-Text-interaction-to-your-app","type":"topic","title":"Add Live Text interaction to your app","abstract":[{"type":"text","text":"Learn how you can bring Live Text support for still photos or paused video frames to your app. We’ll share how you can easily enable text interactions, translation, data detection, and QR code scanning within any image view on iOS, iPadOS, or macOS. We’ll also go over how to control interaction types, manage the supplementary interface, and resolve potential gesture conflicts."}]},"WWDC23-10048-recap2022":{"variants":[{"url":"\/images\/WWDC23-10048-recap2022.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-recap2022","type":"image","alt":"Recap of WWDC 2022"},"WWDC23-10048-selection2":{"variants":[{"url":"\/images\/WWDC23-10048-selection2.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-selection2","type":"image","alt":"Selection Look Up"},"WWDC23-10048-badges":{"variants":[{"url":"\/images\/WWDC23-10048-badges.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-badges","type":"image","alt":"Badges"},"WWDC23-10048-tables":{"variants":[{"url":"\/images\/WWDC23-10048-tables.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-tables","type":"image","alt":"Tables detection"},"WWDC23-10048-ImageAnalysisOverlayView":{"variants":[{"url":"\/images\/WWDC23-10048-ImageAnalysisOverlayView.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-ImageAnalysisOverlayView","type":"image","alt":"ImageAnalysisOverlayView"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10114-Whats-new-in-Wallet-and-Apple-Pay":{"kind":"article","title":"What’s new in Wallet and Apple Pay","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10114-whats-new-in-wallet-and-apple-pay","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10114-Whats-new-in-Wallet-and-Apple-Pay","abstract":[{"text":"Discover the latest updates to Wallet and Apple Pay. Learn how to take advantage of preauthorized payments, funds transfer, and Apple Pay Later merchandising to create great Apple Pay experiences in your app or for the web. Explore improved support for Mail, Messages, Safari, and third-party apps in Wallet Order Tracking, and find out how you can add more information to an order’s transaction or receipt details. And we’ll introduce you to Tap to Present ID on iPhone (or ID Verifier), a new way to accept IDs in Wallet using iPhone — no additional hardware needed.","type":"text"}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10055-Whats-new-in-UIKit":{"role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10055-whats-new-in-uikit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10055-Whats-new-in-UIKit","abstract":[{"text":"Explore enhancements and updates to UIKit and learn how to build better iOS, iPadOS, and Mac Catalyst apps. We’ll show you the latest features and improvements in UIKit and share API refinements, performance improvements, and much more.","type":"text"}],"kind":"article","title":"What’s new in UIKit","type":"topic"},"WWDC23-10048-visualLookUp2":{"variants":[{"url":"\/images\/WWDC23-10048-visualLookUp2.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-visualLookUp2","type":"image","alt":"Visual Look Up"},"WWDC23-10048-visualLookUp4":{"variants":[{"url":"\/images\/WWDC23-10048-visualLookUp4.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-visualLookUp4","type":"image","alt":"Visual Look Up"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10179-Whats-new-in-Core-Motion":{"kind":"article","title":"What’s new in Core Motion","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10179-whats-new-in-core-motion","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10179-Whats-new-in-Core-Motion","abstract":[{"text":"Learn how you can use the latest Core Motion updates to expand how your app uses motion data. Discover how to stream higher-frequency sensor data when recording a HealthKit workout on Apple Watch. We’ll show you how you can get submersion data — including water depth and temperature — during water-based activities like snorkeling. Find out how to stream motion data like attitude, user acceleration, and rotation rate from audio devices like AirPods to connected devices like iPhone and Mac.","type":"text"}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10235-Whats-new-in-voice-processing":{"kind":"article","title":"What’s new in voice processing","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10235-whats-new-in-voice-processing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10235-Whats-new-in-voice-processing","abstract":[{"text":"Learn how to use the Apple voice processing APIs to achieve the best possible audio experience in your VoIP apps. We’ll show you how to detect when someone is talking while muted, adjust ducking behavior of other audio, and more.","type":"text"}],"type":"topic"},"https://developer.apple.com/wwdc23/10176":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/10176","type":"link","titleInlineContent":[{"text":"Lift subjects from images in your app -  WWDC23","type":"text"}],"title":"Lift subjects from images in your app -  WWDC23","url":"https:\/\/developer.apple.com\/wwdc23\/10176"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10176-Lift-subjects-from-images-in-your-app":{"abstract":[{"text":"Discover how you can easily pull the subject of an image from its background in your apps. Learn how to lift the primary subject or to access the subject at a given point with VisionKit. We’ll also share how you can lift subjects using Vision and combine that with lower-level frameworks like Core Image to create fun image effects and more complex compositing pipelines.","type":"text"}],"title":"Lift subjects from images in your app","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10176-lift-subjects-from-images-in-your-app","kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app"},"doc://WWDCNotes/documentation/WWDCNotes":{"role":"collection","title":"WWDC Notes","kind":"symbol","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","url":"\/documentation\/wwdcnotes"},"WWDC23-10048-contentsRect":{"variants":[{"url":"\/images\/WWDC23-10048-contentsRect.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contentsRect","type":"image","alt":"Contents rect"},"doc://WWDCNotes/documentation/WWDCNotes/multitudes":{"url":"\/documentation\/wwdcnotes\/multitudes","title":"laurent b (32 notes)","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes","abstract":[{"text":"student at 42Berlin 🐬 | 🍎 Swift(UI) app dev  | speciality coffee ☕️ & cycling 🚴🏻‍♂️","type":"text"}],"role":"sampleCode"},"WWDC23-10048-selection":{"variants":[{"url":"\/images\/WWDC23-10048-selection.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-selection","type":"image","alt":"Selection"},"https://x.com/wrmultitudes":{"identifier":"https:\/\/x.com\/wrmultitudes","type":"link","titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"title":"X\/Twitter","url":"https:\/\/x.com\/wrmultitudes"},"WWDC23-10048-visualLookUp3":{"variants":[{"url":"\/images\/WWDC23-10048-visualLookUp3.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-visualLookUp3","type":"image","alt":"Visual Look Up"},"https://avatars.githubusercontent.com/u/29355828?v=4":{"variants":[{"url":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4","traits":["1x","light"]}],"identifier":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4","type":"image","alt":"Profile image of laurent b"},"WWDC23-10048-visualLookUp":{"variants":[{"url":"\/images\/WWDC23-10048-visualLookUp.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-visualLookUp","type":"image","alt":"Visual Look Up"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"identifier":"WWDCNotes.png","type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10118-Whats-new-in-Web-Inspector":{"title":"What’s new in Web Inspector","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10118-Whats-new-in-Web-Inspector","url":"\/documentation\/wwdcnotes\/wwdc23-10118-whats-new-in-web-inspector","abstract":[{"text":"Web Inspector provides a powerful set of tools to debug and inspect web pages, web extensions, and WKWebViews on macOS, iOS and iPadOS. We’ll share the latest updates, including improved typography inspection, editing tools for variable fonts, controls to emulate people’s preferences, element badges in the DOM node tree, and Symbolic breakpoints.","type":"text"}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10108-Whats-new-in-Background-Assets":{"role":"sampleCode","title":"What’s new in Background Assets","kind":"article","abstract":[{"text":"Waiting is no fun! Discover how Background Assets can help your app download content before it even launches. We’ll show you how to integrate Background Assets into an existing app, explore when to use essential or non-essential assets, and learn how to make debugging your extension a breeze.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10108-whats-new-in-background-assets","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10108-Whats-new-in-Background-Assets","type":"topic"},"WWDC23-10048-currency":{"variants":[{"url":"\/images\/WWDC23-10048-currency.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-currency","type":"image","alt":"New currency tracker"},"https://developer.apple.com/wwdc21/10041":{"identifier":"https:\/\/developer.apple.com\/wwdc21\/10041","type":"link","titleInlineContent":[{"text":"Extract document data using Vision -  WWDC21","type":"text"}],"title":"Extract document data using Vision -  WWDC21","url":"https:\/\/developer.apple.com\/wwdc21\/10041"},"WWDC23-10048-languages":{"variants":[{"url":"\/images\/WWDC23-10048-languages.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-languages","type":"image","alt":"Live Text enhancements"},"https://developer.apple.com/wwdc23/10048":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/10048","type":"download","checksum":null,"url":"https:\/\/developer.apple.com\/wwdc23\/10048"},"https://github.com/multitudes":{"identifier":"https:\/\/github.com\/multitudes","type":"link","titleInlineContent":[{"text":"GitHub","type":"text"}],"title":"GitHub","url":"https:\/\/github.com\/multitudes"},"WWDC23-10048-badges2":{"variants":[{"url":"\/images\/WWDC23-10048-badges2.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-badges2","type":"image","alt":"Badges"},"WWDC23-10048-contextualMenus3":{"variants":[{"url":"\/images\/WWDC23-10048-contextualMenus3.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contextualMenus3","type":"image","alt":"Contextual Menus"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10053-Whats-new-in-privacy":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10053-Whats-new-in-privacy","title":"What’s new in privacy","url":"\/documentation\/wwdcnotes\/wwdc23-10053-whats-new-in-privacy","type":"topic","abstract":[{"type":"text","text":"At Apple, we believe that privacy is a fundamental human right. Learn about new technologies on Apple platforms that make it easier for you to implement essential privacy patterns that build customer trust in your app. Discover privacy improvements for Apple’s platforms, as well as a study of how privacy shaped the software architecture and design for the input model on visionOS."}],"role":"sampleCode","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10141-Whats-new-in-App-Store-server-APIs":{"role":"sampleCode","title":"What’s new in App Store server APIs","kind":"article","abstract":[{"text":"Discover the latest updates to the App Store Server API and App Store Server Notifications. Explore the current API offerings and learn how to track subscription status with notifications, work with transactions on your server, and efficiently recover missed notifications. We’ll also show you how your server can support apps using StoreKit or StoreKit 2, and share an important deprecation in the API and suggested migration workflow.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10141-whats-new-in-app-store-server-apis","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10141-Whats-new-in-App-Store-server-APIs","type":"topic"},"WWDC23-10048-contacts":{"variants":[{"url":"\/images\/WWDC23-10048-contacts.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contacts","type":"image","alt":"Contacts detection"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10040-Whats-new-in-managing-Apple-devices":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10040-Whats-new-in-managing-Apple-devices","url":"\/documentation\/wwdcnotes\/wwdc23-10040-whats-new-in-managing-apple-devices","abstract":[{"text":"Learn about the latest management capabilities for iOS, iPadOS, and macOS. Discover how you can streamline the setup experience with enhancements to automated device enrollment and a new return-to-service option for iOS and iPadOS devices. We’ll share how to use your identity provider in even more places on macOS and show you how Apple Configurator can help automate tasks.","type":"text"}],"type":"topic","kind":"article","title":"What’s new in managing Apple devices","role":"sampleCode"},"https://developer.apple.com/wwdc22/10026":{"identifier":"https:\/\/developer.apple.com\/wwdc22\/10026","type":"link","titleInlineContent":[{"text":"Add Live Text Interaction to Your App - WWDC22","type":"text"}],"title":"Add Live Text Interaction to Your App - WWDC22","url":"https:\/\/developer.apple.com\/wwdc22\/10026"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10197-Whats-new-in-SF-Symbols-5":{"kind":"article","title":"What’s new in SF Symbols 5","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10197-whats-new-in-sf-symbols-5","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10197-Whats-new-in-SF-Symbols-5","abstract":[{"text":"Explore the latest updates to SF Symbols, Apple’s library of iconography designed to integrate seamlessly with San Francisco, the system font for Apple platforms. Learn about symbol animations: a collection of expressive, configurable animations that can make your interface feel more lively and improve user feedback. See how to draw for animation when creating your own custom symbols, and discover the latest additions to the SF Symbols library. To get the most out of this session, we recommend first watching “What’s new in SF Symbols 4” from WWDC22.","type":"text"}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10165-Whats-new-in-Xcode-15":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10165-whats-new-in-xcode-15","abstract":[{"type":"text","text":"Discover the latest productivity and performance improvements in Xcode 15. Explore enhancements to code completion and Xcode Previews, learn about the test navigator and test report, and find out more about the streamlined distribution process. We’ll also highlight improved navigation, source control management, and debugging."}],"title":"What’s new in Xcode 15","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10165-Whats-new-in-Xcode-15","role":"sampleCode"},"WWDC23-10048-listDetection":{"variants":[{"url":"\/images\/WWDC23-10048-listDetection.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-listDetection","type":"image","alt":"List detection"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10178-Whats-new-in-App-Clips":{"role":"sampleCode","title":"What’s new in App Clips","kind":"article","abstract":[{"text":"Explore the latest updates to App Clips. We’ll show you how to build App Clips more easily using default App Clip links. Learn how you can take advantage of the increased App Clip size limit to build richer and more engaging experiences, and find out how you can launch App Clips directly from your app.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10178-whats-new-in-app-clips","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10178-Whats-new-in-App-Clips","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10164-Whats-new-in-Swift":{"kind":"article","title":"What’s new in Swift","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10164-whats-new-in-swift","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10164-Whats-new-in-Swift","abstract":[{"text":"Join us for an update on Swift. We’ll show you how APIs are becoming more extensible and expressive with features like parameter packs and macros. We’ll also take you through improvements to interoperability and share how we’re expanding Swift’s performance and safety benefits everywhere from Foundation to large-scale distributed programs on the server.","type":"text"}],"type":"topic"},"WWDC23-10048-contentsRect3":{"variants":[{"url":"\/images\/WWDC23-10048-contentsRect3.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contentsRect3","type":"image","alt":"Contents rect"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10054-Whats-new-in-AppKit":{"title":"What’s new in AppKit","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10054-Whats-new-in-AppKit","url":"\/documentation\/wwdcnotes\/wwdc23-10054-whats-new-in-appkit","abstract":[{"text":"Discover the latest advances in Mac app development. We’ll share improvements to controls and menus and explore the tools that can help you break free from your (view) bounds. Learn how to add motion to your user interface, take advantage of improvements to text input, and integrate your existing code with Swift and SwiftUI.","type":"text"}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10148-Whats-new-in-SwiftUI":{"kind":"article","title":"What’s new in SwiftUI","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10148-whats-new-in-swiftui","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10148-Whats-new-in-SwiftUI","abstract":[{"text":"Learn how you can use SwiftUI to build great apps for all Apple platforms. Explore the latest updates to SwiftUI and discover new scene types for visionOS. Simplify your data models with the latest data flow options and learn about the Inspector view. We’ll also take you through enhanced animation APIs, powerful ScrollView improvements, and a host of refinements to help you make tidier tables, improve focus and keyboard input, and so much more.","type":"text"}],"type":"topic"},"WWDC23-10048-tracking":{"variants":[{"url":"\/images\/WWDC23-10048-tracking.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-tracking","type":"image","alt":"New Tracking"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10276-Use-the-camera-for-keyboard-input-in-your-app":{"title":"Use the camera for keyboard input in your app","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc21-10276-use-the-camera-for-keyboard-input-in-your-app","kind":"article","type":"topic","abstract":[{"text":"Learn how you can support Live Text and intelligently pull information from the camera to fill out forms and text fields in your app. We’ll show you how to apply content filtering to capture the correct information when someone uses the camera as keyboard input and apply it to a relevant UITextField, helping your app input data like phone numbers, addresses, and flight information. And we’ll explore how you can create a custom interface, extend other controls like UIImageViews to support this capability, and more.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10276-Use-the-camera-for-keyboard-input-in-your-app"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14, tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}],"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC23","role":"collectionGroup","url":"\/documentation\/wwdcnotes\/wwdc23","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10117-Whats-new-in-App-Store-Connect":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10117-Whats-new-in-App-Store-Connect","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10117-whats-new-in-app-store-connect","title":"What’s new in App Store Connect","type":"topic","kind":"article","abstract":[{"text":"Discover the latest updates to App Store Connect, the suite of tools used to manage and submit apps to the App Store. Explore how you can use the latest features to test, price, promote, and automate the management of your app more easily. We’ll also share enhancements to tools like TestFlight and the App Store Connect API.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10140-Whats-new-in-StoreKit-2-and-StoreKit-Testing-in-Xcode":{"url":"\/documentation\/wwdcnotes\/wwdc23-10140-whats-new-in-storekit-2-and-storekit-testing-in-xcode","title":"What’s new in StoreKit 2 and StoreKit Testing in Xcode","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10140-Whats-new-in-StoreKit-2-and-StoreKit-Testing-in-Xcode","abstract":[{"text":"Get to know the latest enhancements to StoreKit 2 and StoreKit Testing in Xcode. Discover API updates for promoted in-app purchases, StoreKit messages, the Transaction model, the RenewalInfo model, and the App Store sheet for managing subscriptions. Learn how to upgrade to SHA-256 for on-device receipt validation and use APIs to create SwiftUI views.","type":"text"}],"role":"sampleCode","type":"topic"},"WWDC23-10048-subjectLifting":{"variants":[{"url":"\/images\/WWDC23-10048-subjectLifting.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-subjectLifting","type":"image","alt":"Subject Lifting"},"WWDC23-10048-contextualMenus":{"variants":[{"url":"\/images\/WWDC23-10048-contextualMenus.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contextualMenus","type":"image","alt":"Contextual Menus"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10120-Whats-new-in-web-apps":{"type":"topic","abstract":[{"text":"Discover web apps for Mac — a powerful way to experience your website from the Dock. Learn how you can customize your web app to give people the best experience when they add your site. We’ll also share how to take advantage of push notifications and badging for web apps for Mac and Home Screen web apps for iOS and iPadOS.","type":"text"}],"title":"What’s new in web apps","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10120-Whats-new-in-web-apps","kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10120-whats-new-in-web-apps"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10014-Whats-new-in-App-Store-pricing":{"kind":"article","title":"What’s new in App Store pricing","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10014-whats-new-in-app-store-pricing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10014-Whats-new-in-App-Store-pricing","abstract":[{"text":"Discover the latest updates to App Store pricing capabilities and tools. Learn how you can manage pricing for your apps and in-app purchases within App Store Connect and the App Store Connect API, how to set pricing by region, and more.","type":"text"}],"type":"topic"},"https://developer.apple.com/wwdc22/10025":{"identifier":"https:\/\/developer.apple.com\/wwdc22\/10025","type":"link","titleInlineContent":[{"text":"Capture machine-readable codes and text with VisionKit -  WWDC22","type":"text"}],"title":"Capture machine-readable codes and text with VisionKit -  WWDC22","url":"https:\/\/developer.apple.com\/wwdc22\/10025"},"WWDC23-10048-ImageAnalysisInteraction":{"variants":[{"url":"\/images\/WWDC23-10048-ImageAnalysisInteraction.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-ImageAnalysisInteraction","type":"image","alt":"ImageAnalysisInteraction"},"WWDC23-10048-contacts2":{"variants":[{"url":"\/images\/WWDC23-10048-contacts2.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contacts2","type":"image","alt":"Contacts detection"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10119-Whats-new-in-Safari-extensions":{"kind":"article","title":"What’s new in Safari extensions","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10119-whats-new-in-safari-extensions","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10119-Whats-new-in-Safari-extensions","abstract":[{"text":"Learn about the latest improvements to Safari extensions. We’ll take you through new APIs, explore per-site permissions for Safari app extensions, and share how you can make sure your extensions work great in both Private Browsing and Profiles.","type":"text"}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10136-Whats-new-in-ScreenCaptureKit":{"title":"What’s new in ScreenCaptureKit","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10136-Whats-new-in-ScreenCaptureKit","url":"\/documentation\/wwdcnotes\/wwdc23-10136-whats-new-in-screencapturekit","abstract":[{"text":"Level up your screen sharing experience with the latest features in ScreenCaptureKit. Explore the built-in system picker, Presenter Overlay, and screenshot capabilities, and learn how to incorporate these features into your existing ScreenCaptureKit app or game.","type":"text"}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10121-Whats-new-in-CSS":{"title":"What’s new in CSS","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10121-Whats-new-in-CSS","url":"\/documentation\/wwdcnotes\/wwdc23-10121-whats-new-in-css","abstract":[{"text":"Explore the latest advancements in CSS. Learn techniques and best practices for working with wide-gamut color, creating gorgeous typography, and writing simple and robust code. We’ll also peer into the future and preview upcoming layout and typography features.","type":"text"}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10015-Whats-new-in-App-Store-preorders":{"kind":"article","title":"What’s new in App Store pre-orders","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10015-whats-new-in-app-store-preorders","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10015-Whats-new-in-App-Store-preorders","abstract":[{"text":"Discover the latest enhancements to App Store pre-orders, including regional publishing. We’ll show you how to use App Store Connect to set up pre-orders to simultaneously soft launch your app and offer it in different regions.","type":"text"}],"type":"topic"},"WWDC23-10048-sticker1":{"variants":[{"url":"\/images\/WWDC23-10048-sticker1.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-sticker1","type":"image","alt":"Stickers"},"WWDC23-10048-interactionTypes":{"variants":[{"url":"\/images\/WWDC23-10048-interactionTypes.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-interactionTypes","type":"image","alt":"Interaction Types"},"https://laurentbrusa.hashnode.dev/":{"identifier":"https:\/\/laurentbrusa.hashnode.dev\/","type":"link","titleInlineContent":[{"text":"Blog","type":"text"}],"title":"Blog","url":"https:\/\/laurentbrusa.hashnode.dev\/"},"WWDC23-10048-transcript":{"variants":[{"url":"\/images\/WWDC23-10048-transcript.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-transcript","type":"image","alt":"New currency tracker"},"WWDC23-10048-contentsRect2":{"variants":[{"url":"\/images\/WWDC23-10048-contentsRect2.jpg","traits":["1x","light"]}],"identifier":"WWDC23-10048-contentsRect2","type":"image","alt":"Contents rect"}}}