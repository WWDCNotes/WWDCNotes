{"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc23-10273-work-with-reality-composer-pro-content-in-xcode"],"traits":[{"interfaceLanguage":"swift"}]}],"seeAlsoSections":[{"title":"Deep Dives into Topics","generated":true,"identifiers":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10239-Add-SharePlay-to-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10248-Analyze-hangs-with-Instruments","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10258-Animate-symbols-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10158-Animate-with-springs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10159-Beyond-scroll-views","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10170-Beyond-the-basics-of-structured-concurrency","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10028-Bring-widgets-to-life","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10027-Bring-widgets-to-new-places","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10093-Bring-your-Unity-VR-app-to-a-fully-immersive-space","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10123-Bring-your-game-to-Mac-Part-1-Make-a-game-plan","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10124-Bring-your-game-to-Mac-Part-2-Compile-your-shaders","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10125-Bring-your-game-to-Mac-Part-3-Render-with-Metal","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10023-Build-a-multidevice-workout-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10036-Build-accessible-apps-with-SwiftUI-and-UIKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10154-Build-an-app-with-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10056-Build-better-documentbased-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10016-Build-custom-workouts-with-WorkoutKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10096-Build-great-games-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10252-Build-programmatic-UI-with-Xcode-Previews","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10006-Build-robust-and-resumable-file-transfers","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10087-Build-spatial-SharePlay-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10080-Build-spatial-experiences-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10029-Build-widgets-for-the-Smart-Stack-on-Apple-Watch","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10051-Create-a-great-ShazamKit-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10070-Create-a-great-spatial-playback-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10105-Create-a-more-responsive-camera-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10034-Create-accessible-spatial-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10257-Create-animated-symbols","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10088-Create-immersive-Unity-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10278-Create-practical-workflows-in-Xcode-Cloud","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10244-Create-rich-documentation-with-SwiftDocC","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10007-Create-seamless-experiences-with-Virtualization","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10101-Customize-ondevice-speech-recognition","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10226-Debug-with-structured-logging","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10071-Deliver-video-content-for-spatial-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10160-Demystify-SwiftUI-performance","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10263-Deploy-passkeys-at-work","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10193-Design-Shortcuts-for-Spotlight","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10138-Design-and-build-apps-for-watchOS-10","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10078-Design-considerations-for-vision-and-motion","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10073-Design-for-spatial-input","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10076-Design-for-spatial-user-interfaces","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10075-Design-spatial-SharePlay-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10309-Design-widgets-for-the-Smart-Stack-on-Apple-Watch","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10115-Design-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10203-Develop-your-first-immersive-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10052-Discover-Calendar-and-EventKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10256-Discover-Continuity-Camera-for-tvOS","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10089-Discover-Metal-for-immersive-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10149-Discover-Observation-in-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10085-Discover-Quick-Look-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10155-Discover-String-Catalogs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10180-Discover-streamlined-location-updates","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10196-Dive-deeper-into-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10254-Do-more-with-Managed-Apple-IDs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10110-Elevate-your-windowed-app-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10107-Embed-the-Photos-Picker-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10094-Enhance-your-iPad-and-iPhone-apps-for-the-Shared-Space","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10091-Evolve-your-ARKit-app-for-spatial-experiences","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10167-Expand-on-Swift-macros","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10275-Explore-AirPlay-with-interstitials","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10012-Explore-App-Store-Connect-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10042-Explore-Natural-Language-multilingual-models","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10156-Explore-SwiftUI-animation","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10103-Explore-enhancements-to-App-Intents","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10192-Explore-enhancements-to-RoomPlan","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10271-Explore-immersive-sound-design","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10122-Explore-media-formats-for-the-web","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10037-Explore-pie-charts-and-interactivity-in-Swift-Charts","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10095-Explore-rendering-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10142-Explore-testing-inapp-purchases","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10086-Explore-the-USD-ecosystem","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10033-Extend-Speech-Synthesis-with-personal-and-custom-voices","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10175-Fix-failures-faster-with-Xcode-test-reports","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10168-Generalize-APIs-with-parameter-packs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10260-Get-started-with-building-apps-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10060-Get-started-with-privacy-manifests","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10111-Go-beyond-the-window-with-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10161-Inspectors-in-SwiftUI-Discover-the-details","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10104-Integrate-your-media-app-with-HomePod","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10281-Keep-up-with-the-keyboard","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10229-Make-features-discoverable-with-TipKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10189-Migrate-to-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10172-Mix-Swift-and-C++","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10195-Model-your-schema-with-SwiftData","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10150-Optimize-CarPlay-for-vehicle-systems","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10127-Optimize-GPU-renderers-with-Metal","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10100-Optimize-app-power-and-performance-for-spatial-computing","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10050-Optimize-machine-learning-for-Metal-apps","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10035-Perform-accessibility-audits-for-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10072-Principles-of-spatial-design","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10266-Protect-your-Mac-app-with-environment-constraints","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10250-Prototype-with-Xcode-Playgrounds","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10002-Ready-set-relay-Protect-app-traffic-with-network-relays","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10262-Rediscover-Safari-developer-features","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10004-Reduce-network-delays-with-L4S","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10090-Run-your-iPad-and-iPhone-apps-in-the-Shared-Space","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10241-Share-files-with-SharePlay","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10224-Simplify-distribution-in-Xcode-and-Xcode-Cloud","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10102-Spotlight-your-app-with-App-Shortcuts","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10137-Support-Cinematic-mode-videos-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10181-Support-HDR-images-in-your-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10188-Sync-to-iCloud-with-CKSyncEngine","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10113-Take-SwiftUI-to-the-next-dimension","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10162-The-SwiftUI-cookbook-for-focus","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10238-Tune-up-your-AirPlay-audio-experience","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10057-Unleash-the-UIKit-trait-system","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10153-Unlock-the-power-of-grammatical-agreement","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10185-Update-Live-Activities-with-push-notifications","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10031-Update-your-app-for-watchOS-10","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10061-Verify-app-dependencies-with-digital-signatures","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10058-Whats-new-with-text-and-text-interactions","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10157-Wind-your-way-through-advanced-animations-in-SwiftUI","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10166-Write-Swift-macros","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10128-Your-guide-to-Metal-ray-tracing"]}],"kind":"article","abstract":[{"text":"Learn how to bring content from Reality Composer Pro to life in Xcode. We’ll show you how to load 3D scenes into Xcode, integrate your content with your code, and add interactivity to your app. We’ll also share best practices and tips for using these tools together in your development workflow.","type":"text"}],"schemaVersion":{"minor":3,"patch":0,"major":0},"metadata":{"title":"Work with Reality Composer Pro content in Xcode","roleHeading":"WWDC23","modules":[{"name":"WWDC Notes"}],"role":"sampleCode"},"primaryContentSections":[{"content":[{"level":2,"type":"heading","anchor":"Chapters","text":"Chapters:"},{"type":"paragraph","inlineContent":[{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=0"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=157"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=387"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=720"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1671"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1818"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=2005"}]},{"level":1,"type":"heading","anchor":"Reality-Composer-Pro","text":"Reality Composer Pro"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Reality Composer Pro is a developer tool for preparing RealityKit content to be used in spatial computing apps."}]},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-composer"}],"type":"paragraph"},{"inlineContent":[{"text":"The editor UI and features of Reality Composer Pro are covered in the sessions:","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202","type":"reference","isActive":true},{"type":"text","text":""},{"type":"text","text":"\n"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083","type":"reference","isActive":true}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We’re looking at a topographical map of Yosemite National Park."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-yosemite"}],"type":"paragraph"},{"inlineContent":[{"text":"We’ve added a slider to morph between two different California landmarks. Now we’re looking at Catalina Island off the coast of Los Angeles.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-catalina","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We also have hovering 2D SwiftUI buttons positioned in 3D space to learn more about various points of interest in that map."}],"type":"paragraph"},{"level":1,"type":"heading","text":"Load 3D content","anchor":"Load-3D-content"},{"inlineContent":[{"text":"In the above sessions, Apple made a Reality Composer Pro project that contains all the assets for a diorama. These tabs at the top each represent one root entity to be loaded at runtime.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Let’s load this scene named DioramaAssembled at runtime."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-dioramaAssembled","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"We use entity’s asynchronous initializer to make us an entity with the contents from our Reality Composer Pro package.","type":"text"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["let entity = try await Entity(named: \"DioramaAssembled\", in: realityKitContentBundle)"]},{"inlineContent":[{"type":"text","text":"We specify which entity we want to load using its string name, and we give it the bundle that our package produces. It will throw if it can’t find anything in our Reality Composer Pro project by that name. realityKitContentBundle is a constant value that it gets autogenerated in the Reality Composer Pro package. This goes in a RealityView make closure. A RealityView is a new kind of SwiftUI view."},{"type":"text","text":"\n"},{"type":"text","text":"It’s the bridge between the worlds of SwiftUI and RealityKit."}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["RealityView { content in","\tdo {","\t\tlet entity = try await Entity(named: \"DioramaAssembled\", in: realityKitContentBundle)","\t\tcontent.add(entity)","\t} catch {","\t\t\/\/ Handle error","\t}","}"]},{"level":2,"type":"heading","text":"Anatomy of a Reality Composer Pro package","anchor":"Anatomy-of-a-Reality-Composer-Pro-package"},{"inlineContent":[{"text":"Put assets into a Swift Package, with an .rkassets directory inside it, like this.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-anatomy","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Xcode compiles the .rkassets folder into a format that’s faster to load at runtime.","type":"text"},{"text":"\n","type":"text"},{"text":"The entity we just loaded is actually the root of a larger entity hierarchy. It has child entities and they in turn have child entities. If we wanted to address one of the entities lower down in the hierarchy, we could give it a name in Reality Composer Pro, and then at runtime, we could ask the scene to find that entity by its name.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-anatomy2"}],"type":"paragraph"},{"level":2,"type":"heading","text":"Entity Component System.","anchor":"Entity-Component-System"},{"inlineContent":[{"type":"text","text":"Entities are a part of ECS, which stands for Entity Component System."}],"type":"paragraph"},{"inlineContent":[{"text":"ECS has some close parallels to object-oriented programming but is different in some key ways. In the object-oriented programming world, the object has properties which are attributes that define its nature, and it has its own functionality written in a class that defines the object.","type":"text"}],"type":"paragraph"},{"level":3,"type":"heading","text":"Entity","anchor":"Entity"},{"inlineContent":[{"type":"text","text":"In the ECS world, an entity is any thing you see in the scene. They can also be invisible. They don’t hold attributes or data though. We put our data into components instead."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-ecs"}],"type":"paragraph"},{"level":3,"type":"heading","text":"Components","anchor":"Components"},{"inlineContent":[{"type":"text","text":"Components can be added to or removed from entities at any time during app execution, which provides a way to dynamically change the nature of an entity."}],"type":"paragraph"},{"level":3,"type":"heading","text":"System","anchor":"System"},{"inlineContent":[{"text":"A system is where behavior lives. It has an update function that’s called once per frame. That’s where we put our ongoing logic. In the system, we query for all entities that have a certain component on them, or configuration of components, and then we perform some action and store the updated data back into those components.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"For a more in-depth discussion on ECS, check out:","type":"text"}],"type":"paragraph"},{"inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080"}],"type":"paragraph"},{"level":1,"type":"heading","text":"Components","anchor":"Components"},{"inlineContent":[{"type":"text","text":"To add a component to an entity in Swift, we use entity.components.set() and provide the component value."}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["let component = MyComponent()","entity.components.set(component)"]},{"inlineContent":[{"text":"To do the same in Reality Composer Pro, select the entity you want either in the viewport or in the hierarchy.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component"}],"type":"paragraph"},{"inlineContent":[{"text":"Then, at the bottom of the Inspector Panel, click the Add Component button to bring up a list of all RealityKit’s available components.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component2"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We can add as many components to an entity as we want, and we can only add one of each type; it’s a set. We’ll also see any custom components we’ve made in this list as well. Let’s create our own custom components."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We’re going to make those floating buttons that hover over specific points on our terrain. We’ll prepare a lot of that UI and functionality in code, but we will mark these entities in Reality Composer Pro."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component3","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To do this, we’re going to"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"add entities at locations above our terrain map, which will signify to the app that these are the places we want to show our floating buttons."}]}]},{"content":[{"inlineContent":[{"text":"create a point of interest component to house our information about each place.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"open the PointOfInterestComponent.swift in Xcode to edit it, adding properties like a name and a description."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"In Reality Composer Pro, we’ll add our new PointOfInterestComponent to each of our new entities"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"fill in the properties’ values."}]}]}],"type":"unorderedList"},{"inlineContent":[{"identifier":"WWDC23-10273-component4","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Let’s make our first location marker entity, Ribbon Beach, which is a place on Catalina Island. We click the plus menu and select Transform to make us a new invisible entity.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component5","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"We can name our entity Ribbon_Beach.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component6"}],"type":"paragraph"},{"inlineContent":[{"text":"Let’s put this entity where Ribbon Beach actually is on the island.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component7"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We click on the Add Component button, but this time, we select New Component because we’re going to make our own."},{"type":"text","text":" "},{"type":"text","text":"Let’s give it a name, PointOfInterest."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component8","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Now it shows up in the Inspector Panel just like our other components do."},{"type":"text","text":" "},{"type":"text","text":"But what’s this count property?"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component9","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Let’s open our new component in Xcode. In Xcode, we see that Reality Composer Pro created PointOfInterestComponent.swift for us.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Reality Composer Pro projects are Swift packages, and the Swift code we just generated lives here in the package. Looking at the template code, we see that that’s where the count property came from."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component10"}],"type":"paragraph"},{"inlineContent":[{"text":"Let’s have another property instead. We want each point of interest to know which map it’s associated with so that when you change maps, we can fade out the old points of interest and fade in the appropriate ones. So we add an enumeration property, var region.  Let’s make our enum region up here…  …and give it two cases, since we’re only building two maps right now: Catalina and Yosemite.  It can serialize as a string. We also conform it to the Codable protocol so that Reality Composer Pro can see it and serialize instances of it.","type":"text"}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["import RealityKit","","public enum Region: String, Codable {","\tcase catalina","\tcase yosemite"," }"," ","\/\/ Ensure you register this component in your app's delegate using:","\/\/ PointOfInterestComponent.registerComponent ()","public struct Point0fInterestComponent: Component, Codable { ","\tvar region: Region = .yosemite","\t","\tpublic init() {","\t}","}"]},{"inlineContent":[{"identifier":"WWDC23-10273-component11","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"Back in Reality Composer Pro, the count property has gone away and our new region property shows up. It has a default value of yosemite because that’s what we initialized in the code, but we can override it here for this particular entity.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component12","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"If we override it, this value will only take effect on this particular entity. The rest of the point of interest components will have the default value of yosemite unless we override them too."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We’re using our PointOfInterestComponent like a signifier, a marker that we stick on these entities. These entities act like placeholders for where we’ll put our SwiftUI buttons at runtime. We add our other Catalina Island points of interest the same way we just added Ribbon Beach."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The app doesn’t do anything yet. That’s because we haven’t written any code to handle these point of interest components yet."}],"type":"paragraph"},{"level":1,"anchor":"User-interface","type":"heading","text":"User interface"},{"inlineContent":[{"text":"There is a new way of putting SwiftUI content into a RealityKit scene. This is called the Attachments API. We’re going to combine attachments with our PointOfInterestComponent to create hovering buttons with custom data at runtime.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Attachments are a part of the RealityView. We first look at a simplified example:"}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["RealityView { _,_ in","\t\/\/ load entities from your Reality Composer Pro package bundle","} update: { _,_ in","\t\/\/ update your RealityKit entities","} attachments: {","\t\/\/ SwiftUI Views","}"]},{"inlineContent":[{"text":"The RealityView initializer takes three parameters: a make closure, an update closure, and an attachments ViewBuilder. We now add a bare-minimum implementation of creating an Attachment View, a green SwiftUI button, and adding it to our RealityKit scene.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"In the Attachments ViewBuilder, we make a normal SwiftUI view; we can use view modifiers and gestures and all the rest that SwiftUI gives us. We tag our View with any unique hashable. I’ve chosen to tag this button view with a fish emoji.","type":"text"}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["RealityView { _,_ in","\t\/\/ load entities from your Reality Composer Pro package bundle","} update: { _,_ in","\t\/\/ update your RealityKit entities","} attachments: {","\tButton { ... }","\t\t.background(.green)","\t\t.tag (\"🐠\")","}"]},{"inlineContent":[{"type":"text","text":"Later, when SwiftUI invokes the update closure, our button View has become an entity. It’s stored in the attachments parameter to this closure, and we get it using the tag we gave it before. We can then treat it like any other entity, add it as a child of any existing entity in our scene, or add it as a new top-level entity in the content’s entities collection."}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["RealityView { _,_ in","\t\/\/ load entities from your Reality Composer Pro package bundle","} update: { _,_ in","\tif let attachmentEntity = attachments.entity(for: \"S\") {","\t\tcontent.add(attachmentEntity)","\t}","} attachments: {","\tButton { ... }","\t\t.background(.green)","\t\t.tag (\"🐠\")","}"]},{"inlineContent":[{"text":"And since it’s become a regular entity, we can set its position so it shows up where we want in 3D, and we can add any components we want as well.","type":"text"}],"type":"paragraph"},{"level":2,"anchor":"RealityView-attachments","type":"heading","text":"RealityView attachments"},{"inlineContent":[{"type":"text","text":"Here’s how data flows from one part of the RealityView to another."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The three parameters of this RealityView initializer:"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The first is make, where the initial set-up scene from your Reality Composer Pro bundle is loaded as an entity and then added to the RealityKit scene."}]}]},{"content":[{"inlineContent":[{"text":"The second is update, which is a closure that will be called when there are changes to the view’s state. Here, we can change things about the entities, like properties in their components, their position, and even add or remove entities from the scene. This update closure is not executed every frame. It’s called whenever the SwiftUI view state changes.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The third is the attachments ViewBuilder. This is where we can make SwiftUI views to put into your RealityKit scene.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component13"}],"type":"paragraph"},{"inlineContent":[{"text":"SwiftUI views start out in the attachments ViewBuilder, then they are delivered in the update closure in the attachments parameter.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component14"}],"type":"paragraph"},{"inlineContent":[{"text":"Here, asking the attachments parameter if it has an entity using the same tag we gave to the button in the attachments ViewBuilder.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component15"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"If there is one, we get a RealityKit entity. In the update closure, we set its 3D position and add it to your RealityKit scene so we can see it floating in space wherever we want."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Here, we’ve added the button entity as a child of a sphere entity positioned 0.2 meters above its parent."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component16","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The make closure also has an attachments parameter. This one is for adding any attachments that we have ready to go at the time this view is first evaluated, because the make closure is only run once."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component17","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"This is the general flow of a RealityView, let’s check the update closure. The parameter to make and update closures is a RealityKitContent. When adding an entity to the RealityKit content, it becomes a top-level entity in the scene.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"Likewise, from the update function, adding an entity to the content gives a new top-level entity in the scene. While the make closure will only be called once, the update closure will be called more than once. If we create a new entity in the update closure and add it to the content there, we’ll get duplicates of that entity. (not good.)","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component18"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To guard against that, we should only add entities to content that are created somewhere that’s only run once. We don’t need to check if the content.entities already contains the entity."}],"type":"paragraph"},{"inlineContent":[{"text":"It’s a no-op if we call add with the same entity twice, like a set. It’s the same when parent an entity to an existing entity in the scene – it won’t be added twice.","type":"text"}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["let myEntity = Entity()","","RealityView { content, in","\tif let entity = try? await Entity(named: \"MyScene\", in: realityKitContentBundle){","\t\tcontent.add(entity)","\t}","} update: { content, attachments in","\t","\tif let attachmentEntity = attachments.entity(for: \"🐠\") {","\t\tcontent.add(attachmentEntity)","\t}\t","\t","\tcontent.add(myEntity)","","} attachments: {","\tButton { ... }","\t\t.background(.green)","\t\t.tag(\"🐠\")","}"]},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component19"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Attachment entities are not created by us; they’re created by the RealityView for each attachment view that we provide in your attachments ViewBuilder. That means it’s safe to add them to the content in our update closure without checking if it’s already there."}],"type":"paragraph"},{"level":2,"anchor":"Data-driven","type":"heading","text":"Data-driven"},{"inlineContent":[{"text":"So, that was how we’d write code if we wanted to hardcode our points of interest in the attachments ViewBuilder. But let’s make it more flexible.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"To make it data-driven, we need the code to read the data that we set up in the Reality Composer Pro scene. We’ll be creating our attachment views dynamically."}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Add invisible entities","type":"text"},{"text":" ","type":"text"},{"text":"In Reality Composer Pro, we already set up our placeholder entity for Ribbon Beach, and we’ll do the same for the other points of interest that we want to highlight in our diorama. We’ll fill out all the info each one needs, like their name and which map they belong on.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Query for entities"},{"type":"text","text":" "},{"type":"text","text":"Now, in code, we’ll query for those entities"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Create a SwiftUI View for each"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Store Views in @State collection"},{"type":"text","text":" "},{"type":"text","text":"In order to get SwiftUI to invoke our attachments ViewBuilder every time we add a new button to our collection, we’ll add the @State property wrapper to this collection."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"We’ll serve those buttons up to the attachments ViewBuilder.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Add them as entities in the update closure of our RealityView, We’ll add each one as a child of the marker entities we set up in Reality Composer Pro.","type":"text"}]}]}],"type":"unorderedList"},{"inlineContent":[{"type":"text","text":"We’re making use of the Transform Component here, which all entities have by default."}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component21"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Then we add our PointOfInterestComponent to each of them. In our code, we get references to these entities by querying for all entities in the scene that have the PointOfInterestComponent on them. The query returns the three invisible entities we set up in Reality Composer Pro."}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component22","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"We create a new SwiftUI view for each one and store them in a collection.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC23-10273-component23"}],"type":"paragraph"},{"inlineContent":[{"text":"To get our buttons into our RealityView, we’ll make use of the SwiftUI view-updating flow. This means adding the property wrapper @State to the collection of buttons in our View. The @State property wrapper tells SwiftUI that when we add items to this collection, SwiftUI should trigger a view update on our ImmersiveView. That will cause SwiftUI to evaluate our attachments ViewBuilder and our update closure again.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10273-component24","type":"image"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"The RealityView’s attachments ViewBuilder is where we’ll declare to SwiftUI that we want these buttons to be made into entities. Our RealityView’s update closure will be called next, and our buttons will be delivered to us as entities.   They’re no longer SwiftUI Views now. That’s why we can add them to our entity hierarchy."}],"type":"paragraph"},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component25","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"In the update closure, we add our attachment entities to the scene, positioned floating above each of our invisible entities. Now they will show up visually when we look at our diorama scene.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component26"}]},{"type":"paragraph","inlineContent":[{"text":"Let’s see how each of these steps is done.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"First, we mark our invisible entities in our Reality Composer Pro scene.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component27"}]},{"type":"paragraph","inlineContent":[{"text":"To find our entities that we marked, we’ll make an EntityQuery. We’ll use it to ask for all entities that have a PointOfInterestComponent on them.","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["static let markersQuery = EntityQuery(where: .has (PointOfInterestComponent.self))"]},{"type":"paragraph","inlineContent":[{"text":"We’ll then iterate through our QueryResult and create a new SwiftUI View for each entity in our scene that has a PointOfInterestComponent on it. We’ll fill it in with information we grab from the component, the data we entered in Reality Composer Pro.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"That view is going to be one of our attachments, so we put a tag on it. We’ll use an ObjectIdentifier rather than a fish emoji. Here’s the part where we make our collection of SwiftUI Views. We’ll call it attachmentsProvider since it will provide our attachments to the RealityView’s attachments ViewBuilder. We’ll then store our view in the attachmentsProvider."}]},{"type":"codeListing","syntax":"swift","code":["static let markersQuery = EntityQuery(where: .has (PointOfInterestComponent.self))","@State var attachmentsProvider = AttachmentsProvider ()","","rootEntity.scene?.performQuery(Self.markersQuery).forEach { entity in","\tguard let pointOfInterest = entity.components[PointOfInterestComponent.self] else { return }","\t","\tlet attachmentTag: ObjectIdentifier = entity.id","\t","\tlet view = LearnMoreView(name: pointOfInterest.name, description: pointOfInterest.description)","\t\t.tag(attachmentTag)","\t","\tattachmentsProvider.attachments[attachmentTag]= AnyView(view)"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s take a look at that collection type."}]},{"type":"paragraph","inlineContent":[{"text":"AttachmentsProvider has a dictionary of attachment tags to views. We type-erased our view so we can put other kinds of views in there besides our LearnMoreView. We have a computed property called sortedTagViewPairs that returns an array of tuples – tags and their corresponding views – in the same order every time. Then, in the attachments ViewBuilder, we’ll ForEach over our collection of attachments that we made.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This tells SwiftUI that we want one view for each of the pairs we’ve given it, and we provide our views from our collection. We’re letting the ObjectIdentifier do double duty here as both an attachment tag for the view, and as an identifier for the ForEach structure."}]},{"type":"codeListing","syntax":"swift","code":["@Observable final class AttachmentsProvider {","\tvar attachments: [ObjectIdentifier: AnyView] = [:]","\tvar sortedTagViewPairs: [(tag: ObjectIdentifier, view: AnyView)] { ... }","}","","...","","@State var attachmentsProvider = AttachmentsProvider()","","RealityView { _, _ in","","} update: { _, _ in","","} attachments: {","\tForEach(attachmentsProvider.sortedTagViewPairs, id: \\.tag) { pair in","\t\tpair.view","\t}","}"]},{"type":"paragraph","inlineContent":[{"text":"why didn’t we just add a tag property to our PointOfInterestComponent instead?","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component28","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"Attachment tags need to be unique, both for the ForEach struct and the attachments mechanism to work. And since all the properties in our custom component will be shown in Reality Composer Pro’s Inspector Panel when we add the component to an entity, that means the attachmentTag would show up there too.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Entities conform to the Identifiable protocol, so they have identifiers that are unique automatically. We can get this identifier at runtime from the entity.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"To have the attachmentTag property not show up in Reality Composer Pro, we use a technique called “design-time versus runtime components.”","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We’ll separate our data into two different components, one for design-time data that we want to arrange in Reality Composer Pro, and one for runtime data that we will attach to those same entities dynamically at runtime."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This is for properties that we don’t want to show in our Inspector Panel in Reality Composer Pro. So we’ll define a new component, PointOfInterestRuntimeComponent, and move our attachment tag inside it."}]},{"type":"codeListing","syntax":"swift","code":["\/\/ Design-time component","public struct PointOfInterestComponent: Component, Codable {","\tpublic var region: Region = .yosemite","\tpublic var name: String = \"Ribbon Beach\"","\tpublic var description: String?","}","","\/\/ Run-time component","public struct PointOfInterestRuntimeComponent: Component {","\tpublic let attachmentTag: ObjectIdentifier","}"]},{"type":"heading","level":2,"anchor":"Custom-components-in-Reality-Composer-Pro","text":"Custom components in Reality Composer Pro"},{"type":"paragraph","inlineContent":[{"type":"text","text":"So we’ll define a new component, PointOfInterestRuntimeComponent, and move our attachment tag inside it. Reality Composer Pro automatically builds the component UI for you based on what it reads in your Swift package."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"It inspects the Swift code in your package and makes any codable components it finds available for you to use in your scenes. Here we’re showing four components. Components A and B are in our Xcode project, but they are not inside the Reality Composer Pro package, so they won’t be available for you to attach to your entities in Reality Composer Pro."}]},{"type":"paragraph","inlineContent":[{"text":"Component C is inside the package but it is not codable, so Reality Composer Pro will ignore it. Of the four components shown here, only Component D will be shown in the list in Reality Composer Pro because it is within the Swift package and it is a codable component.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"That one is our design-time component, while all the others may be used as runtime components.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component29"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Design-time components are for housing simpler data, such as ints, strings, and SIMD values, things that 3D artists and designers will make use of."}]},{"type":"paragraph","inlineContent":[{"text":"We will see an error in your Xcode project if you add a property to your custom component that’s of a type that Reality Composer Pro won’t serialize.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We’ll first add our PointOfInterest runtime component to our entity and then use the runtime component to help us match up our attachment entities with their corresponding points of interest on the diorama."}]},{"type":"paragraph","inlineContent":[{"text":"Here’s where our runtime component comes in. We’re reading in our PointOfInterest entities and creating our attachment views. We queried for all our design-time components, and now we’ll make a new corresponding runtime component for each of them. We store our attachmentTag in our runtime component, and we store our runtime component on that same entity. In this way, the design-time component is like a signifier. It tells our app that it wants an attachment made for it.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The runtime component handles any other kinds of data we need during app execution, but don’t want to store in the design-time component."}]},{"type":"codeListing","syntax":"swift","code":["static let markersQuery = EntityQuery(where: .has(PointOfInterestComponent.self))","@State var attachmentsProvider = AttachmentsProvider()","","rootEntity.scene?.performQuery(Self.markersQuery).forEach { entity in","\tguard let pointOfInterest = entity.components[PointOfInterestComponent.self] else { return }","","\tlet attachmentTag: ObjectIdentifier = entity.id","","\tlet view = LearnMoreView(name: pointOfInterest.name, description: pointOfInterest.description)","\t\t.tag(attachmentTag)","\t\t","\tattachmentsProvider.attachments[attachmentTag] = AnyView(view)","\tlet runtimeComponent = PointOfInterestRuntimeComponent(attachmentTag: attachmentTag)","\tentity.components.set(runtimeComponent)","}"]},{"type":"paragraph","inlineContent":[{"text":"In our RealityView, we have one more step before we see our attachment entities show up in our scene. Once we’ve provided our SwiftUI Views in the attachments ViewBuilder, SwiftUI will call our RealityView’s update closure and give us our attachments as RealityKit entities.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"But if we just add them to the content without positioning them, they’ll all show up sitting at the origin of the scene, position 0, 0, 0."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"That’s not where we want them. We want them to float above each point of interest on the terrain. We need to match up our attachment entities with our invisible point of interest entities that we set up in Reality Composer Pro."}]},{"type":"paragraph","inlineContent":[{"text":"The runtime component we put on the invisible entity has our tag in it. That’s how we’ll match up which attachmentEntity goes with each point of interest entity. We query for all our PointOf InterestRuntimeComponents, we get that runtime component from each entity returned by the query, then we use the component’s attachmentTag property to get our attachmentEntity from the attachments parameter to the update closure.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now we add our attachmentEntity to the content and position it half a meter above the point of interest entity."}]},{"type":"codeListing","syntax":"swift","code":["static let runtimeQuery = EntityQuery(where: .has(PointOfInterestRuntimeComponent.self))","","RealityView { _, _ in","","} update: { content, attachments in","\t","\trootEntity.scene?.performQuery(Self.runtimeQuery).forEach { entity in","\t\tguard let component = entity. components[Point0fInterestRuntimeComponent.self],","\t\t\tlet attachmentEntity = attachments.entity(for: component.attachmentTag) else {","\t\t\t\treturn","\t\t\t} content.add(attachmentEntity)","\t\t\tattachmentEntity.setPosition([O, 0.5, 0], relativeTo: entity)","\t}","} attachments: {","\tForEach(attachmentsProvider.sortedTagViewPairs,id:\\.tag) { pair in","\t\tpair.view","\t}\t","}"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s run our app again…"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component30","type":"image"}]},{"type":"heading","level":1,"anchor":"Play-audio","text":"Play audio"},{"type":"paragraph","inlineContent":[{"text":"To set up something that plays audio in Reality Composer Pro, we can bring in an audio entity by clicking the plus button, selecting Audio, and then selecting Ambient Audio.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-audio","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This creates a regular invisible entity with an AmbientAudioComponent on it. Let’s name it OceanEmitter. It will be used to play ocean sounds for Catalina Island."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-audio2","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"You need to add an audio file."}]},{"type":"paragraph","inlineContent":[{"text":"We can preview audio component by selecting a sound in the Preview menu of the component in the Inspector Panel, but this won’t automatically play the selected sound when the entity is loaded in the app.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"For that, we need to load the audio resource and tell it to play. To play this sound, we’ll get a reference to the entity that we put the audio component on. We’ve named our entity OceanEmitter, so we’ll find our entity by that name. We load the sound file using the AudioFileResource initializer, passing it the full path to the audio file resource prim in our scene. We give it the name of the .usda file that contains it in our Reality Composer Pro project. In our case, that’s our main scene named DioramaAssembled.usda."}]},{"type":"paragraph","inlineContent":[{"text":"We create an audioPlaybackController by calling entity.prepareAudio so we can play, pause, and stop this sound.","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["func playOceanSound() {","","\tguard let entity = entity.findEntity(named: \"OceanEmitter\"),","\t\tlet resource = try? AudioFileResource(named: \"\/Root\/Resources\/Ocean_Sounds_wav\", ","\t\t\tfrom: \"DioramaAssembled.usda\",","\t\t\tin: RealityContent.realityContentBundle) else { return }","","\tlet audioPlaybackController = entity.prepareAudio(resource)","\taudioPlaybackController.play ()"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now, we’re going to crossfade between two audio sources. We add a forest audio emitter the same way we added our ocean emitter entity."}]},{"type":"heading","level":1,"anchor":"Material-properties","text":"Material properties"},{"type":"paragraph","inlineContent":[{"type":"text","text":"How we’re morphing our terrain using the slider, and including audio in this transition. We’ll use a property from our Shader Graph material to morph between the two terrains. Let’s see how we do that."}]},{"type":"paragraph","inlineContent":[{"text":"In the other session, the Apple engineer Niels, created a geometry modifier using Reality Composer Pro’s Shader Graph. We can hook it up to our scene and drive some of the parameters at runtime. We want to connect this Shader Graph material with a slider. To do that, we need to promote an input node. Command-click on the node and select Promote.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-materials","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This tells the project that we intend to supply data at runtime to this part of the material. We’ll name this promoted node Progress, so we can address it by that name at runtime."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-materials2"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We can now dynamically change this value in code. We get a reference to the entity that our material is on. Then we get its ModelComponent, which is the RealityKit component that houses materials. From the ModelComponent, we get its first material."}]},{"type":"paragraph","inlineContent":[{"text":"There’s only one on this particular entity. We cast it to type ShaderGraphMaterial. Now, we can set a new value for our parameter with name Progress. Finally, we store the material back onto the ModelComponent, and the ModelComponent back onto the terrain entity.","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["guard let terrain = rootEntity.findEntity(named: \"DioramaTerrain\"),","\tvar modelComponent = terrain.components[ModelComponent.self,","\tvar shaderGraphMaterial = modelComponent.materials.first","\tas? ShaderGraphMaterial else { return }","do {","\ttry shaderGraphMaterial.setParameter(name: \"Progress\", value: .float(sliderValue))","\tmodelComponent.materials = [shaderGraphMaterial]","\tterrain.components.set(modelComponent)","} catch { }"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now we’ll hook that up to our SwiftUI slider. Whenever the slider’s value changes, we’ll grab that value, which is in the range 0 to 1, and feed it into our ShaderGraphMaterial."}]},{"type":"codeListing","syntax":"swift","code":["@State private var sliderValue: Float = 0.0","","Slider (value: $sliderValue, in: (0.0)... (1.0)) ","\t.onChange (of: sliderValue) { _, _ in","\t\tguard let terrain = rootEntity.findEntity(named: \"DioramaTerrain\"),","\t\t\tvar modelComponent = terrain.components[ModelComponent.self,","\t\t\tvar shaderGraphMaterial = modelComponent.materials.first","\t\t\tas? ShaderGraphMaterial else { return }","\t\tdo {","\t\t\ttry shaderGraphMaterial.setParameter(name: \"Progress\", value: \t.float(sliderValue))","\t\t\tmodelComponent.materials = [shaderGraphMaterial]","\t\t\tterrain.components.set(modelComponent)","\t\t} catch { }","\t}","}"]},{"type":"paragraph","inlineContent":[{"text":"Crossfading between the ambient audio tracks for the two terrains.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Because we’ve also put an AmbientAudioComponent on our two audio entities, ocean and forest, we can adjust how loud the sound plays using the gain property on them.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"We’ll query for all entities – all two at this point, our ocean and our forest – that have the AmbientAudioComponent on them. Plus, we added another custom component called RegionSpecificComponent so we can mark the entities that go with one region or the other. We get a mutable copy of our audioComponent because we’re going to change it and store it back onto our entity.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We call a function that calculates what the gain should be given a region and a sliderValue. We set the gain value onto the AmbientAudioComponent, and then we store the component back onto the entity."}]},{"type":"codeListing","syntax":"swift","code":["@State private var sliderValue: Float = 0.0","static let audioQuery = EntityQuery(where: .has(RegionSpecificComponent.self)","\t&& .has (AmbientAudioComponent.self))","","Slider (value: $sliderValue, in: (0.0)... (1.0)) ","\t.onChange (of: sliderValue) { _, _ in","","\/\/ ... Change the terrain material property ...","","\t\trootEntity?.scene?.performQuery(Self.audioQuery).forEach({audioEmitter in","\t\t\tguard var audioComponent = audioEmitter.components[AmbientAudioComponent.self],","\t\t\t\tlet regionComponent = audioEmitter.components[RegionSpecificComponent.self]","\t\t\telse { return }","\t\t\t","\t\t\tlet gain = regionComponent.region.gain(forSliderValue: sliderValue)","\t\t\taudioComponent.gain = gain","\t\t\taudioEmitter.components.set(audioComponent)","\t\t})","\t}","}"]},{"type":"paragraph","inlineContent":[{"text":"In action:","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Moving the slider, we can see our Shader Graph material changing the geometry of the terrain map, and we can hear the forest sound fading out and the ocean sound coming in."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-audio3","type":"image"}]},{"type":"heading","level":2,"anchor":"Wrap-Up","text":"Wrap Up"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Use Reality Composer Pro to prepare RealityKit content"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Place attachment placeholders in Reality Composer Pro","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Load and play audio"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Drive material properties"}],"type":"paragraph"}]}]},{"type":"heading","level":1,"anchor":"Resources","text":"Resources"},{"type":"paragraph","inlineContent":[{"type":"reference","identifier":"https:\/\/developer.apple.com\/forums\/create\/question?&tag1=740030&tag2=266&tag3=796030","isActive":true},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10273","isActive":true}]},{"type":"heading","level":1,"anchor":"Check-out-also","text":"Check out also"},{"type":"paragraph","inlineContent":[{"isActive":true,"overridingTitle":"Build spatial experiences with RealityKit - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080","overridingTitleInlineContent":[{"text":"Build spatial experiences with RealityKit - WWDC23","type":"text"}],"type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10203","type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10081","type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"overridingTitle":"Explore materials in Reality Composer Pro - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202","overridingTitleInlineContent":[{"text":"Explore materials in Reality Composer Pro - WWDC23","type":"text"}],"type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"overridingTitle":"Meet Reality Composer Pro - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083","overridingTitleInlineContent":[{"text":"Meet Reality Composer Pro - WWDC23","type":"text"}],"type":"reference"},{"type":"text","text":""},{"type":"text","text":"\n"},{"isActive":true,"overridingTitle":"Dive into RealityKit 2 - WWDC21","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074","overridingTitleInlineContent":[{"text":"Dive into RealityKit 2 - WWDC21","type":"text"}],"type":"reference"}]},{"type":"heading","level":2,"anchor":"Written-By","text":"Written By"},{"type":"row","numberOfColumns":5,"columns":[{"size":1,"content":[{"inlineContent":[{"type":"image","identifier":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4"}],"type":"paragraph"}]},{"size":4,"content":[{"text":"laurent b","level":3,"anchor":"laurent-b","type":"heading"},{"inlineContent":[{"overridingTitle":"Contributed Notes","isActive":true,"overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/x.com\/wrmultitudes"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/laurentbrusa.hashnode.dev\/"}],"type":"paragraph"}]}]},{"type":"heading","level":2,"anchor":"Related-Sessions","text":"Related Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10080-Build-spatial-experiences-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10203-Develop-your-first-immersive-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10074-Dive-into-RealityKit-2"],"style":"list"},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}]},{"type":"small","inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}],"kind":"content"}],"sampleCodeDownload":{"action":{"isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc23\/10273","type":"reference","overridingTitle":"Watch Video (34 min)"},"kind":"sampleDownload"},"sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10273-Work-with-Reality-Composer-Pro-content-in-Xcode"},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10172-Mix-Swift-and-C++":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10172-Mix-Swift-and-C++","title":"Mix Swift and C++","url":"\/documentation\/wwdcnotes\/wwdc23-10172-mix-swift-and-c++","kind":"article","abstract":[{"type":"text","text":"Learn how you can use Swift in your C++ and Objective-C++ projects to make your code safer, faster, and easier to develop. We’ll show you how to use C++ and Swift APIs to incrementally incorporate Swift into your app."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10256-Discover-Continuity-Camera-for-tvOS":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10256-Discover-Continuity-Camera-for-tvOS","title":"Discover Continuity Camera for tvOS","url":"\/documentation\/wwdcnotes\/wwdc23-10256-discover-continuity-camera-for-tvos","kind":"article","abstract":[{"type":"text","text":"Discover how you can bring AVFoundation, AVFAudio, and AudioToolbox to your apps on tvOS and create camera and microphone experiences for the living room. Find out how to support tvOS in your existing iOS camera experience with the Device Discovery API, build apps that use iPhone as a webcam or FaceTime source, and explore special considerations when developing for tvOS. We’ll also show you how to enable audio recording for tvOS, and how to use echo cancellation to create great voice-driven experiences."}]},"WWDC23-10273-component":{"identifier":"WWDC23-10273-component","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10193-Design-Shortcuts-for-Spotlight":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10193-Design-Shortcuts-for-Spotlight","role":"sampleCode","abstract":[{"type":"text","text":"Learn about the latest updates to the visual language of App Shortcuts and find out how to design your shortcut to appear as a top hit in Spotlight. We’ll share how shortcuts can appear on iOS or iPadOS, and show you how to customize the visual appearance of a shortcut, personalize its order, select its correct behavior, and increase discoverability."}],"title":"Design Shortcuts for Spotlight","url":"\/documentation\/wwdcnotes\/wwdc23-10193-design-shortcuts-for-spotlight","kind":"article","type":"topic"},"WWDC23-10273-component18":{"identifier":"WWDC23-10273-component18","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component18.jpg"}]},"https://avatars.githubusercontent.com/u/29355828?v=4":{"identifier":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4","type":"image","alt":"Profile image of laurent b","variants":[{"traits":["1x","light"],"url":"https:\/\/avatars.githubusercontent.com\/u\/29355828?v=4"}]},"WWDCNotes.png":{"identifier":"WWDCNotes.png","type":"image","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10224-Simplify-distribution-in-Xcode-and-Xcode-Cloud":{"role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc23-10224-simplify-distribution-in-xcode-and-xcode-cloud","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10224-Simplify-distribution-in-Xcode-and-Xcode-Cloud","title":"Simplify distribution in Xcode and Xcode Cloud","abstract":[{"text":"Discover how to share your app using Xcode’s streamlined distribution, which allows you to submit your app to TestFlight or the App Store with one click. We’ll also show you how to use Xcode Cloud to simplify your distribution process by automatically including notes for testers in TestFlight, and use post-action to automatically notarize your Mac apps.","type":"text"}],"type":"topic"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=2005":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=2005","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=2005","title":"33:25 - Wrap-up","type":"link","titleInlineContent":[{"type":"text","text":"33:25 - Wrap-up"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","title":"Integrate with motorized iPhone stands using DockKit","url":"\/documentation\/wwdcnotes\/wwdc23-10304-integrate-with-motorized-iphone-stands-using-dockkit","kind":"article","abstract":[{"type":"text","text":"Discover how you can create incredible photo and video experiences in your camera app when integrating with DockKit-compatible motorized stands. We’ll show how your app can automatically track subjects in live video across a 360-degree field of view, take direct control of the stand to customize framing, directly control the motors, and provide your own inference model for tracking other objects. Finally, we’ll demonstrate how to create a sense of emotion through dynamic device animations."}]},"WWDC23-10273-component4":{"identifier":"WWDC23-10273-component4","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component4.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10138-Design-and-build-apps-for-watchOS-10":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10138-Design-and-build-apps-for-watchOS-10","title":"Design and build apps for watchOS 10","url":"\/documentation\/wwdcnotes\/wwdc23-10138-design-and-build-apps-for-watchos-10","kind":"article","abstract":[{"type":"text","text":"Dive into the details of watchOS design principles and learn how to apply them in your app using SwiftUI. We’ll show you how to build an app for the redesigned user interface to surface timely information, communicate focused content at a glance, and make navigation consistent and predictable."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10093-Bring-your-Unity-VR-app-to-a-fully-immersive-space":{"url":"\/documentation\/wwdcnotes\/wwdc23-10093-bring-your-unity-vr-app-to-a-fully-immersive-space","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10093-Bring-your-Unity-VR-app-to-a-fully-immersive-space","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Discover how you can bring your existing Unity VR apps and games to visionOS. We’ll explore workflows that can help you get started and show you how to build for eyes and hands in your apps and games with the Unity Input System. Learn about Unity’s XR Interaction Toolkit, tips for foveated rendering, and best practices."}],"title":"Bring your Unity VR app to a fully immersive space"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10094-Enhance-your-iPad-and-iPhone-apps-for-the-Shared-Space":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10094-Enhance-your-iPad-and-iPhone-apps-for-the-Shared-Space","title":"Enhance your iPad and iPhone apps for the Shared Space","url":"\/documentation\/wwdcnotes\/wwdc23-10094-enhance-your-ipad-and-iphone-apps-for-the-shared-space","kind":"article","abstract":[{"type":"text","text":"Get ready to enhance your iPad and iPhone apps for the Shared Space! We’ll show you how to optimize your experience to make it feel great on visionOS and explore Designed for iPad app interaction, visual treatments, and media."}]},"https://developer.apple.com/videos/play/wwdc2023/10203":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10203","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10203","title":"Develop your first immersive app - WWDC23","type":"link","titleInlineContent":[{"type":"text","text":"Develop your first immersive app - WWDC23"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10027-Bring-widgets-to-new-places":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10027-Bring-widgets-to-new-places","title":"Bring widgets to new places","url":"\/documentation\/wwdcnotes\/wwdc23-10027-bring-widgets-to-new-places","kind":"article","abstract":[{"type":"text","text":"The widget ecosystem is expanding: Discover how you can use the latest WidgetKit APIs to make your widget look great everywhere. We’ll show you how to identify your widget’s background, adjust layout dynamically, and prepare colors for vibrant rendering so that your widget can sit seamlessly in any environment."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10080-Build-spatial-experiences-with-RealityKit":{"type":"topic","abstract":[{"type":"text","text":"Discover how RealityKit can bring your apps into a new dimension. Get started with RealityKit entities, components, and systems, and learn how you can add 3D models and effects to your app on visionOS. We’ll also take you through the RealityView API and demonstrate how to add 3D objects to windows, volumes, and spaces to make your apps more immersive. And we’ll explore combining RealityKit with spatial input, animation, and spatial audio."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10080-Build-spatial-experiences-with-RealityKit","kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10080-build-spatial-experiences-with-realitykit","title":"Build spatial experiences with RealityKit"},"WWDC23-10273-component21":{"identifier":"WWDC23-10273-component21","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component21.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10100-Optimize-app-power-and-performance-for-spatial-computing":{"url":"\/documentation\/wwdcnotes\/wwdc23-10100-optimize-app-power-and-performance-for-spatial-computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10100-Optimize-app-power-and-performance-for-spatial-computing","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Learn how you can create powerful apps and games for visionOS by optimizing for performance and efficiency. We’ll cover the unique power characteristics of the platform, explore building a performance plan, and share some of the tools and strategies to test and optimize your apps."}],"title":"Optimize app power and performance for spatial computing"},"WWDC23-10273-component25":{"identifier":"WWDC23-10273-component25","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component25.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10090-Run-your-iPad-and-iPhone-apps-in-the-Shared-Space":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10090-Run-your-iPad-and-iPhone-apps-in-the-Shared-Space","title":"Run your iPad and iPhone apps in the Shared Space","url":"\/documentation\/wwdcnotes\/wwdc23-10090-run-your-ipad-and-iphone-apps-in-the-shared-space","kind":"article","abstract":[{"type":"text","text":"Discover how you can run your existing iPad and iPhone apps on Vision Pro. Learn how iPadOS and iOS apps operate on this platform, find out about the Designed for iPad experience, and explore the paths available for enhancing your app experience on visionOS."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10047-Use-Core-ML-Tools-for-machine-learning-model-compression","title":"Use Core ML Tools for machine learning model compression","url":"\/documentation\/wwdcnotes\/wwdc23-10047-use-core-ml-tools-for-machine-learning-model-compression","kind":"article","abstract":[{"type":"text","text":"Discover how to reduce the footprint of machine learning models in your app with Core ML Tools. Learn how to use techniques like palettization, pruning, and quantization to dramatically reduce model size while still achieving great accuracy. Explore comparisons between compression during the training stages and on fully trained models, and learn how compressed models can run even faster when your app takes full advantage of the Apple Neural Engine."}]},"WWDC23-10273-component11":{"identifier":"WWDC23-10273-component11","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component11.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10166-Write-Swift-macros":{"url":"\/documentation\/wwdcnotes\/wwdc23-10166-write-swift-macros","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10166-Write-Swift-macros","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Discover how you can use Swift macros to make your codebase more expressive and easier to read. Code along as we explore how macros can help you avoid writing repetitive code and find out how to use them in your app. We’ll share the building blocks of a macro, show you how to test it, and take you through how you can emit compilation errors from macros."}],"title":"Write Swift macros"},"WWDC23-10273-component19":{"identifier":"WWDC23-10273-component19","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component19.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10157-Wind-your-way-through-advanced-animations-in-SwiftUI":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10157-Wind-your-way-through-advanced-animations-in-SwiftUI","title":"Wind your way through advanced animations in SwiftUI","url":"\/documentation\/wwdcnotes\/wwdc23-10157-wind-your-way-through-advanced-animations-in-swiftui","kind":"article","abstract":[{"text":"Discover how you can take animation to the next level with the latest updates to SwiftUI. Join us as we wind our way through animation and build out multiple steps, use keyframes to add coordinated multi-track animated effects, and combine APIs in unique ways to make your app spring to life.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10110-Elevate-your-windowed-app-for-spatial-computing":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10110-Elevate-your-windowed-app-for-spatial-computing","title":"Elevate your windowed app for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-10110-elevate-your-windowed-app-for-spatial-computing","kind":"article","abstract":[{"type":"text","text":"Discover how you can bring your multiplatform SwiftUI app to visionOS and the Shared Space. We’ll show you how to add the visionOS destination to an existing app and view your app in the Simulator. Explore how your SwiftUI code automatically adapts to support the unique context and presentation of the visionOS platform. Learn how you can update custom views, improve your app’s UI, and add features and controls specific to this platform."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10254-Do-more-with-Managed-Apple-IDs":{"url":"\/documentation\/wwdcnotes\/wwdc23-10254-do-more-with-managed-apple-ids","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10254-Do-more-with-Managed-Apple-IDs","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Explore the latest updates to Managed Apple IDs and learn how you can use them in your organization. Take advantage of additional apps and services available to Managed Apple IDs, discover the Account-Driven Device Enrollment flow, and find out how to use access management controls to limit the devices and Apple services that Managed Apple IDs can access. We’ll also show you how to federate with your identity provider to automate creation and sync with your directory."}],"title":"Do more with Managed Apple IDs"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10233-Enhance-your-apps-audio-experience-with-AirPods","title":"Enhance your app’s audio experience with AirPods","url":"\/documentation\/wwdcnotes\/wwdc23-10233-enhance-your-apps-audio-experience-with-airpods","kind":"article","abstract":[{"type":"text","text":"Discover how you can create transformative audio experiences in your app using AirPods. Learn how to incorporate AirPods Automatic Switching, use AVAudioApplication to support Mute Control, and take advantage of Spatial Audio to create immersive soundscapes in your app or game."}]},"WWDC23-10273-component26":{"identifier":"WWDC23-10273-component26","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component26.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14, tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"SwiftData"},{"type":"text","text":", "},{"type":"codeVoice","code":"Observation"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit"},{"type":"text","text":" views, and more."}],"role":"collectionGroup","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23","title":"WWDC23"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10262-Rediscover-Safari-developer-features":{"url":"\/documentation\/wwdcnotes\/wwdc23-10262-rediscover-safari-developer-features","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10262-Rediscover-Safari-developer-features","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Get ready to explore Safari’s rich set of tools for web developers and designers. Learn how you can inspect web content, find out about Responsive Design Mode and WebDriver, and get started with simulators and devices. We’ll also show you how to pair with Vision Pro, make content inspectable in your apps, and use Open with Simulator in Responsive Design Mode to help you test your websites on any device."}],"title":"Rediscover Safari developer features"},"WWDC23-10273-component28":{"identifier":"WWDC23-10273-component28","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component28.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10309-Design-widgets-for-the-Smart-Stack-on-Apple-Watch":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10309-Design-widgets-for-the-Smart-Stack-on-Apple-Watch","title":"Design widgets for the Smart Stack on Apple Watch","url":"\/documentation\/wwdcnotes\/wwdc23-10309-design-widgets-for-the-smart-stack-on-apple-watch","kind":"article","abstract":[{"type":"text","text":"Bring your widgets to watchOS with the new Smart Stack. We’ll show you how to use standard design layouts, color and iconography, and signal-based relevancy to ensure your app’s widgets are glanceable, distinctive and smart."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10155-Discover-String-Catalogs":{"type":"topic","abstract":[{"type":"text","text":"Discover how Xcode 15 makes it easy to localize your app by managing all of your strings in one place. We’ll show you how to extract, edit, export, and build strings in your project using String Catalogs. We’ll also share how you can adopt String Catalogs in existing projects at your own pace by choosing which files to migrate."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10155-Discover-String-Catalogs","kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10155-discover-string-catalogs","title":"Discover String Catalogs"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10002-Ready-set-relay-Protect-app-traffic-with-network-relays":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10002-Ready-set-relay-Protect-app-traffic-with-network-relays","title":"Ready, set, relay: Protect app traffic with network relays","url":"\/documentation\/wwdcnotes\/wwdc23-10002-ready-set-relay-protect-app-traffic-with-network-relays","kind":"article","abstract":[{"type":"text","text":"Learn how relays can make your app’s network traffic more private and secure without the overhead of a VPN. We’ll show you how to integrate relay servers in your own app and explore how enterprise networks can use relays to securely access internal resources."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10158-Animate-with-springs":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10158-Animate-with-springs","title":"Animate with springs","url":"\/documentation\/wwdcnotes\/wwdc23-10158-animate-with-springs","kind":"article","abstract":[{"type":"text","text":"Discover how you can bring life to your app with animation! We’ll show you how to create amazing animations when you take advantage of springs and help you learn how to use them in your app."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10086-Explore-the-USD-ecosystem":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10086-Explore-the-USD-ecosystem","title":"Explore the USD ecosystem","url":"\/documentation\/wwdcnotes\/wwdc23-10086-explore-the-usd-ecosystem","kind":"article","abstract":[{"type":"text","text":"Discover the latest updates to Universal Scene Description (USD) on Apple platforms and learn how you can deliver great 3D content for your apps, games, and websites. Get to know USD for visionOS, explore MaterialX shaders and color management, and find out about some of the other improvements to the USD ecosystem."}]},"WWDC23-10273-materials2":{"identifier":"WWDC23-10273-materials2","type":"image","alt":"Material properties","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-materials2.jpg"}]},"WWDC23-10273-audio3":{"identifier":"WWDC23-10273-audio3","type":"image","alt":"Material properties","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-audio3.jpg"}]},"WWDC23-10273-component14":{"identifier":"WWDC23-10273-component14","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component14.jpg"}]},"https://developer.apple.com/videos/play/wwdc2023/10081":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10081","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10081","title":"Enhance your spatial computing app with RealityKit - WWDC23","type":"link","titleInlineContent":[{"type":"text","text":"Enhance your spatial computing app with RealityKit - WWDC23"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10006-Build-robust-and-resumable-file-transfers":{"url":"\/documentation\/wwdcnotes\/wwdc23-10006-build-robust-and-resumable-file-transfers","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10006-Build-robust-and-resumable-file-transfers","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Find out how URLSession can help your apps transfer large files and recover from network interruptions. Learn how to pause and resume HTTP file transfers and support resumable uploads, and explore best practices for using URLSession to transfer files even when your app is suspended in the background."}],"title":"Build robust and resumable file transfers"},"WWDC23-10273-component24":{"identifier":"WWDC23-10273-component24","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component24.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/multitudes":{"url":"\/documentation\/wwdcnotes\/multitudes","title":"laurent b (32 notes)","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes","abstract":[{"text":"student at 42Berlin 🐬 | 🍎 Swift(UI) app dev  | speciality coffee ☕️ & cycling 🚴🏻‍♂️","type":"text"}],"type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10044-Discover-machine-learning-enhancements-in-Create-ML","title":"Discover machine learning enhancements in Create ML","url":"\/documentation\/wwdcnotes\/wwdc23-10044-discover-machine-learning-enhancements-in-create-ml","kind":"article","abstract":[{"type":"text","text":"Find out how Create ML can help you do even more with machine learning models. Learn about the latest updates to image understanding and text-based tasks with multilingual BERT embeddings. Discover how easy it is to train models that can understand the content of images using multi-label classification. We’ll also share information about interactive model evaluation and the latest APIs for custom training data augmentations."}]},"https://developer.apple.com/wwdc23/10273":{"identifier":"https:\/\/developer.apple.com\/wwdc23\/10273","url":"https:\/\/developer.apple.com\/wwdc23\/10273","type":"download","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10096-Build-great-games-for-spatial-computing":{"url":"\/documentation\/wwdcnotes\/wwdc23-10096-build-great-games-for-spatial-computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10096-Build-great-games-for-spatial-computing","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Find out how you can develop great gaming experiences for visionOS. We’ll share some of the key building blocks that help you create games for this platform, explore how your experiences can fluidly move between levels of immersion, and provide a roadmap for exploring ARKit, RealityKit, Reality Composer Pro, Unity, Metal, and Compositor."}],"title":"Build great games for spatial computing"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10105-Create-a-more-responsive-camera-experience":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10105-Create-a-more-responsive-camera-experience","url":"\/documentation\/wwdcnotes\/wwdc23-10105-create-a-more-responsive-camera-experience","title":"Create a more responsive camera experience","kind":"article","abstract":[{"type":"text","text":"Discover how AVCapture and PhotoKit can help you create more responsive and delightful apps. Learn about the camera capture process and find out how deferred photo processing can help create the best quality photo. We’ll show you how zero shutter lag uses time travel to capture the perfect action photo, dive into building a responsive capture pipeline, and share how you can adopt the Video Effects API to recognize pre-defined gestures that trigger real-time video effects."}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10271-Explore-immersive-sound-design":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10271-Explore-immersive-sound-design","title":"Explore immersive sound design","url":"\/documentation\/wwdcnotes\/wwdc23-10271-explore-immersive-sound-design","kind":"article","abstract":[{"type":"text","text":"Discover how you can use sound to enhance the experience of your visionOS apps and games. Learn how Apple designers select sounds and build soundscapes to create textural, immersive experiences. We’ll share how you can enrich basic interactions in your app with sound when you place audio cues spatially, vary repetitive sounds, and build moments of sonic delight into your app."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10170-Beyond-the-basics-of-structured-concurrency":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10170-Beyond-the-basics-of-structured-concurrency","title":"Beyond the basics of structured concurrency","url":"\/documentation\/wwdcnotes\/wwdc23-10170-beyond-the-basics-of-structured-concurrency","kind":"article","abstract":[{"type":"text","text":"It’s all about the task tree: Find out how structured concurrency can help your apps manage automatic task cancellation, task priority propagation, and useful task-local value patterns. Learn how to manage resources in your app with useful patterns and the latest task group APIs. We’ll show you how you can leverage the power of the task tree and task-local values to gain insight into distributed systems."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10107-Embed-the-Photos-Picker-in-your-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10107-Embed-the-Photos-Picker-in-your-app","title":"Embed the Photos Picker in your app","url":"\/documentation\/wwdcnotes\/wwdc23-10107-embed-the-photos-picker-in-your-app","kind":"article","abstract":[{"type":"text","text":"Discover how you can simply, safely, and securely access the Photos Library in your app. Learn how to get started with the embedded picker and explore the options menu and HDR still image support. We’ll also show you how to take advantage of UI customization options to help the picker blend into your existing interface."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10128-Your-guide-to-Metal-ray-tracing":{"url":"\/documentation\/wwdcnotes\/wwdc23-10128-your-guide-to-metal-ray-tracing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10128-Your-guide-to-Metal-ray-tracing","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Discover how you can enhance the visual quality of your games and apps with Metal ray tracing. We’ll take you through the fundamentals of the Metal ray tracing API. Explore the latest enhancements and techniques that will enable you to create larger and more complex scenes, reduce memory usage and build times, and efficiently render visual content like hair and fur."}],"title":"Your guide to Metal ray tracing"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10102-Spotlight-your-app-with-App-Shortcuts":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10102-Spotlight-your-app-with-App-Shortcuts","title":"Spotlight your app with App Shortcuts","url":"\/documentation\/wwdcnotes\/wwdc23-10102-spotlight-your-app-with-app-shortcuts","kind":"article","abstract":[{"text":"Discover how to use App Shortcuts to surface frequently used features from your app in Spotlight or through Siri. Find out how to configure search results for your app and learn best practices for creating great App Shortcuts. We’ll also show you how to build great visual and voice experiences and extend to other Apple devices like Apple Watch and HomePod.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10248-Analyze-hangs-with-Instruments":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10248-Analyze-hangs-with-Instruments","role":"sampleCode","abstract":[{"type":"text","text":"User interface elements often mimic real-world interactions, including real-time responses. Apps with a noticeable delay in user interaction — a hang — can break that illusion and create frustration. We’ll show you how to use Instruments to analyze, understand, and fix hangs in your apps on all Apple platforms. Discover how you can efficiently navigate an Instruments trace document, interpret trace data, and record additional profiling data to better understand your specific hang."}],"title":"Analyze hangs with Instruments","url":"\/documentation\/wwdcnotes\/wwdc23-10248-analyze-hangs-with-instruments","kind":"article","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10033-Extend-Speech-Synthesis-with-personal-and-custom-voices":{"url":"\/documentation\/wwdcnotes\/wwdc23-10033-extend-speech-synthesis-with-personal-and-custom-voices","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10033-Extend-Speech-Synthesis-with-personal-and-custom-voices","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Bring the latest advancements in Speech Synthesis to your apps. Learn how you can integrate your custom speech synthesizer and voices into iOS and macOS. We’ll show you how SSML is used to generate expressive speech synthesis, and explore how Personal Voice can enable your augmentative and assistive communication app to speak on a person’s behalf in an authentic way."}],"title":"Extend Speech Synthesis with personal and custom voices"},"https://developer.apple.com/forums/tags/wwdc2023-10273":{"identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10273","url":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10273","title":"Search the forums for tag wwdc2023-10273","type":"link","titleInlineContent":[{"type":"text","text":"Search the forums for tag wwdc2023-10273"}]},"WWDC23-10273-composer":{"identifier":"WWDC23-10273-composer","type":"image","alt":"Reality Composer Pro Icon","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-composer.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10229-Make-features-discoverable-with-TipKit":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10229-Make-features-discoverable-with-TipKit","title":"Make features discoverable with TipKit","url":"\/documentation\/wwdcnotes\/wwdc23-10229-make-features-discoverable-with-tipkit","role":"sampleCode","abstract":[{"text":"Teach people how to use your app with TipKit! Learn how you can create effective educational moments through tips. We’ll share how you can build eligibility rules to reach the ideal audience, control tip frequency, and strategies for testing to ensure successful interactions.","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=0":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=0","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=0","title":"0:00 - Introduction","type":"link","titleInlineContent":[{"type":"text","text":"0:00 - Introduction"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10189-Migrate-to-SwiftData":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10189-Migrate-to-SwiftData","url":"\/documentation\/wwdcnotes\/wwdc23-10189-migrate-to-swiftdata","title":"Migrate to SwiftData","kind":"article","abstract":[{"type":"text","text":"Discover how you can start using SwiftData in your apps. We’ll show you how to use Xcode to generate model classes from your existing Core Data object models, use SwiftData alongside your previous implementation, or even completely replace your existing solution."}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10180-Discover-streamlined-location-updates":{"url":"\/documentation\/wwdcnotes\/wwdc23-10180-discover-streamlined-location-updates","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10180-Discover-streamlined-location-updates","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Move into the future with Core Location! Meet the CLLocationUpdate class, designed for modern Swift concurrency, and learn how it simplifies getting location updates. We’ll show you how this class works with your apps when they run in the foreground or background and share some best practices."}],"title":"Discover streamlined location updates"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=720":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=720","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=720","title":"12:00 - User Interface","type":"link","titleInlineContent":[{"type":"text","text":"12:00 - User Interface"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10070-Create-a-great-spatial-playback-experience":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10070-Create-a-great-spatial-playback-experience","title":"Create a great spatial playback experience","url":"\/documentation\/wwdcnotes\/wwdc23-10070-create-a-great-spatial-playback-experience","kind":"article","abstract":[{"type":"text","text":"Get ready to support video in your visionOS app! Take a tour of the frameworks and APIs that power video playback and learn how you can update your app to play 3D content. We’ll also share tips for customizing playback to create a more immersive watching experience."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10115-Design-with-SwiftUI":{"url":"\/documentation\/wwdcnotes\/wwdc23-10115-design-with-swiftui","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10115-Design-with-SwiftUI","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Discover how SwiftUI can help you quickly iterate and explore design ideas. Learn from Apple designers as they share how working with SwiftUI influenced the design of the Maps app in watchOS 10 and other elements of their work, and find out how you can incorporate these workflows in your own process."}],"title":"Design with SwiftUI"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10087-Build-spatial-SharePlay-experiences":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10087-Build-spatial-SharePlay-experiences","title":"Build spatial SharePlay experiences","url":"\/documentation\/wwdcnotes\/wwdc23-10087-build-spatial-shareplay-experiences","kind":"article","abstract":[{"text":"Discover how you can use the GroupActivities framework to build unique sharing and collaboration experiences for visionOS. We’ll introduce you to SharePlay on this platform, learn how to create experiences that make people feel present as if they were in the same space, and explore how immersive apps can respect shared context between participants.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10159-Beyond-scroll-views":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10159-Beyond-scroll-views","title":"Beyond scroll views","url":"\/documentation\/wwdcnotes\/wwdc23-10159-beyond-scroll-views","kind":"article","abstract":[{"type":"text","text":"Find out how you can take your scroll views to the next level with the latest APIs in SwiftUI. We’ll show you how to customize scroll views like never before. Explore the relationship between safe areas and a scroll view’s margins, learn how to interact with the content offset of a scroll view, and discover how you can add a bit of flair to your content with scroll transitions."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10241-Share-files-with-SharePlay":{"abstract":[{"text":"Discover how to work with files and attachments in a SharePlay activity. We’ll explain how to use the GroupSessionJournal API to sync large amounts of data faster and show you how to adopt it in a demo of the sample app DrawTogether.","type":"text"}],"type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10241-Share-files-with-SharePlay","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10241-share-files-with-shareplay","title":"Share files with SharePlay"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10016-Build-custom-workouts-with-WorkoutKit":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10016-Build-custom-workouts-with-WorkoutKit","title":"Build custom workouts with WorkoutKit","url":"\/documentation\/wwdcnotes\/wwdc23-10016-build-custom-workouts-with-workoutkit","kind":"article","abstract":[{"type":"text","text":"WorkoutKit makes it easy to create, preview, and schedule planned workouts for the Workout app on Apple Watch. Learn how to build custom intervals, create alerts, and use the built-in preview UI to send your own workout routines to Apple Watch."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10037-Explore-pie-charts-and-interactivity-in-Swift-Charts":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10037-Explore-pie-charts-and-interactivity-in-Swift-Charts","title":"Explore pie charts and interactivity in Swift Charts","url":"\/documentation\/wwdcnotes\/wwdc23-10037-explore-pie-charts-and-interactivity-in-swift-charts","kind":"article","abstract":[{"type":"text","text":"Swift Charts has come full circle: Get ready to bake up pie and donut charts in your app with the latest improvements to the framework. Learn how to make your charts scrollable, explore the chart selection API for revealing additional details in your data, and find out how enabling additional interactivity can make your charts even more delightful."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10127-Optimize-GPU-renderers-with-Metal":{"type":"topic","kind":"article","title":"Optimize GPU renderers with Metal","url":"\/documentation\/wwdcnotes\/wwdc23-10127-optimize-gpu-renderers-with-metal","abstract":[{"type":"text","text":"Discover how to optimize your GPU renderer using the latest Metal features and best practices. We’ll show you how to use function specialization and parallel shader compilation to maintain responsive authoring workflows and the fastest rendering speeds, and help you tune your compute shaders for optimal performance."}],"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10127-Optimize-GPU-renderers-with-Metal"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10061-Verify-app-dependencies-with-digital-signatures":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10061-Verify-app-dependencies-with-digital-signatures","title":"Verify app dependencies with digital signatures","url":"\/documentation\/wwdcnotes\/wwdc23-10061-verify-app-dependencies-with-digital-signatures","role":"sampleCode","abstract":[{"text":"Discover how you can help secure your app’s dependencies. We’ll show you how Xcode can automatically verify any signed XCFrameworks you include within a project. Learn how code signatures work, the benefits they provide to help protect your software supply chain, and how SDK developers can sign their XCFrameworks to help keep your apps secure.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10012-Explore-App-Store-Connect-for-spatial-computing":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10012-Explore-App-Store-Connect-for-spatial-computing","title":"Explore App Store Connect for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-10012-explore-app-store-connect-for-spatial-computing","kind":"article","abstract":[{"type":"text","text":"App Store Connect provides the tools you need to test, submit, and manage your visionOS apps on the App Store. Explore basics and best practices for deploying your first spatial computing app, adding support for visionOS to an existing app, and managing compatibility. We’ll also show you how TestFlight for visionOS can help you test your apps and collect valuable feedback as you iterate."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10122-Explore-media-formats-for-the-web":{"url":"\/documentation\/wwdcnotes\/wwdc23-10122-explore-media-formats-for-the-web","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10122-Explore-media-formats-for-the-web","role":"sampleCode","kind":"article","type":"topic","abstract":[{"text":"Learn about the latest image formats and video technologies supported in Safari 17. Discover how you can use JPEG XL, AVIF, and HEIC in your websites and experiences and learn how they differ from previous formats. We’ll also show you how the Managed Media Source API draws less power than Media Source Extensions (MSE) and explore how you can use it to more efficiently manage streaming video over 5G.","type":"text"}],"title":"Explore media formats for the web"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10042-Explore-Natural-Language-multilingual-models":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10042-Explore-Natural-Language-multilingual-models","title":"Explore Natural Language multilingual models","url":"\/documentation\/wwdcnotes\/wwdc23-10042-explore-natural-language-multilingual-models","kind":"article","abstract":[{"type":"text","text":"Learn how to create custom Natural Language models for text classification and word tagging using multilingual, transformer-based embeddings. We’ll show you how to train with less data and support up to 27 different languages across three scripts. Find out how to use these embeddings to fine-tune complex models trained in PyTorch and TensorFlow."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10142-Explore-testing-inapp-purchases":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10142-Explore-testing-inapp-purchases","title":"Explore testing in-app purchases","url":"\/documentation\/wwdcnotes\/wwdc23-10142-explore-testing-inapp-purchases","kind":"article","abstract":[{"type":"text","text":"Learn how you can test in-app purchases throughout development with StoreKit Testing in Xcode, App Store sandbox, and TestFlight. Explore how each tool functions and how you can combine them to build the right workflow for testing your apps and games. We’ll also share a sneak preview of how you can test Family Sharing for in-app purchases & subscriptions in the App Store sandbox."}]},"WWDC23-10273-component17":{"identifier":"WWDC23-10273-component17","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component17.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10161-Inspectors-in-SwiftUI-Discover-the-details":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10161-Inspectors-in-SwiftUI-Discover-the-details","title":"Inspectors in SwiftUI: Discover the details","url":"\/documentation\/wwdcnotes\/wwdc23-10161-inspectors-in-swiftui-discover-the-details","kind":"article","abstract":[{"type":"text","text":"Meet Inspectors — a structural API that can help bring a new level of detail to your apps. We’ll take you through the fundamentals of the API and show you how to adopt it. Learn about the latest updates to sheet presentation customizations and find out how you can combine the two to create perfect presentation experiences."}]},"https://developer.apple.com/videos/play/wwdc2021/10074":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074","title":"Dive into RealityKit 2 - WWDC21","type":"link","titleInlineContent":[{"type":"text","text":"Dive into RealityKit 2 - WWDC21"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10029-Build-widgets-for-the-Smart-Stack-on-Apple-Watch":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10029-Build-widgets-for-the-Smart-Stack-on-Apple-Watch","title":"Build widgets for the Smart Stack on Apple Watch","url":"\/documentation\/wwdcnotes\/wwdc23-10029-build-widgets-for-the-smart-stack-on-apple-watch","kind":"article","abstract":[{"type":"text","text":"Follow along as we build a widget for the Smart Stack on watchOS 10 using the latest SwiftUI and WidgetKit APIs. Learn tips, techniques, and best practices for creating widgets that show relevant information on Apple Watch."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10045-Detect-animal-poses-in-Vision":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","title":"Detect animal poses in Vision","url":"\/documentation\/wwdcnotes\/wwdc23-10045-detect-animal-poses-in-vision","kind":"article","abstract":[{"type":"text","text":"Go beyond detecting cats and dogs in images. We’ll show you how to use Vision to detect the individual joints and poses of these animals as well — all in real time — and share how you can enable exciting features like animal tracking for a camera app, creative embellishment on an animal photo, and more. We’ll also explore other important enhancements to Vision and share best practices."}]},"WWDC23-10273-component30":{"identifier":"WWDC23-10273-component30","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component30.jpg"}]},"https://developer.apple.com/videos/play/wwdc2023/10083":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083","title":"Meet Reality Composer Pro - WWDC23","type":"link","titleInlineContent":[{"type":"text","text":"Meet Reality Composer Pro - WWDC23"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10137-Support-Cinematic-mode-videos-in-your-app":{"abstract":[{"text":"Discover how the Cinematic Camera API helps your app work with Cinematic mode videos captured in the Camera app. We’ll share the fundamentals — including Decision layers — that make up Cinematic mode video, show you how to access and update Decisions in your app, and help you save and load those changes.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10137-Support-Cinematic-mode-videos-in-your-app","role":"sampleCode","title":"Support Cinematic mode videos in your app","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10137-support-cinematic-mode-videos-in-your-app","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10088-Create-immersive-Unity-apps":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10088-Create-immersive-Unity-apps","title":"Create immersive Unity apps","url":"\/documentation\/wwdcnotes\/wwdc23-10088-create-immersive-unity-apps","kind":"article","abstract":[{"type":"text","text":"Explore how you can use Unity to create engaging and immersive experiences for visionOS. We’ll share how Unity integrates seamlessly with Apple frameworks, take you through the tools you can use to build natively for the platform, and show you how volume cameras can bring your existing scenes into visionOS windows, volumes, and spaces."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10154-Build-an-app-with-SwiftData":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10154-Build-an-app-with-SwiftData","title":"Build an app with SwiftData","url":"\/documentation\/wwdcnotes\/wwdc23-10154-build-an-app-with-swiftdata","kind":"article","abstract":[{"type":"text","text":"Discover how SwiftData can help you persist data in your app. Code along with us as we bring SwiftData to a multi-platform SwiftUI app. Learn how to convert existing model classes into SwiftData models, set up the environment, reflect model layer changes in UI, and build document-based applications backed by SwiftData storage."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10239-Add-SharePlay-to-your-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10239-Add-SharePlay-to-your-app","title":"Add SharePlay to your app","url":"\/documentation\/wwdcnotes\/wwdc23-10239-add-shareplay-to-your-app","kind":"article","abstract":[{"type":"text","text":"Discover how your app can take advantage of SharePlay to turn any activity into a shareable experience with friends! We’ll share the latest updates to SharePlay, explore the benefits of creating shared activities, dive into some exciting use cases, and take you through best practices to create engaging and fun moments of connection in your app."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10124-Bring-your-game-to-Mac-Part-2-Compile-your-shaders":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10124-Bring-your-game-to-Mac-Part-2-Compile-your-shaders","title":"Bring your game to Mac, Part 2: Compile your shaders","url":"\/documentation\/wwdcnotes\/wwdc23-10124-bring-your-game-to-mac-part-2-compile-your-shaders","kind":"article","abstract":[{"type":"text","text":"Discover how the Metal shader converter streamlines the process of bringing your HLSL shaders to Metal as we continue our three-part series on bringing your game to Mac. Find out how to build a fast, end-to-end shader pipeline from DXIL that supports all shader stages and allows you to leverage the advanced features of Apple GPUs. We’ll also show you how to reduce app launch time and stutters by generating GPU binaries with the offline compiler."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10181-Support-HDR-images-in-your-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10181-Support-HDR-images-in-your-app","title":"Support HDR images in your app","url":"\/documentation\/wwdcnotes\/wwdc23-10181-support-hdr-images-in-your-app","kind":"article","abstract":[{"type":"text","text":"Learn how to identify, load, display, and create High Dynamic Range (HDR) still images in your app. Explore common HDR concepts and find out about the latest updates to the ISO specification. Learn how to identify and display HDR images with SwiftUI and UIKit, create them from ProRAW and RAW captures, and display them in CALayers. We’ll also take you through CoreGraphics support for ISO HDR and share best practices for HDR adoption."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10168-Generalize-APIs-with-parameter-packs":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10168-Generalize-APIs-with-parameter-packs","title":"Generalize APIs with parameter packs","url":"\/documentation\/wwdcnotes\/wwdc23-10168-generalize-apis-with-parameter-packs","kind":"article","abstract":[{"type":"text","text":"Swift parameter packs are a powerful tool to expand what is possible in your generic code while also enabling you to simplify common generic patterns. We’ll show you how to abstract over types as well as the number of arguments in generic code and simplify common generic patterns to avoid overloads."}]},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=1671":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1671","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1671","title":"27:51 - Play audio","type":"link","titleInlineContent":[{"type":"text","text":"27:51 - Play audio"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10275-Explore-AirPlay-with-interstitials":{"abstract":[{"text":"Learn how you can use HLS Interstitials with AirPlay to create seamless transitions for your video content between advertisements. We’ll share best practices and tips for creating a great experience when sharing content from Apple devices to popular smart TVs.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10275-Explore-AirPlay-with-interstitials","role":"sampleCode","title":"Explore AirPlay with interstitials","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10275-explore-airplay-with-interstitials","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10160-Demystify-SwiftUI-performance":{"url":"\/documentation\/wwdcnotes\/wwdc23-10160-demystify-swiftui-performance","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10160-Demystify-SwiftUI-performance","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Learn how you can build a mental model for performance in SwiftUI and write faster, more efficient code. We’ll share some of the common causes behind performance issues and help you triage hangs and hitches in SwiftUI to create more responsive views in your app."}],"title":"Demystify SwiftUI performance"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10162-The-SwiftUI-cookbook-for-focus":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10162-The-SwiftUI-cookbook-for-focus","url":"\/documentation\/wwdcnotes\/wwdc23-10162-the-swiftui-cookbook-for-focus","title":"The SwiftUI cookbook for focus","kind":"article","abstract":[{"type":"text","text":"The SwiftUI team is back in the coding “kitchen” with powerful tools to shape your app’s focus experience. Join us and learn about the staple ingredients that support focus-driven interactions in your app. Discover focus interactions for custom views, find out about key-press handlers for keyboard input, and learn how to support movement and hierarchy with focus sections. We’ll also go through some tasty recipes for common focus patterns in your app."}],"role":"sampleCode","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10023-Build-a-multidevice-workout-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10023-Build-a-multidevice-workout-app","title":"Build a multi-device workout app","url":"\/documentation\/wwdcnotes\/wwdc23-10023-build-a-multidevice-workout-app","kind":"article","abstract":[{"type":"text","text":"Learn how you can get iPhone involved in your Apple Watch-based workout apps with HealthKit. We’ll show you how to mirror workouts between devices and take a ride with cycling data types. Plus, get to know HealthKit for iPad."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10238-Tune-up-your-AirPlay-audio-experience":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10238-Tune-up-your-AirPlay-audio-experience","title":"Tune up your AirPlay audio experience","url":"\/documentation\/wwdcnotes\/wwdc23-10238-tune-up-your-airplay-audio-experience","kind":"article","abstract":[{"type":"text","text":"Learn how you can upgrade your app’s AirPlay audio experience to be more robust and responsive. We’ll show you how to adopt enhanced audio buffering with AVQueuePlayer, explore alternatives when building a custom player in your app, and share best practices."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10028-Bring-widgets-to-life":{"url":"\/documentation\/wwdcnotes\/wwdc23-10028-bring-widgets-to-life","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10028-Bring-widgets-to-life","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Learn how to make animated and interactive widgets for your apps and games. We’ll show you how to tweak animations for entry transitions and add interactivity using SwiftUI Button and Toggle so that you can create powerful moments right from the Home Screen and Lock Screen."}],"title":"Bring widgets to life"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10153-Unlock-the-power-of-grammatical-agreement":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10153-Unlock-the-power-of-grammatical-agreement","title":"Unlock the power of grammatical agreement","url":"\/documentation\/wwdcnotes\/wwdc23-10153-unlock-the-power-of-grammatical-agreement","kind":"article","abstract":[{"type":"text","text":"Discover how you can use automatic grammatical agreement in your apps and games to create inclusive and more natural-sounding expressions. We’ll share best practices for working with Foundation, showcase examples in multiple languages, and demonstrate how to use these APIs to enhance the user experience for your apps."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10056-Build-better-documentbased-apps":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10056-Build-better-documentbased-apps","title":"Build better document-based apps","url":"\/documentation\/wwdcnotes\/wwdc23-10056-build-better-documentbased-apps","kind":"article","abstract":[{"type":"text","text":"Discover how you can use the latest features in iPadOS to improve your document-based apps. We’ll show you how to take advantage of UIDocument as well as existing desktop-class iPad and document-based APIs to add new features in your app. Find out how to convert data models to UIDocument, present documents with UIDocumentViewController, learn how to migrate your apps to the latest APIs, and explore best practices."}]},"WWDC23-10273-component23":{"identifier":"WWDC23-10273-component23","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component23.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10281-Keep-up-with-the-keyboard":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10281-Keep-up-with-the-keyboard","title":"Keep up with the keyboard","url":"\/documentation\/wwdcnotes\/wwdc23-10281-keep-up-with-the-keyboard","kind":"article","abstract":[{"type":"text","text":"Each year, the keyboard evolves to support an increasing range of languages, sizes, and features. Discover how you can design your app to keep up with the keyboard, regardless of how it appears on a device. We’ll show you how to create frictionless text entry and share important architectural changes to help you understand how the keyboard works within the system."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10075-Design-spatial-SharePlay-experiences":{"kind":"article","type":"topic","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10075-Design-spatial-SharePlay-experiences","title":"Design spatial SharePlay experiences","url":"\/documentation\/wwdcnotes\/wwdc23-10075-design-spatial-shareplay-experiences","abstract":[{"text":"Explore the types of shared activities you can create in your visionOS apps and find out how your apps can use Spatial Persona templates to support meaningful interactions between people. Discover how to design your UI around a shared context, handle immersive content in a shared activity, and more.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10031-Update-your-app-for-watchOS-10":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10031-Update-your-app-for-watchOS-10","title":"Update your app for watchOS 10","url":"\/documentation\/wwdcnotes\/wwdc23-10031-update-your-app-for-watchos-10","kind":"article","abstract":[{"type":"text","text":"Join us as we update an Apple Watch app to take advantage of the latest features in watchOS 10. In this code-along, we’ll show you how to use the latest SwiftUI APIs to maximize glanceability and reorient app navigation around the Digital Crown."}]},"WWDC23-10273-component9":{"identifier":"WWDC23-10273-component9","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component9.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10195-Model-your-schema-with-SwiftData":{"url":"\/documentation\/wwdcnotes\/wwdc23-10195-model-your-schema-with-swiftdata","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10195-Model-your-schema-with-SwiftData","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Learn how to use schema macros and migration plans with SwiftData to build more complex features for your app. We’ll show you how to fine-tune your persistence with @Attribute and @Relationship options. Learn how to exclude properties from your data model with @Transient and migrate from one version of your schema to the next with ease."}],"title":"Model your schema with SwiftData"},"WWDC23-10273-component10":{"identifier":"WWDC23-10273-component10","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component10.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10257-Create-animated-symbols":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10257-Create-animated-symbols","title":"Create animated symbols","url":"\/documentation\/wwdcnotes\/wwdc23-10257-create-animated-symbols","kind":"article","abstract":[{"text":"Discover animation presets and learn how to use them with SF Symbols and custom symbols. We’ll show you how to experiment with different options and configurations to find the perfect animation for your app. Learn how to update custom symbols for animation using annotation features, find out how to modify your custom symbols with symbol components, and explore the redesigned export process to help keep symbols looking great on all platforms.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10176-Lift-subjects-from-images-in-your-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","title":"Lift subjects from images in your app","url":"\/documentation\/wwdcnotes\/wwdc23-10176-lift-subjects-from-images-in-your-app","kind":"article","abstract":[{"type":"text","text":"Discover how you can easily pull the subject of an image from its background in your apps. Learn how to lift the primary subject or to access the subject at a given point with VisionKit. We’ll also share how you can lift subjects using Vision and combine that with lower-level frameworks like Core Image to create fun image effects and more complex compositing pipelines."}]},"WWDC23-10273-audio2":{"identifier":"WWDC23-10273-audio2","type":"image","alt":"Play audio","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-audio2.jpg"}]},"WWDC23-10273-audio":{"identifier":"WWDC23-10273-audio","type":"image","alt":"Play audio","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-audio.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10258-Animate-symbols-in-your-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10258-Animate-symbols-in-your-app","title":"Animate symbols in your app","url":"\/documentation\/wwdcnotes\/wwdc23-10258-animate-symbols-in-your-app","kind":"article","abstract":[{"type":"text","text":"Bring delight to your app with animated symbols. Explore the new Symbols framework, which features a unified API to create and configure symbol effects. Learn how SwiftUI, AppKit, and UIKit make it easy to animate symbols in user interfaces. Discover tips and tricks to seamlessly integrate the new animations alongside other app content. To get the most from this session, we recommend first watching “What’s new in SF Symbols 5.”"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10034-Create-accessible-spatial-experiences":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10034-Create-accessible-spatial-experiences","title":"Create accessible spatial experiences","url":"\/documentation\/wwdcnotes\/wwdc23-10034-create-accessible-spatial-experiences","kind":"article","abstract":[{"type":"text","text":"Learn how you can make spatial computing apps that work well for everyone. Like all Apple platforms, visionOS is designed for accessibility: We’ll share how we’ve reimagined assistive technologies like VoiceOver and Pointer Control and designed features like Dwell Control to help people interact in the way that works best for them. Learn best practices for vision, motor, cognitive, and hearing accessibility and help everyone enjoy immersive experiences for visionOS."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10095-Explore-rendering-for-spatial-computing":{"url":"\/documentation\/wwdcnotes\/wwdc23-10095-explore-rendering-for-spatial-computing","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10095-Explore-rendering-for-spatial-computing","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Find out how you can take control of RealityKit rendering to improve the look and feel of your apps and games on visionOS. Discover how you can customize lighting, add grounding shadows, and control tone mapping for your content. We’ll also go over best practices for two key treatments on the platform: rasterization rate maps and dynamic content scaling."}],"title":"Explore rendering for spatial computing"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10074-Dive-into-RealityKit-2":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10074-Dive-into-RealityKit-2","abstract":[{"type":"text","text":"Creating engaging AR experiences has never been easier with RealityKit 2. Explore the latest enhancements to the RealityKit framework and take a deep dive into this underwater sample project. We’ll take you through the improved Entity Component System, streamlined animation pipeline, and the plug-and-play character controller with enhancements to face mesh and audio."}],"role":"sampleCode","title":"Dive into RealityKit 2","type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc21-10074-dive-into-realitykit-2"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10007-Create-seamless-experiences-with-Virtualization":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10007-Create-seamless-experiences-with-Virtualization","title":"Create seamless experiences with Virtualization","url":"\/documentation\/wwdcnotes\/wwdc23-10007-create-seamless-experiences-with-virtualization","kind":"article","abstract":[{"type":"text","text":"Discover the latest updates to the Virtualization framework. We’ll show you how to configure a virtual machine (VM) to automatically resize its display, take you through saving and restoring a running VM, and explore storage and performance options for Virtualization apps running on the desktop or in the data center."}]},"WWDC23-10273-component8":{"identifier":"WWDC23-10273-component8","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component8.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10244-Create-rich-documentation-with-SwiftDocC":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10244-Create-rich-documentation-with-SwiftDocC","title":"Create rich documentation with Swift-DocC","url":"\/documentation\/wwdcnotes\/wwdc23-10244-create-rich-documentation-with-swiftdocc","kind":"article","abstract":[{"text":"Learn how you can take advantage of the latest features in Swift-DocC to create rich and detailed documentation for your app or framework. We’ll show you how to use the Xcode 15 Documentation Preview editor to efficiently iterate on your existing project’s documentation, and explore expanded authoring capabilities like grid-based layouts, video support, and custom themes.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10085-Discover-Quick-Look-for-spatial-computing":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10085-Discover-Quick-Look-for-spatial-computing","title":"Discover Quick Look for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-10085-discover-quick-look-for-spatial-computing","kind":"article","abstract":[{"type":"text","text":"Learn how to use Quick Look on visionOS to add powerful previews for 3D content, spatial images and videos, and much more. We’ll show you the different ways that the system presents these experiences, demonstrate how someone can drag and drop Quick Look content from an app or website to create a separate window with that content, and explore how you can present Quick Look directly within an app."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10071-Deliver-video-content-for-spatial-experiences":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10071-Deliver-video-content-for-spatial-experiences","title":"Deliver video content for spatial experiences","url":"\/documentation\/wwdcnotes\/wwdc23-10071-deliver-video-content-for-spatial-experiences","kind":"article","abstract":[{"type":"text","text":"Learn how to prepare and deliver video content for visionOS using HTTP Live Streaming (HLS). Discover the current HLS delivery process for media and explore how you can expand your delivery pipeline to support 3D content. Get up to speed with tips and techniques for spatial media streaming and adapting your existing caption production workflows for 3D. And find out how to share audio tracks across video variants and add spatial audio to make your video content more immersive."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10266-Protect-your-Mac-app-with-environment-constraints":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10266-Protect-your-Mac-app-with-environment-constraints","title":"Protect your Mac app with environment constraints","url":"\/documentation\/wwdcnotes\/wwdc23-10266-protect-your-mac-app-with-environment-constraints","kind":"article","abstract":[{"type":"text","text":"Learn how to improve the security of your Mac app by adopting environment constraints. We’ll show you how to set limits on how processes are launched, make sure your Launch Agents and Launch Daemons aren’t tampered with, and prevent unwanted code from running in your address space."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10103-Explore-enhancements-to-App-Intents":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10103-Explore-enhancements-to-App-Intents","title":"Explore enhancements to App Intents","url":"\/documentation\/wwdcnotes\/wwdc23-10103-explore-enhancements-to-app-intents","kind":"article","abstract":[{"type":"text","text":"Bring your widgets to life with App Intents! Explore the latest updates and learn how you can take advantage of dynamic options and user interactivity to build better experiences for your App Shortcuts. We’ll share how you can integrate with Apple Pay, structure your code more efficiently, and take your Shortcuts app integration to the next level."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10167-Expand-on-Swift-macros":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10167-Expand-on-Swift-macros","title":"Expand on Swift macros","url":"\/documentation\/wwdcnotes\/wwdc23-10167-expand-on-swift-macros","kind":"article","abstract":[{"type":"text","text":"Discover how Swift macros can help you reduce boilerplate in your codebase and adopt complex features more easily. Learn how macros can analyze code, emit rich compiler errors to guide developers towards correct usage, and generate new code that is automatically incorporated back into your project. We’ll also take you through important concepts like macro roles, compiler plugins, and syntax trees."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10076-Design-for-spatial-user-interfaces":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10076-Design-for-spatial-user-interfaces","title":"Design for spatial user interfaces","url":"\/documentation\/wwdcnotes\/wwdc23-10076-design-for-spatial-user-interfaces","kind":"article","abstract":[{"type":"text","text":"Learn how to design great interfaces for spatial computing apps. We’ll share how your existing screen-based knowledge easily translates into creating great experiences for visionOS. Explore guidelines for UI components, materials, and typography and find out how you can design experiences that are familiar, legible, and easy to use."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10060-Get-started-with-privacy-manifests":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10060-Get-started-with-privacy-manifests","title":"Get started with privacy manifests","url":"\/documentation\/wwdcnotes\/wwdc23-10060-get-started-with-privacy-manifests","kind":"article","abstract":[{"type":"text","text":"Meet privacy manifests: a new tool that helps you accurately identify the privacy practices of your app’s dependencies. Find out how third-party SDK developers can use these manifests to share privacy practices for their frameworks. We’ll also share how Xcode can produce a full privacy report to help you more easily represent the privacy practices of all the code in your app."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10149-Discover-Observation-in-SwiftUI":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10149-Discover-Observation-in-SwiftUI","title":"Discover Observation in SwiftUI","url":"\/documentation\/wwdcnotes\/wwdc23-10149-discover-observation-in-swiftui","kind":"article","abstract":[{"type":"text","text":"Simplify your SwiftUI data models with Observation. We’ll share how the Observable macro can help you simplify models and improve your app’s performance. Get to know Observation, learn the fundamentals of the macro, and find out how to migrate from ObservableObject to Observable."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10104-Integrate-your-media-app-with-HomePod":{"url":"\/documentation\/wwdcnotes\/wwdc23-10104-integrate-your-media-app-with-homepod","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10104-Integrate-your-media-app-with-HomePod","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Learn how people can interact with your media app directly from HomePod. We’ll show you how to add a media intent to your iPhone or iPad app and help people stream your content to a HomePod speaker over AirPlay simply by using their voice. Explore implementation details and get tips and best practices on how to create a great experience for music, audiobooks, podcasts, meditations, or other media types."}],"title":"Integrate your media app with HomePod"},"WWDC23-10273-component7":{"identifier":"WWDC23-10273-component7","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component7.jpg"}]},"WWDC23-10273-component16":{"identifier":"WWDC23-10273-component16","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component16.jpg"}]},"WWDC23-10273-anatomy2":{"identifier":"WWDC23-10273-anatomy2","type":"image","alt":"Anatomy of a Reality Composer Pro package","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-anatomy2.jpg"}]},"WWDC23-10273-catalina":{"identifier":"WWDC23-10273-catalina","type":"image","alt":"Catalina Island off the coast of Los Angeles.","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-catalina.jpg"}]},"WWDC23-10273-component2":{"identifier":"WWDC23-10273-component2","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component2.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10036-Build-accessible-apps-with-SwiftUI-and-UIKit":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10036-Build-accessible-apps-with-SwiftUI-and-UIKit","title":"Build accessible apps with SwiftUI and UIKit","url":"\/documentation\/wwdcnotes\/wwdc23-10036-build-accessible-apps-with-swiftui-and-uikit","kind":"article","abstract":[{"type":"text","text":"Discover how advancements in UI frameworks make it easier to build rich, accessible experiences. Find out how technologies like VoiceOver can better interact with your app’s interface through accessibility traits and actions. We’ll share the latest updates to SwiftUI that help you refine your accessibility experience and show you how to keep accessibility information up-to-date in your UIKit apps."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10250-Prototype-with-Xcode-Playgrounds":{"type":"topic","kind":"article","title":"Prototype with Xcode Playgrounds","url":"\/documentation\/wwdcnotes\/wwdc23-10250-prototype-with-xcode-playgrounds","abstract":[{"text":"Speed up feature development by prototyping new code with Xcode Playgrounds, eliminating the need to keep rebuilding and relaunching your project to verify your changes. We’ll show you how using a playground in your project or package can help you try out your code in various scenarios and take a close look at the returned values, including complex structures and user interface elements, so you can quickly iterate on a feature before integrating it into your project.","type":"text"}],"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10250-Prototype-with-Xcode-Playgrounds"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10123-Bring-your-game-to-Mac-Part-1-Make-a-game-plan":{"url":"\/documentation\/wwdcnotes\/wwdc23-10123-bring-your-game-to-mac-part-1-make-a-game-plan","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10123-Bring-your-game-to-Mac-Part-1-Make-a-game-plan","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Bring modern, high-end games to Mac and iPad with the powerful features of Metal and Apple silicon. Discover the game porting toolkit and learn how it can help you evaluate your existing Windows game for graphics feature compatibility and performance. We’ll share best practices and technical resources for handling audio, input, and advanced display features."}],"title":"Bring your game to Mac, Part 1: Make a game plan"},"WWDC23-10273-component6":{"identifier":"WWDC23-10273-component6","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component6.jpg"}]},"WWDC23-10273-component13":{"identifier":"WWDC23-10273-component13","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component13.jpg"}]},"WWDC23-10273-dioramaAssembled":{"identifier":"WWDC23-10273-dioramaAssembled","type":"image","alt":"dioramaAssembled","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-dioramaAssembled.jpg"}]},"https://laurentbrusa.hashnode.dev/":{"identifier":"https:\/\/laurentbrusa.hashnode.dev\/","url":"https:\/\/laurentbrusa.hashnode.dev\/","title":"Blog","type":"link","titleInlineContent":[{"type":"text","text":"Blog"}]},"WWDC23-10273-materials":{"identifier":"WWDC23-10273-materials","type":"image","alt":"Material properties","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-materials.jpg"}]},"WWDC23-10273-component22":{"identifier":"WWDC23-10273-component22","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component22.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","title":"Explore 3D body pose and person segmentation in Vision","url":"\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision","kind":"article","abstract":[{"type":"text","text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We’ll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image."}]},"WWDC23-10273-component29":{"identifier":"WWDC23-10273-component29","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component29.jpg"}]},"WWDC23-10273-component12":{"identifier":"WWDC23-10273-component12","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component12.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10058-Whats-new-with-text-and-text-interactions":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10058-Whats-new-with-text-and-text-interactions","role":"sampleCode","abstract":[{"text":"Text is an absolutely critical component of every app. Discover the latest features and enhancements for creating rich text experiences on Apple platforms. We’ll show you how to take advantage of common text elements and create entirely custom interactions for your app. Learn about updates to dictation, text loupe, and text selection, and explore improvements to text clipping, line wrapping, and hyphenation.","type":"text"}],"title":"What’s new with text and text interactions","url":"\/documentation\/wwdcnotes\/wwdc23-10058-whats-new-with-text-and-text-interactions","kind":"article","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10035-Perform-accessibility-audits-for-your-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10035-Perform-accessibility-audits-for-your-app","title":"Perform accessibility audits for your app","url":"\/documentation\/wwdcnotes\/wwdc23-10035-perform-accessibility-audits-for-your-app","kind":"article","abstract":[{"type":"text","text":"Discover how you can test your app for accessibility with every build. Learn how to perform automated audits for accessibility using XCTest and find out how to interpret the results. We’ll also share enhancements to the accessibility API that can help you improve UI test coverage."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10156-Explore-SwiftUI-animation":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10156-Explore-SwiftUI-animation","title":"Explore SwiftUI animation","url":"\/documentation\/wwdcnotes\/wwdc23-10156-explore-swiftui-animation","kind":"article","abstract":[{"type":"text","text":"Explore SwiftUI’s powerful animation capabilities and find out how these features work together to produce impressive visual effects. Learn how SwiftUI refreshes the rendering of a view, determines what to animate, interpolates values over time, and propagates context for the current transaction."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10150-Optimize-CarPlay-for-vehicle-systems":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10150-Optimize-CarPlay-for-vehicle-systems","title":"Optimize CarPlay for vehicle systems","url":"\/documentation\/wwdcnotes\/wwdc23-10150-optimize-carplay-for-vehicle-systems","kind":"article","abstract":[{"type":"text","text":"Discover how you can integrate CarPlay into modern vehicle systems. We’ll show you how to adjust CarPlay for any high-resolution display — regardless of configuration or size. Learn how you can use CarPlay-supplied metadata and video streams to show information on additional displays, and find out how advances in wireless connectivity, audio, and video encoding can help prepare your vehicle system for the next generation of CarPlay."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10278-Create-practical-workflows-in-Xcode-Cloud":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10278-Create-practical-workflows-in-Xcode-Cloud","title":"Create practical workflows in Xcode Cloud","url":"\/documentation\/wwdcnotes\/wwdc23-10278-create-practical-workflows-in-xcode-cloud","kind":"article","abstract":[{"type":"text","text":"Learn how Xcode Cloud can help teams of all shapes and sizes in their development process. We’ll share different ways to configure actions to help you create simple yet powerful workflows, and show you how to extend Xcode Cloud when you integrate with additional tools."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10203-Develop-your-first-immersive-app":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10203-Develop-your-first-immersive-app","title":"Develop your first immersive app","url":"\/documentation\/wwdcnotes\/wwdc23-10203-develop-your-first-immersive-app","kind":"article","abstract":[{"type":"text","text":"Find out how you can build immersive apps for visionOS using Xcode and Reality Composer Pro. We’ll show you how to get started with a new visionOS project, use Xcode Previews for your SwiftUI development, and take advantage of RealityKit and RealityView to render 3D content."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10175-Fix-failures-faster-with-Xcode-test-reports":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10175-Fix-failures-faster-with-Xcode-test-reports","title":"Fix failures faster with Xcode test reports","url":"\/documentation\/wwdcnotes\/wwdc23-10175-fix-failures-faster-with-xcode-test-reports","kind":"article","abstract":[{"type":"text","text":"Discover how you can find, debug, and fix test failures faster with the test report in Xcode and Xcode Cloud. Learn how Xcode identifies failure patterns to help you find the right place to start investigating. We’ll also show you how to use the UI automation explorer and video recordings to understand the events that led up to your UI test failure."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10004-Reduce-network-delays-with-L4S":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10004-Reduce-network-delays-with-L4S","title":"Reduce network delays with L4S","url":"\/documentation\/wwdcnotes\/wwdc23-10004-reduce-network-delays-with-l4s","kind":"article","abstract":[{"type":"text","text":"Streaming video, multiplayer games, and other real-time experiences depend on responsive, low latency networking. Learn how Low Latency, Low Loss, Scalable throughput (L4S) can reduce network delays and improve the overall experience in your app. We’ll show you how to set up and test your app, network, and server with L4S."}]},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=1818":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1818","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1818","title":"30:18 - Material properties","type":"link","titleInlineContent":[{"type":"text","text":"30:18 - Material properties"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10111-Go-beyond-the-window-with-SwiftUI":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10111-Go-beyond-the-window-with-SwiftUI","title":"Go beyond the window with SwiftUI","url":"\/documentation\/wwdcnotes\/wwdc23-10111-go-beyond-the-window-with-swiftui","kind":"article","abstract":[{"type":"text","text":"Get ready to launch into space — a new SwiftUI scene type that can help you make great immersive experiences for visionOS. We’ll show you how to create a new scene with ImmersiveSpace, place 3D content, and integrate RealityView. Explore how you can use the immersionStyle scene modifier to increase the level of immersion in an app and learn best practices for managing spaces, adding virtual hands with ARKit, adding support for SharePlay, and building an “out of this world” experience!"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10052-Discover-Calendar-and-EventKit":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10052-Discover-Calendar-and-EventKit","title":"Discover Calendar and EventKit","url":"\/documentation\/wwdcnotes\/wwdc23-10052-discover-calendar-and-eventkit","kind":"article","abstract":[{"text":"Discover how you can bring Calendar into your app and help people better manage their time. Find out how to create new events from your app, fetch events, and implement a virtual conference extension. We’ll also take you through some of the changes to calendar access levels that help your app stay connected without compromising the privacy of someone’s calendar data.","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=387":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=387","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=387","title":"6:27 - Components","type":"link","titleInlineContent":[{"type":"text","text":"6:27 - Components"}]},"WWDC23-10273-anatomy":{"identifier":"WWDC23-10273-anatomy","type":"image","alt":"Anatomy of a Reality Composer Pro package","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-anatomy.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit","title":"Enhance your spatial computing app with RealityKit","url":"\/documentation\/wwdcnotes\/wwdc23-10081-enhance-your-spatial-computing-app-with-realitykit","kind":"article","abstract":[{"type":"text","text":"Go beyond the window and learn how you can bring engaging and immersive 3D content to your apps with RealityKit. Discover how SwiftUI scenes work in tandem with RealityView and how you can embed your content into an entity hierarchy. We’ll also explore how you can blend virtual content and the real world using anchors, bring particle effects into your apps, add video content, and create more immersive experiences with portals."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10073-Design-for-spatial-input":{"url":"\/documentation\/wwdcnotes\/wwdc23-10073-design-for-spatial-input","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10073-Design-for-spatial-input","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Learn how to design great interactions for eyes and hands. We’ll share the design principles for spatial input, explore best practices around input methods, and help you create spatial experiences that are comfortable, intuitive, and satisfying."}],"title":"Design for spatial input"},"WWDC23-10273-component27":{"identifier":"WWDC23-10273-component27","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component27.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","kind":"symbol","role":"collection","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","title":"WWDC Notes","url":"\/documentation\/wwdcnotes"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10078-Design-considerations-for-vision-and-motion":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10078-Design-considerations-for-vision-and-motion","title":"Design considerations for vision and motion","url":"\/documentation\/wwdcnotes\/wwdc23-10078-design-considerations-for-vision-and-motion","kind":"article","abstract":[{"type":"text","text":"Learn how to design engaging immersive experiences for visionOS that respect the limitations of human vision and motion perception. We’ll show you how you can use depth cues, contrast, focus, and motion to keep people comfortable as they enjoy your apps and games."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10196-Dive-deeper-into-SwiftData":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10196-Dive-deeper-into-SwiftData","title":"Dive deeper into SwiftData","url":"\/documentation\/wwdcnotes\/wwdc23-10196-dive-deeper-into-swiftdata","kind":"article","abstract":[{"type":"text","text":"Learn how you can harness the power of SwiftData in your app. Find out how ModelContext and ModelContainer work together to persist your app’s data. We’ll show you how to track and make your changes manually and use SwiftData at scale with FetchDescriptor, SortDescriptor, and enumerate."}]},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=157":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=157","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=157","title":"2:37 - Load 3D content","type":"link","titleInlineContent":[{"type":"text","text":"2:37 - Load 3D content"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10091-Evolve-your-ARKit-app-for-spatial-experiences":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10091-Evolve-your-ARKit-app-for-spatial-experiences","title":"Evolve your ARKit app for spatial experiences","url":"\/documentation\/wwdcnotes\/wwdc23-10091-evolve-your-arkit-app-for-spatial-experiences","kind":"article","abstract":[{"type":"text","text":"Discover how you can bring your app’s AR experience to visionOS. Learn how ARKit and RealityKit have evolved for spatial computing: We’ll highlight conceptual and API changes for those coming from iPadOS and iOS and guide you to sessions with more details to help you bring your AR experience to this platform."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10185-Update-Live-Activities-with-push-notifications":{"abstract":[{"text":"Discover how you can remotely update Live Activities in your app when you push content through Apple Push Notification service (APNs). We’ll show you how to configure your first Live Activity push locally so you can quickly iterate on your implementation. Learn best practices for determining your push priority and configuring alerting updates, and explore how to further improve your Live Activities with relevance score and stale date.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10185-update-live-activities-with-push-notifications","role":"sampleCode","kind":"article","title":"Update Live Activities with push notifications","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10185-Update-Live-Activities-with-push-notifications"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10192-Explore-enhancements-to-RoomPlan":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10192-Explore-enhancements-to-RoomPlan","title":"Explore enhancements to RoomPlan","url":"\/documentation\/wwdcnotes\/wwdc23-10192-explore-enhancements-to-roomplan","kind":"article","abstract":[{"type":"text","text":"Join us for an exciting update to RoomPlan as we explore MultiRoom support and enhancements to room representations. Learn how you can scan areas with more detail, capture multiple rooms, and merge individual scans into one larger structure. We’ll also share workflows and best practices when working with RoomPlan results that you want to combine into your existing 3D model library."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10049-Improve-Core-ML-integration-with-async-prediction","title":"Improve Core ML integration with async prediction","url":"\/documentation\/wwdcnotes\/wwdc23-10049-improve-core-ml-integration-with-async-prediction","kind":"article","abstract":[{"text":"Learn how to speed up machine learning features in your app with the latest Core ML execution engine improvements and find out how aggressive asset caching can help with inference and faster model loads. We’ll show you some of the latest options for async prediction and discuss considerations for balancing performance with overall memory usage to help you create a highly responsive app. Discover APIs to help you understand and maximize hardware utilization for your models.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10089-Discover-Metal-for-immersive-apps":{"url":"\/documentation\/wwdcnotes\/wwdc23-10089-discover-metal-for-immersive-apps","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10089-Discover-Metal-for-immersive-apps","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Find out how you can use Metal to render fully immersive experiences for visionOS. We’ll show you how to set up a rendering session on the platform and create a basic render loop, and share how you can make your experience interactive by incorporating spatial input."}],"title":"Discover Metal for immersive apps"},"https://developer.apple.com/forums/create/question?&tag1=740030&tag2=266&tag3=796030":{"identifier":"https:\/\/developer.apple.com\/forums\/create\/question?&tag1=740030&tag2=266&tag3=796030","url":"https:\/\/developer.apple.com\/forums\/create\/question?&tag1=740030&tag2=266&tag3=796030","title":"Have a question? Ask with tag wwdc2023-10273","type":"link","titleInlineContent":[{"type":"text","text":"Have a question? Ask with tag wwdc2023-10273"}]},"https://developer.apple.com/videos/play/wwdc2023/10202":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202","title":"Explore materials in Reality Composer Pro - WWDC23","type":"link","titleInlineContent":[{"type":"text","text":"Explore materials in Reality Composer Pro - WWDC23"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10101-Customize-ondevice-speech-recognition":{"url":"\/documentation\/wwdcnotes\/wwdc23-10101-customize-ondevice-speech-recognition","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10101-Customize-ondevice-speech-recognition","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Find out how you can improve on-device speech recognition in your app by customizing the underlying model with additional vocabulary. We’ll share how speech recognition works on device and show you how to boost specific words and phrases for more predictable transcription. Learn how you can provide specific pronunciations for words and use template support to quickly generate a full set of custom phrases — all at runtime."}],"title":"Customize on-device speech recognition"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10072-Principles-of-spatial-design":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10072-Principles-of-spatial-design","title":"Principles of spatial design","url":"\/documentation\/wwdcnotes\/wwdc23-10072-principles-of-spatial-design","kind":"article","abstract":[{"text":"Discover the fundamentals of spatial design. Learn how to design with depth, scale, windows, and immersion, and apply best practices for creating comfortable, human-centered experiences that transform reality. Find out how you can use these spatial design principles to extend your existing app or bring a new idea to life.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10057-Unleash-the-UIKit-trait-system":{"url":"\/documentation\/wwdcnotes\/wwdc23-10057-unleash-the-uikit-trait-system","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10057-Unleash-the-UIKit-trait-system","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Discover powerful enhancements to the trait system in UIKit. Learn how you can define custom traits to add your own data to UITraitCollection, modify the data propagated to view controllers and views with trait override APIs, and adopt APIs to improve flexibility and performance. We’ll also show you how to bridge UIKit traits with SwiftUI environment keys to seamlessly access data from both UIKit and SwiftUI components in your app."}],"title":"Unleash the UIKit trait system"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10260-Get-started-with-building-apps-for-spatial-computing":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10260-Get-started-with-building-apps-for-spatial-computing","title":"Get started with building apps for spatial computing","url":"\/documentation\/wwdcnotes\/wwdc23-10260-get-started-with-building-apps-for-spatial-computing","role":"sampleCode","abstract":[{"text":"Get ready to develop apps and games for visionOS! Discover the fundamental building blocks that make up spatial computing — windows, volumes, and spaces — and find out how you can use these elements to build engaging and immersive experiences.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10188-Sync-to-iCloud-with-CKSyncEngine":{"url":"\/documentation\/wwdcnotes\/wwdc23-10188-sync-to-icloud-with-cksyncengine","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10188-Sync-to-iCloud-with-CKSyncEngine","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Discover how CKSyncEngine can help you sync people’s CloudKit data to iCloud. Learn how you can reduce the amount of code in your app when you let the system handle scheduling for your sync operations. We’ll share how you can automatically benefit from enhanced performance as CloudKit evolves, explore testing for your sync implementation, and more."}],"title":"Sync to iCloud with CKSyncEngine"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10113-Take-SwiftUI-to-the-next-dimension":{"url":"\/documentation\/wwdcnotes\/wwdc23-10113-take-swiftui-to-the-next-dimension","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10113-Take-SwiftUI-to-the-next-dimension","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Get ready to add depth and dimension to your visionOS apps. Find out how to bring three-dimensional objects to your app using volumes, get to know the Model 3D API, and learn how to position and animate content. We’ll also show you how to use UI attachments in RealityView and support gestures in your content."}],"title":"Take SwiftUI to the next dimension"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10051-Create-a-great-ShazamKit-experience":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10051-Create-a-great-ShazamKit-experience","title":"Create a great ShazamKit experience","url":"\/documentation\/wwdcnotes\/wwdc23-10051-create-a-great-shazamkit-experience","kind":"article","abstract":[{"type":"text","text":"Discover how your app can offer a great audio matching experience with the latest updates to ShazamKit. We’ll take you through matching features, updates to audio recognition, and interactions with the Shazam library. Learn tips and best practices for using ShazamKit in your audio apps."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10252-Build-programmatic-UI-with-Xcode-Previews":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10252-Build-programmatic-UI-with-Xcode-Previews","title":"Build programmatic UI with Xcode Previews","url":"\/documentation\/wwdcnotes\/wwdc23-10252-build-programmatic-ui-with-xcode-previews","kind":"article","abstract":[{"type":"text","text":"Learn how you can use the #Preview macro on Xcode 15 to quickly iterate on your UI code written in SwiftUI, UIKit, or AppKit. Explore a collage of unique workflows for interacting with views right in the canvas, find out how to view multiple variations of UI simultaneously, and discover how you can travel through your widget’s timeline in seconds to test the transitions between entries. We’ll also show you how to add previews to libraries, provide sample assets, and preview your views in your physical devices to leverage their capabilities and existing data."}]},"WWDC23-10273-component15":{"identifier":"WWDC23-10273-component15","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component15.jpg"}]},"WWDC23-10273-ecs":{"identifier":"WWDC23-10273-ecs","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-ecs.jpg"}]},"https://developer.apple.com/videos/play/wwdc2023/10080":{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080","title":"Build spatial experiences with RealityKit - WWDC23","type":"link","titleInlineContent":[{"type":"text","text":"Build spatial experiences with RealityKit - WWDC23"}]},"WWDC23-10273-component5":{"identifier":"WWDC23-10273-component5","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component5.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro":{"url":"\/documentation\/wwdcnotes\/wwdc23-10202-explore-materials-in-reality-composer-pro","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro","role":"sampleCode","kind":"article","type":"topic","abstract":[{"type":"text","text":"Learn how Reality Composer Pro can help you alter the appearance of your 3D objects using RealityKit materials. We’ll introduce you to MaterialX and physically-based (PBR) shaders, show you how to design dynamic materials using the shader graph editor, and explore adding custom inputs to a material so that you can control it in your visionOS app."}],"title":"Explore materials in Reality Composer Pro"},"WWDC23-10273-component3":{"identifier":"WWDC23-10273-component3","type":"image","alt":"Entity Component System","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component3.jpg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10125-Bring-your-game-to-Mac-Part-3-Render-with-Metal":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10125-Bring-your-game-to-Mac-Part-3-Render-with-Metal","title":"Bring your game to Mac, Part 3: Render with Metal","url":"\/documentation\/wwdcnotes\/wwdc23-10125-bring-your-game-to-mac-part-3-render-with-metal","kind":"article","abstract":[{"type":"text","text":"Discover how you can support Metal in your rendering code as we close out our three-part series on bringing your game to Mac. Once you’ve evaluated your existing Windows binary with the game porting toolkit and brought your HLSL shaders over to Metal, learn how you can optimally implement the features that high-end, modern games require. We’ll show you how to manage GPU resource bindings, residency, and synchronization. Find out how to optimize GPU commands submission, render rich visuals with MetalFX Upscaling, and more."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10050-Optimize-machine-learning-for-Metal-apps":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10050-Optimize-machine-learning-for-Metal-apps","title":"Optimize machine learning for Metal apps","url":"\/documentation\/wwdcnotes\/wwdc23-10050-optimize-machine-learning-for-metal-apps","kind":"article","abstract":[{"type":"text","text":"Discover the latest enhancements to accelerated ML training in Metal. Find out about updates to PyTorch and TensorFlow, and learn about Metal acceleration for JAX. We’ll show you how MPS Graph can support faster ML inference when you use both the GPU and Apple Neural Engine, and share how the same API can rapidly integrate your Core ML and ONNX models."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10083-Meet-Reality-Composer-Pro":{"url":"\/documentation\/wwdcnotes\/wwdc23-10083-meet-reality-composer-pro","title":"Meet Reality Composer Pro","type":"topic","abstract":[{"text":"Discover how to easily compose, edit, and preview 3D content with Reality Composer Pro. Follow along as we explore this developer tool by setting up a new project, composing scenes, adding particle emitters and audio, and even previewing content on device.","type":"text"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10263-Deploy-passkeys-at-work":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10263-Deploy-passkeys-at-work","role":"sampleCode","abstract":[{"type":"text","text":"Discover how you can take advantage of passkeys in managed environments at work. We’ll explore how passkeys can work well in enterprise environments through Managed Apple ID support for iCloud Keychain. We’ll also share how administrators can manage passkeys for specific devices using Access Management controls in Apple Business Manager and Apple School Manager."}],"title":"Deploy passkeys at work","url":"\/documentation\/wwdcnotes\/wwdc23-10263-deploy-passkeys-at-work","kind":"article","type":"topic"},"WWDC23-10273-yosemite":{"identifier":"WWDC23-10273-yosemite","type":"image","alt":"Yosemite National Park","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-yosemite.jpg"}]},"https://x.com/wrmultitudes":{"identifier":"https:\/\/x.com\/wrmultitudes","url":"https:\/\/x.com\/wrmultitudes","title":"X\/Twitter","type":"link","titleInlineContent":[{"type":"text","text":"X\/Twitter"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10226-Debug-with-structured-logging":{"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10226-Debug-with-structured-logging","title":"Debug with structured logging","url":"\/documentation\/wwdcnotes\/wwdc23-10226-debug-with-structured-logging","kind":"article","abstract":[{"text":"Discover the debug console in Xcode 15 and learn how you can improve your diagnostic experience through logging. Explore how you can navigate your logs easily and efficiently using advanced filtering and improved visualization. We’ll also show you how to use the dwim-print command to evaluate expressions in your code while debugging.","type":"text"}]}}}