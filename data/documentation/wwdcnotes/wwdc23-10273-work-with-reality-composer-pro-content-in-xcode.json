{"metadata":{"role":"sampleCode","modules":[{"name":"WWDC Notes"}],"title":"Work with Reality Composer Pro content in Xcode","roleHeading":"WWDC23"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"primaryContentSections":[{"content":[{"text":"Chapters:","anchor":"Chapters","level":2,"type":"heading"},{"type":"paragraph","inlineContent":[{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=0","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=157","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=387","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=720","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1671","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1818","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=2005","isActive":true,"type":"reference"}]},{"text":"Reality Composer Pro","anchor":"Reality-Composer-Pro","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"Reality Composer Pro is a developer tool for preparing RealityKit content to be used in spatial computing apps.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-composer"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The editor UI and features of Reality Composer Pro are covered in the sessions:"}]},{"type":"paragraph","inlineContent":[{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202"},{"type":"text","text":""},{"type":"text","text":"\n"},{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083"}]},{"type":"paragraph","inlineContent":[{"text":"We’re looking at a topographical map of Yosemite National Park.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-yosemite","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We’ve added a slider to morph between two different California landmarks. Now we’re looking at Catalina Island off the coast of Los Angeles."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-catalina"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We also have hovering 2D SwiftUI buttons positioned in 3D space to learn more about various points of interest in that map."}]},{"text":"Load 3D content","anchor":"Load-3D-content","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"In the above sessions, Apple made a Reality Composer Pro project that contains all the assets for a diorama. These tabs at the top each represent one root entity to be loaded at runtime.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Let’s load this scene named DioramaAssembled at runtime.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-dioramaAssembled"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We use entity’s asynchronous initializer to make us an entity with the contents from our Reality Composer Pro package."}]},{"syntax":"swift","code":["let entity = try await Entity(named: \"DioramaAssembled\", in: realityKitContentBundle)"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"We specify which entity we want to load using its string name, and we give it the bundle that our package produces. It will throw if it can’t find anything in our Reality Composer Pro project by that name. realityKitContentBundle is a constant value that it gets autogenerated in the Reality Composer Pro package. This goes in a RealityView make closure. A RealityView is a new kind of SwiftUI view."},{"type":"text","text":"\n"},{"type":"text","text":"It’s the bridge between the worlds of SwiftUI and RealityKit."}]},{"syntax":"swift","code":["RealityView { content in","\tdo {","\t\tlet entity = try await Entity(named: \"DioramaAssembled\", in: realityKitContentBundle)","\t\tcontent.add(entity)","\t} catch {","\t\t\/\/ Handle error","\t}","}"],"type":"codeListing"},{"text":"Anatomy of a Reality Composer Pro package","anchor":"Anatomy-of-a-Reality-Composer-Pro-package","level":2,"type":"heading"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Put assets into a Swift Package, with an .rkassets directory inside it, like this."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-anatomy"}]},{"type":"paragraph","inlineContent":[{"text":"Xcode compiles the .rkassets folder into a format that’s faster to load at runtime.","type":"text"},{"text":"\n","type":"text"},{"text":"The entity we just loaded is actually the root of a larger entity hierarchy. It has child entities and they in turn have child entities. If we wanted to address one of the entities lower down in the hierarchy, we could give it a name in Reality Composer Pro, and then at runtime, we could ask the scene to find that entity by its name.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-anatomy2","type":"image"}]},{"text":"Entity Component System.","anchor":"Entity-Component-System","level":2,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"Entities are a part of ECS, which stands for Entity Component System.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"ECS has some close parallels to object-oriented programming but is different in some key ways. In the object-oriented programming world, the object has properties which are attributes that define its nature, and it has its own functionality written in a class that defines the object.","type":"text"}]},{"text":"Entity","anchor":"Entity","level":3,"type":"heading"},{"type":"paragraph","inlineContent":[{"type":"text","text":"In the ECS world, an entity is any thing you see in the scene. They can also be invisible. They don’t hold attributes or data though. We put our data into components instead."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-ecs"}]},{"text":"Components","anchor":"Components","level":3,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"Components can be added to or removed from entities at any time during app execution, which provides a way to dynamically change the nature of an entity.","type":"text"}]},{"text":"System","anchor":"System","level":3,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"A system is where behavior lives. It has an update function that’s called once per frame. That’s where we put our ongoing logic. In the system, we query for all entities that have a certain component on them, or configuration of components, and then we perform some action and store the updated data back into those components.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"For a more in-depth discussion on ECS, check out:"}]},{"type":"paragraph","inlineContent":[{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074","isActive":true,"type":"reference"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080","isActive":true,"type":"reference"}]},{"text":"Components","anchor":"Components","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"type":"text","text":"To add a component to an entity in Swift, we use entity.components.set() and provide the component value."}]},{"syntax":"swift","code":["let component = MyComponent()","entity.components.set(component)"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"To do the same in Reality Composer Pro, select the entity you want either in the viewport or in the hierarchy."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"Then, at the bottom of the Inspector Panel, click the Add Component button to bring up a list of all RealityKit’s available components.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component2"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We can add as many components to an entity as we want, and we can only add one of each type; it’s a set. We’ll also see any custom components we’ve made in this list as well. Let’s create our own custom components."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We’re going to make those floating buttons that hover over specific points on our terrain. We’ll prepare a lot of that UI and functionality in code, but we will mark these entities in Reality Composer Pro."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component3","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"To do this, we’re going to","type":"text"}]},{"items":[{"content":[{"inlineContent":[{"text":"add entities at locations above our terrain map, which will signify to the app that these are the places we want to show our floating buttons.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"create a point of interest component to house our information about each place.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"open the PointOfInterestComponent.swift in Xcode to edit it, adding properties like a name and a description."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"In Reality Composer Pro, we’ll add our new PointOfInterestComponent to each of our new entities","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"fill in the properties’ values."}]}]}],"type":"unorderedList"},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component4"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s make our first location marker entity, Ribbon Beach, which is a place on Catalina Island. We click the plus menu and select Transform to make us a new invisible entity."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component5"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We can name our entity Ribbon_Beach."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component6","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"Let’s put this entity where Ribbon Beach actually is on the island.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component7"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We click on the Add Component button, but this time, we select New Component because we’re going to make our own."},{"type":"text","text":" "},{"type":"text","text":"Let’s give it a name, PointOfInterest."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component8"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now it shows up in the Inspector Panel just like our other components do."},{"type":"text","text":" "},{"type":"text","text":"But what’s this count property?"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component9","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s open our new component in Xcode. In Xcode, we see that Reality Composer Pro created PointOfInterestComponent.swift for us."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Reality Composer Pro projects are Swift packages, and the Swift code we just generated lives here in the package. Looking at the template code, we see that that’s where the count property came from."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component10","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s have another property instead. We want each point of interest to know which map it’s associated with so that when you change maps, we can fade out the old points of interest and fade in the appropriate ones. So we add an enumeration property, var region.  Let’s make our enum region up here…  …and give it two cases, since we’re only building two maps right now: Catalina and Yosemite.  It can serialize as a string. We also conform it to the Codable protocol so that Reality Composer Pro can see it and serialize instances of it."}]},{"syntax":"swift","code":["import RealityKit","","public enum Region: String, Codable {","\tcase catalina","\tcase yosemite"," }"," ","\/\/ Ensure you register this component in your app's delegate using:","\/\/ PointOfInterestComponent.registerComponent ()","public struct Point0fInterestComponent: Component, Codable { ","\tvar region: Region = .yosemite","\t","\tpublic init() {","\t}","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component11","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"Back in Reality Composer Pro, the count property has gone away and our new region property shows up. It has a default value of yosemite because that’s what we initialized in the code, but we can override it here for this particular entity.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component12","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"If we override it, this value will only take effect on this particular entity. The rest of the point of interest components will have the default value of yosemite unless we override them too."}]},{"type":"paragraph","inlineContent":[{"text":"We’re using our PointOfInterestComponent like a signifier, a marker that we stick on these entities. These entities act like placeholders for where we’ll put our SwiftUI buttons at runtime. We add our other Catalina Island points of interest the same way we just added Ribbon Beach.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"The app doesn’t do anything yet. That’s because we haven’t written any code to handle these point of interest components yet.","type":"text"}]},{"text":"User interface","anchor":"User-interface","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"type":"text","text":"There is a new way of putting SwiftUI content into a RealityKit scene. This is called the Attachments API. We’re going to combine attachments with our PointOfInterestComponent to create hovering buttons with custom data at runtime."}]},{"type":"paragraph","inlineContent":[{"text":"Attachments are a part of the RealityView. We first look at a simplified example:","type":"text"}]},{"syntax":"swift","code":["RealityView { _,_ in","\t\/\/ load entities from your Reality Composer Pro package bundle","} update: { _,_ in","\t\/\/ update your RealityKit entities","} attachments: {","\t\/\/ SwiftUI Views","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The RealityView initializer takes three parameters: a make closure, an update closure, and an attachments ViewBuilder. We now add a bare-minimum implementation of creating an Attachment View, a green SwiftUI button, and adding it to our RealityKit scene."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"In the Attachments ViewBuilder, we make a normal SwiftUI view; we can use view modifiers and gestures and all the rest that SwiftUI gives us. We tag our View with any unique hashable. I’ve chosen to tag this button view with a fish emoji."}]},{"syntax":"swift","code":["RealityView { _,_ in","\t\/\/ load entities from your Reality Composer Pro package bundle","} update: { _,_ in","\t\/\/ update your RealityKit entities","} attachments: {","\tButton { ... }","\t\t.background(.green)","\t\t.tag (\"🐠\")","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Later, when SwiftUI invokes the update closure, our button View has become an entity. It’s stored in the attachments parameter to this closure, and we get it using the tag we gave it before. We can then treat it like any other entity, add it as a child of any existing entity in our scene, or add it as a new top-level entity in the content’s entities collection."}]},{"syntax":"swift","code":["RealityView { _,_ in","\t\/\/ load entities from your Reality Composer Pro package bundle","} update: { _,_ in","\tif let attachmentEntity = attachments.entity(for: \"S\") {","\t\tcontent.add(attachmentEntity)","\t}","} attachments: {","\tButton { ... }","\t\t.background(.green)","\t\t.tag (\"🐠\")","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"And since it’s become a regular entity, we can set its position so it shows up where we want in 3D, and we can add any components we want as well."}]},{"text":"RealityView attachments","anchor":"RealityView-attachments","level":2,"type":"heading"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Here’s how data flows from one part of the RealityView to another."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The three parameters of this RealityView initializer:"}]},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The first is make, where the initial set-up scene from your Reality Composer Pro bundle is loaded as an entity and then added to the RealityKit scene."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The second is update, which is a closure that will be called when there are changes to the view’s state. Here, we can change things about the entities, like properties in their components, their position, and even add or remove entities from the scene. This update closure is not executed every frame. It’s called whenever the SwiftUI view state changes.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"The third is the attachments ViewBuilder. This is where we can make SwiftUI views to put into your RealityKit scene.","type":"text"}],"type":"paragraph"}]}],"type":"unorderedList"},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component13","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"SwiftUI views start out in the attachments ViewBuilder, then they are delivered in the update closure in the attachments parameter."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component14"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Here, asking the attachments parameter if it has an entity using the same tag we gave to the button in the attachments ViewBuilder."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component15"}]},{"type":"paragraph","inlineContent":[{"text":"If there is one, we get a RealityKit entity. In the update closure, we set its 3D position and add it to your RealityKit scene so we can see it floating in space wherever we want.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Here, we’ve added the button entity as a child of a sphere entity positioned 0.2 meters above its parent."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component16","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"The make closure also has an attachments parameter. This one is for adding any attachments that we have ready to go at the time this view is first evaluated, because the make closure is only run once.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component17"}]},{"type":"paragraph","inlineContent":[{"text":"This is the general flow of a RealityView, let’s check the update closure. The parameter to make and update closures is a RealityKitContent. When adding an entity to the RealityKit content, it becomes a top-level entity in the scene.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Likewise, from the update function, adding an entity to the content gives a new top-level entity in the scene. While the make closure will only be called once, the update closure will be called more than once. If we create a new entity in the update closure and add it to the content there, we’ll get duplicates of that entity. (not good.)"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component18"}]},{"type":"paragraph","inlineContent":[{"text":"To guard against that, we should only add entities to content that are created somewhere that’s only run once. We don’t need to check if the content.entities already contains the entity.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"It’s a no-op if we call add with the same entity twice, like a set. It’s the same when parent an entity to an existing entity in the scene – it won’t be added twice."}]},{"syntax":"swift","code":["let myEntity = Entity()","","RealityView { content, in","\tif let entity = try? await Entity(named: \"MyScene\", in: realityKitContentBundle){","\t\tcontent.add(entity)","\t}","} update: { content, attachments in","\t","\tif let attachmentEntity = attachments.entity(for: \"🐠\") {","\t\tcontent.add(attachmentEntity)","\t}\t","\t","\tcontent.add(myEntity)","","} attachments: {","\tButton { ... }","\t\t.background(.green)","\t\t.tag(\"🐠\")","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component19"}]},{"type":"paragraph","inlineContent":[{"text":"Attachment entities are not created by us; they’re created by the RealityView for each attachment view that we provide in your attachments ViewBuilder. That means it’s safe to add them to the content in our update closure without checking if it’s already there.","type":"text"}]},{"text":"Data-driven","anchor":"Data-driven","level":2,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"So, that was how we’d write code if we wanted to hardcode our points of interest in the attachments ViewBuilder. But let’s make it more flexible.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"To make it data-driven, we need the code to read the data that we set up in the Reality Composer Pro scene. We’ll be creating our attachment views dynamically.","type":"text"}]},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Add invisible entities"},{"type":"text","text":" "},{"type":"text","text":"In Reality Composer Pro, we already set up our placeholder entity for Ribbon Beach, and we’ll do the same for the other points of interest that we want to highlight in our diorama. We’ll fill out all the info each one needs, like their name and which map they belong on."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Query for entities"},{"type":"text","text":" "},{"type":"text","text":"Now, in code, we’ll query for those entities"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Create a SwiftUI View for each"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Store Views in @State collection","type":"text"},{"text":" ","type":"text"},{"text":"In order to get SwiftUI to invoke our attachments ViewBuilder every time we add a new button to our collection, we’ll add the @State property wrapper to this collection.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"We’ll serve those buttons up to the attachments ViewBuilder."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Add them as entities in the update closure of our RealityView, We’ll add each one as a child of the marker entities we set up in Reality Composer Pro."}]}]}],"type":"unorderedList"},{"type":"paragraph","inlineContent":[{"text":"We’re making use of the Transform Component here, which all entities have by default.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component21"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Then we add our PointOfInterestComponent to each of them. In our code, we get references to these entities by querying for all entities in the scene that have the PointOfInterestComponent on them. The query returns the three invisible entities we set up in Reality Composer Pro."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component22"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We create a new SwiftUI view for each one and store them in a collection."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component23"}]},{"type":"paragraph","inlineContent":[{"text":"To get our buttons into our RealityView, we’ll make use of the SwiftUI view-updating flow. This means adding the property wrapper @State to the collection of buttons in our View. The @State property wrapper tells SwiftUI that when we add items to this collection, SwiftUI should trigger a view update on our ImmersiveView. That will cause SwiftUI to evaluate our attachments ViewBuilder and our update closure again.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component24","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"The RealityView’s attachments ViewBuilder is where we’ll declare to SwiftUI that we want these buttons to be made into entities. Our RealityView’s update closure will be called next, and our buttons will be delivered to us as entities.   They’re no longer SwiftUI Views now. That’s why we can add them to our entity hierarchy.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component25"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"In the update closure, we add our attachment entities to the scene, positioned floating above each of our invisible entities. Now they will show up visually when we look at our diorama scene."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component26"}]},{"type":"paragraph","inlineContent":[{"text":"Let’s see how each of these steps is done.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"First, we mark our invisible entities in our Reality Composer Pro scene."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component27"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"To find our entities that we marked, we’ll make an EntityQuery. We’ll use it to ask for all entities that have a PointOfInterestComponent on them."}]},{"syntax":"swift","code":["static let markersQuery = EntityQuery(where: .has (PointOfInterestComponent.self))"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"We’ll then iterate through our QueryResult and create a new SwiftUI View for each entity in our scene that has a PointOfInterestComponent on it. We’ll fill it in with information we grab from the component, the data we entered in Reality Composer Pro.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"That view is going to be one of our attachments, so we put a tag on it. We’ll use an ObjectIdentifier rather than a fish emoji. Here’s the part where we make our collection of SwiftUI Views. We’ll call it attachmentsProvider since it will provide our attachments to the RealityView’s attachments ViewBuilder. We’ll then store our view in the attachmentsProvider.","type":"text"}]},{"syntax":"swift","code":["static let markersQuery = EntityQuery(where: .has (PointOfInterestComponent.self))","@State var attachmentsProvider = AttachmentsProvider ()","","rootEntity.scene?.performQuery(Self.markersQuery).forEach { entity in","\tguard let pointOfInterest = entity.components[PointOfInterestComponent.self] else { return }","\t","\tlet attachmentTag: ObjectIdentifier = entity.id","\t","\tlet view = LearnMoreView(name: pointOfInterest.name, description: pointOfInterest.description)","\t\t.tag(attachmentTag)","\t","\tattachmentsProvider.attachments[attachmentTag]= AnyView(view)"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"Let’s take a look at that collection type.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"AttachmentsProvider has a dictionary of attachment tags to views. We type-erased our view so we can put other kinds of views in there besides our LearnMoreView. We have a computed property called sortedTagViewPairs that returns an array of tuples – tags and their corresponding views – in the same order every time. Then, in the attachments ViewBuilder, we’ll ForEach over our collection of attachments that we made."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This tells SwiftUI that we want one view for each of the pairs we’ve given it, and we provide our views from our collection. We’re letting the ObjectIdentifier do double duty here as both an attachment tag for the view, and as an identifier for the ForEach structure."}]},{"syntax":"swift","code":["@Observable final class AttachmentsProvider {","\tvar attachments: [ObjectIdentifier: AnyView] = [:]","\tvar sortedTagViewPairs: [(tag: ObjectIdentifier, view: AnyView)] { ... }","}","","...","","@State var attachmentsProvider = AttachmentsProvider()","","RealityView { _, _ in","","} update: { _, _ in","","} attachments: {","\tForEach(attachmentsProvider.sortedTagViewPairs, id: \\.tag) { pair in","\t\tpair.view","\t}","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"why didn’t we just add a tag property to our PointOfInterestComponent instead?","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component28","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"Attachment tags need to be unique, both for the ForEach struct and the attachments mechanism to work. And since all the properties in our custom component will be shown in Reality Composer Pro’s Inspector Panel when we add the component to an entity, that means the attachmentTag would show up there too.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Entities conform to the Identifiable protocol, so they have identifiers that are unique automatically. We can get this identifier at runtime from the entity.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"To have the attachmentTag property not show up in Reality Composer Pro, we use a technique called “design-time versus runtime components.”"}]},{"type":"paragraph","inlineContent":[{"text":"We’ll separate our data into two different components, one for design-time data that we want to arrange in Reality Composer Pro, and one for runtime data that we will attach to those same entities dynamically at runtime.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"This is for properties that we don’t want to show in our Inspector Panel in Reality Composer Pro. So we’ll define a new component, PointOfInterestRuntimeComponent, and move our attachment tag inside it.","type":"text"}]},{"syntax":"swift","code":["\/\/ Design-time component","public struct PointOfInterestComponent: Component, Codable {","\tpublic var region: Region = .yosemite","\tpublic var name: String = \"Ribbon Beach\"","\tpublic var description: String?","}","","\/\/ Run-time component","public struct PointOfInterestRuntimeComponent: Component {","\tpublic let attachmentTag: ObjectIdentifier","}"],"type":"codeListing"},{"text":"Custom components in Reality Composer Pro","anchor":"Custom-components-in-Reality-Composer-Pro","level":2,"type":"heading"},{"type":"paragraph","inlineContent":[{"type":"text","text":"So we’ll define a new component, PointOfInterestRuntimeComponent, and move our attachment tag inside it. Reality Composer Pro automatically builds the component UI for you based on what it reads in your Swift package."}]},{"type":"paragraph","inlineContent":[{"text":"It inspects the Swift code in your package and makes any codable components it finds available for you to use in your scenes. Here we’re showing four components. Components A and B are in our Xcode project, but they are not inside the Reality Composer Pro package, so they won’t be available for you to attach to your entities in Reality Composer Pro.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Component C is inside the package but it is not codable, so Reality Composer Pro will ignore it. Of the four components shown here, only Component D will be shown in the list in Reality Composer Pro because it is within the Swift package and it is a codable component.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"That one is our design-time component, while all the others may be used as runtime components."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-component29"}]},{"type":"paragraph","inlineContent":[{"text":"Design-time components are for housing simpler data, such as ints, strings, and SIMD values, things that 3D artists and designers will make use of.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"We will see an error in your Xcode project if you add a property to your custom component that’s of a type that Reality Composer Pro won’t serialize.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We’ll first add our PointOfInterest runtime component to our entity and then use the runtime component to help us match up our attachment entities with their corresponding points of interest on the diorama."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Here’s where our runtime component comes in. We’re reading in our PointOfInterest entities and creating our attachment views. We queried for all our design-time components, and now we’ll make a new corresponding runtime component for each of them. We store our attachmentTag in our runtime component, and we store our runtime component on that same entity. In this way, the design-time component is like a signifier. It tells our app that it wants an attachment made for it."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The runtime component handles any other kinds of data we need during app execution, but don’t want to store in the design-time component."}]},{"syntax":"swift","code":["static let markersQuery = EntityQuery(where: .has(PointOfInterestComponent.self))","@State var attachmentsProvider = AttachmentsProvider()","","rootEntity.scene?.performQuery(Self.markersQuery).forEach { entity in","\tguard let pointOfInterest = entity.components[PointOfInterestComponent.self] else { return }","","\tlet attachmentTag: ObjectIdentifier = entity.id","","\tlet view = LearnMoreView(name: pointOfInterest.name, description: pointOfInterest.description)","\t\t.tag(attachmentTag)","\t\t","\tattachmentsProvider.attachments[attachmentTag] = AnyView(view)","\tlet runtimeComponent = PointOfInterestRuntimeComponent(attachmentTag: attachmentTag)","\tentity.components.set(runtimeComponent)","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"In our RealityView, we have one more step before we see our attachment entities show up in our scene. Once we’ve provided our SwiftUI Views in the attachments ViewBuilder, SwiftUI will call our RealityView’s update closure and give us our attachments as RealityKit entities.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"But if we just add them to the content without positioning them, they’ll all show up sitting at the origin of the scene, position 0, 0, 0."}]},{"type":"paragraph","inlineContent":[{"text":"That’s not where we want them. We want them to float above each point of interest on the terrain. We need to match up our attachment entities with our invisible point of interest entities that we set up in Reality Composer Pro.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The runtime component we put on the invisible entity has our tag in it. That’s how we’ll match up which attachmentEntity goes with each point of interest entity. We query for all our PointOf InterestRuntimeComponents, we get that runtime component from each entity returned by the query, then we use the component’s attachmentTag property to get our attachmentEntity from the attachments parameter to the update closure."}]},{"type":"paragraph","inlineContent":[{"text":"Now we add our attachmentEntity to the content and position it half a meter above the point of interest entity.","type":"text"}]},{"syntax":"swift","code":["static let runtimeQuery = EntityQuery(where: .has(PointOfInterestRuntimeComponent.self))","","RealityView { _, _ in","","} update: { content, attachments in","\t","\trootEntity.scene?.performQuery(Self.runtimeQuery).forEach { entity in","\t\tguard let component = entity. components[Point0fInterestRuntimeComponent.self],","\t\t\tlet attachmentEntity = attachments.entity(for: component.attachmentTag) else {","\t\t\t\treturn","\t\t\t} content.add(attachmentEntity)","\t\t\tattachmentEntity.setPosition([O, 0.5, 0], relativeTo: entity)","\t}","} attachments: {","\tForEach(attachmentsProvider.sortedTagViewPairs,id:\\.tag) { pair in","\t\tpair.view","\t}\t","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s run our app again…"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-component30","type":"image"}]},{"text":"Play audio","anchor":"Play-audio","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"To set up something that plays audio in Reality Composer Pro, we can bring in an audio entity by clicking the plus button, selecting Audio, and then selecting Ambient Audio.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-audio","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This creates a regular invisible entity with an AmbientAudioComponent on it. Let’s name it OceanEmitter. It will be used to play ocean sounds for Catalina Island."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-audio2"}]},{"type":"paragraph","inlineContent":[{"text":"You need to add an audio file.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We can preview audio component by selecting a sound in the Preview menu of the component in the Inspector Panel, but this won’t automatically play the selected sound when the entity is loaded in the app."}]},{"type":"paragraph","inlineContent":[{"text":"For that, we need to load the audio resource and tell it to play. To play this sound, we’ll get a reference to the entity that we put the audio component on. We’ve named our entity OceanEmitter, so we’ll find our entity by that name. We load the sound file using the AudioFileResource initializer, passing it the full path to the audio file resource prim in our scene. We give it the name of the .usda file that contains it in our Reality Composer Pro project. In our case, that’s our main scene named DioramaAssembled.usda.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We create an audioPlaybackController by calling entity.prepareAudio so we can play, pause, and stop this sound."}]},{"syntax":"swift","code":["func playOceanSound() {","","\tguard let entity = entity.findEntity(named: \"OceanEmitter\"),","\t\tlet resource = try? AudioFileResource(named: \"\/Root\/Resources\/Ocean_Sounds_wav\", ","\t\t\tfrom: \"DioramaAssembled.usda\",","\t\t\tin: RealityContent.realityContentBundle) else { return }","","\tlet audioPlaybackController = entity.prepareAudio(resource)","\taudioPlaybackController.play ()"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now, we’re going to crossfade between two audio sources. We add a forest audio emitter the same way we added our ocean emitter entity."}]},{"text":"Material properties","anchor":"Material-properties","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"text":"How we’re morphing our terrain using the slider, and including audio in this transition. We’ll use a property from our Shader Graph material to morph between the two terrains. Let’s see how we do that.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"In the other session, the Apple engineer Niels, created a geometry modifier using Reality Composer Pro’s Shader Graph. We can hook it up to our scene and drive some of the parameters at runtime. We want to connect this Shader Graph material with a slider. To do that, we need to promote an input node. Command-click on the node and select Promote.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-materials","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"This tells the project that we intend to supply data at runtime to this part of the material. We’ll name this promoted node Progress, so we can address it by that name at runtime.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC23-10273-materials2","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"We can now dynamically change this value in code. We get a reference to the entity that our material is on. Then we get its ModelComponent, which is the RealityKit component that houses materials. From the ModelComponent, we get its first material.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"There’s only one on this particular entity. We cast it to type ShaderGraphMaterial. Now, we can set a new value for our parameter with name Progress. Finally, we store the material back onto the ModelComponent, and the ModelComponent back onto the terrain entity.","type":"text"}]},{"syntax":"swift","code":["guard let terrain = rootEntity.findEntity(named: \"DioramaTerrain\"),","\tvar modelComponent = terrain.components[ModelComponent.self,","\tvar shaderGraphMaterial = modelComponent.materials.first","\tas? ShaderGraphMaterial else { return }","do {","\ttry shaderGraphMaterial.setParameter(name: \"Progress\", value: .float(sliderValue))","\tmodelComponent.materials = [shaderGraphMaterial]","\tterrain.components.set(modelComponent)","} catch { }"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"Now we’ll hook that up to our SwiftUI slider. Whenever the slider’s value changes, we’ll grab that value, which is in the range 0 to 1, and feed it into our ShaderGraphMaterial.","type":"text"}]},{"syntax":"swift","code":["@State private var sliderValue: Float = 0.0","","Slider (value: $sliderValue, in: (0.0)... (1.0)) ","\t.onChange (of: sliderValue) { _, _ in","\t\tguard let terrain = rootEntity.findEntity(named: \"DioramaTerrain\"),","\t\t\tvar modelComponent = terrain.components[ModelComponent.self,","\t\t\tvar shaderGraphMaterial = modelComponent.materials.first","\t\t\tas? ShaderGraphMaterial else { return }","\t\tdo {","\t\t\ttry shaderGraphMaterial.setParameter(name: \"Progress\", value: \t.float(sliderValue))","\t\t\tmodelComponent.materials = [shaderGraphMaterial]","\t\t\tterrain.components.set(modelComponent)","\t\t} catch { }","\t}","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"Crossfading between the ambient audio tracks for the two terrains.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Because we’ve also put an AmbientAudioComponent on our two audio entities, ocean and forest, we can adjust how loud the sound plays using the gain property on them.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"We’ll query for all entities – all two at this point, our ocean and our forest – that have the AmbientAudioComponent on them. Plus, we added another custom component called RegionSpecificComponent so we can mark the entities that go with one region or the other. We get a mutable copy of our audioComponent because we’re going to change it and store it back onto our entity.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"We call a function that calculates what the gain should be given a region and a sliderValue. We set the gain value onto the AmbientAudioComponent, and then we store the component back onto the entity.","type":"text"}]},{"syntax":"swift","code":["@State private var sliderValue: Float = 0.0","static let audioQuery = EntityQuery(where: .has(RegionSpecificComponent.self)","\t&& .has (AmbientAudioComponent.self))","","Slider (value: $sliderValue, in: (0.0)... (1.0)) ","\t.onChange (of: sliderValue) { _, _ in","","\/\/ ... Change the terrain material property ...","","\t\trootEntity?.scene?.performQuery(Self.audioQuery).forEach({audioEmitter in","\t\t\tguard var audioComponent = audioEmitter.components[AmbientAudioComponent.self],","\t\t\t\tlet regionComponent = audioEmitter.components[RegionSpecificComponent.self]","\t\t\telse { return }","\t\t\t","\t\t\tlet gain = regionComponent.region.gain(forSliderValue: sliderValue)","\t\t\taudioComponent.gain = gain","\t\t\taudioEmitter.components.set(audioComponent)","\t\t})","\t}","}"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"In action:","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Moving the slider, we can see our Shader Graph material changing the geometry of the terrain map, and we can hear the forest sound fading out and the ocean sound coming in."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC23-10273-audio3"}]},{"text":"Wrap Up","anchor":"Wrap-Up","level":2,"type":"heading"},{"items":[{"content":[{"inlineContent":[{"type":"text","text":"Use Reality Composer Pro to prepare RealityKit content"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Place attachment placeholders in Reality Composer Pro"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Load and play audio","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Drive material properties"}]}]}],"type":"unorderedList"},{"text":"Resources","anchor":"Resources","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/forums\/create\/question?&tag1=740030&tag2=266&tag3=796030"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10273"}]},{"text":"Check out also","anchor":"Check-out-also","level":1,"type":"heading"},{"type":"paragraph","inlineContent":[{"overridingTitleInlineContent":[{"text":"Build spatial experiences with RealityKit - WWDC23","type":"text"}],"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080","type":"reference","isActive":true,"overridingTitle":"Build spatial experiences with RealityKit - WWDC23"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10203","type":"reference","isActive":true},{"text":"","type":"text"},{"text":"\n","type":"text"},{"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10081","type":"reference","isActive":true},{"text":"","type":"text"},{"text":"\n","type":"text"},{"overridingTitleInlineContent":[{"text":"Explore materials in Reality Composer Pro - WWDC23","type":"text"}],"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202","type":"reference","isActive":true,"overridingTitle":"Explore materials in Reality Composer Pro - WWDC23"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"overridingTitleInlineContent":[{"type":"text","text":"Meet Reality Composer Pro - WWDC23"}],"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083","type":"reference","isActive":true,"overridingTitle":"Meet Reality Composer Pro - WWDC23"},{"text":"","type":"text"},{"text":"\n","type":"text"},{"overridingTitleInlineContent":[{"text":"Dive into RealityKit 2 - WWDC21","type":"text"}],"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074","type":"reference","isActive":true,"overridingTitle":"Dive into RealityKit 2 - WWDC21"}]},{"text":"Written By","anchor":"Written-By","level":2,"type":"heading"},{"columns":[{"size":1,"content":[{"inlineContent":[{"identifier":"multitudes","type":"image"}],"type":"paragraph"}]},{"size":4,"content":[{"anchor":"laurent-b","type":"heading","text":"laurent b","level":3},{"inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes","overridingTitle":"Contributed Notes","type":"reference","isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/github.com\/multitudes","type":"reference","isActive":true},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/laurentbrusa.hashnode.dev\/","type":"reference","isActive":true},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/x.com\/wrmultitudes","type":"reference","isActive":true}],"type":"paragraph"}]}],"numberOfColumns":5,"type":"row"},{"type":"paragraph","inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"isActive":true,"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference"}]},{"text":"Related Sessions","anchor":"Related-Sessions","level":2,"type":"heading"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10080-Build-spatial-experiences-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10203-Develop-your-first-immersive-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10074-Dive-into-RealityKit-2"],"style":"list","type":"links"},{"type":"small","inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}],"kind":"content"}],"schemaVersion":{"minor":3,"patch":0,"major":0},"sections":[],"abstract":[{"text":"Learn how to bring content from Reality Composer Pro to life in Xcode. We’ll show you how to load 3D scenes into Xcode, integrate your content with your code, and add interactivity to your app. We’ll also share best practices and tips for using these tools together in your development workflow.","type":"text"}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"overridingTitle":"Watch Video (34 min)","type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc23\/10273"}},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc23-10273-work-with-reality-composer-pro-content-in-xcode"],"traits":[{"interfaceLanguage":"swift"}]}],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10273-Work-with-Reality-Composer-Pro-content-in-Xcode","interfaceLanguage":"swift"},"kind":"article","references":{"https://developer.apple.com/videos/play/wwdc2023/10083":{"title":"Meet Reality Composer Pro - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083","titleInlineContent":[{"type":"text","text":"Meet Reality Composer Pro - WWDC23"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10083"},"WWDC23.jpeg":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23.jpeg"}],"identifier":"WWDC23.jpeg","alt":null,"type":"image"},"WWDC23-10273-component12":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component12.jpg"}],"identifier":"WWDC23-10273-component12","alt":"Entity Component System","type":"image"},"WWDCNotes.png":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"identifier":"WWDCNotes.png","alt":null,"type":"image"},"WWDC23-10273-materials2":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-materials2.jpg"}],"identifier":"WWDC23-10273-materials2","alt":"Material properties","type":"image"},"https://developer.apple.com/forums/tags/wwdc2023-10273":{"title":"Search the forums for tag wwdc2023-10273","identifier":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10273","titleInlineContent":[{"type":"text","text":"Search the forums for tag wwdc2023-10273"}],"type":"link","url":"https:\/\/developer.apple.com\/forums\/tags\/wwdc2023-10273"},"https://developer.apple.com/videos/play/wwdc2023/10080":{"title":"Build spatial experiences with RealityKit - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080","titleInlineContent":[{"type":"text","text":"Build spatial experiences with RealityKit - WWDC23"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10080"},"WWDC23-10273-component27":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component27.jpg"}],"identifier":"WWDC23-10273-component27","alt":"Entity Component System","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10083-Meet-Reality-Composer-Pro":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10083-Meet-Reality-Composer-Pro","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10083-meet-reality-composer-pro","type":"topic","abstract":[{"type":"text","text":"Discover how to easily compose, edit, and preview 3D content with Reality Composer Pro. Follow along as we explore this developer tool by setting up a new project, composing scenes, adding particle emitters and audio, and even previewing content on device."}],"title":"Meet Reality Composer Pro"},"https://developer.apple.com/videos/play/wwdc2021/10074":{"title":"Dive into RealityKit 2 - WWDC21","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074","titleInlineContent":[{"type":"text","text":"Dive into RealityKit 2 - WWDC21"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2021\/10074"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit":{"title":"Enhance your spatial computing app with RealityKit","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10081-enhance-your-spatial-computing-app-with-realitykit","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10081-Enhance-your-spatial-computing-app-with-RealityKit","abstract":[{"text":"Go beyond the window and learn how you can bring engaging and immersive 3D content to your apps with RealityKit. Discover how SwiftUI scenes work in tandem with RealityView and how you can embed your content into an entity hierarchy. We’ll also explore how you can blend virtual content and the real world using anchors, bring particle effects into your apps, add video content, and create more immersive experiences with portals.","type":"text"}],"kind":"article"},"https://github.com/multitudes":{"title":"GitHub","identifier":"https:\/\/github.com\/multitudes","titleInlineContent":[{"type":"text","text":"GitHub"}],"type":"link","url":"https:\/\/github.com\/multitudes"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"images":[{"identifier":"WWDC23-Icon.png","type":"icon"},{"identifier":"WWDC23.jpeg","type":"card"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","role":"collectionGroup","url":"\/documentation\/wwdcnotes\/wwdc23","type":"topic","abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"SwiftData"},{"type":"text","text":", "},{"type":"codeVoice","code":"Observation"},{"type":"text","text":", "},{"type":"codeVoice","code":"StoreKit"},{"type":"text","text":" views, and more."}],"title":"WWDC23"},"WWDC23-10273-component2":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component2.jpg"}],"identifier":"WWDC23-10273-component2","alt":"Entity Component System","type":"image"},"WWDC23-Icon.png":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-Icon.png"}],"identifier":"WWDC23-Icon.png","alt":null,"type":"image"},"WWDC23-10273-dioramaAssembled":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-dioramaAssembled.jpg"}],"identifier":"WWDC23-10273-dioramaAssembled","alt":"dioramaAssembled","type":"image"},"WWDC23-10273-component26":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component26.jpg"}],"identifier":"WWDC23-10273-component26","alt":"Entity Component System","type":"image"},"WWDC23-10273-yosemite":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-yosemite.jpg"}],"identifier":"WWDC23-10273-yosemite","alt":"Yosemite National Park","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10202-Explore-materials-in-Reality-Composer-Pro","title":"Explore materials in Reality Composer Pro","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10202-explore-materials-in-reality-composer-pro","abstract":[{"text":"Learn how Reality Composer Pro can help you alter the appearance of your 3D objects using RealityKit materials. We’ll introduce you to MaterialX and physically-based (PBR) shaders, show you how to design dynamic materials using the shader graph editor, and explore adding custom inputs to a material so that you can control it in your visionOS app.","type":"text"}]},"https://developer.apple.com/videos/play/wwdc2023/10203":{"title":"Develop your first immersive app - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10203","titleInlineContent":[{"type":"text","text":"Develop your first immersive app - WWDC23"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10203"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=0":{"title":"0:00 - Introduction","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=0","titleInlineContent":[{"type":"text","text":"0:00 - Introduction"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=0"},"multitudes.jpeg":{"variants":[{"traits":["1x","light"],"url":"\/images\/multitudes.jpeg"}],"identifier":"multitudes.jpeg","alt":null,"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10203-Develop-your-first-immersive-app":{"title":"Develop your first immersive app","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10203-develop-your-first-immersive-app","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10203-Develop-your-first-immersive-app","abstract":[{"text":"Find out how you can build immersive apps for visionOS using Xcode and Reality Composer Pro. We’ll show you how to get started with a new visionOS project, use Xcode Previews for your SwiftUI development, and take advantage of RealityKit and RealityView to render 3D content.","type":"text"}],"kind":"article"},"WWDC23-10273-component23":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component23.jpg"}],"identifier":"WWDC23-10273-component23","alt":"Entity Component System","type":"image"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=2005":{"title":"33:25 - Wrap-up","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=2005","titleInlineContent":[{"type":"text","text":"33:25 - Wrap-up"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=2005"},"https://laurentbrusa.hashnode.dev/":{"title":"Blog","identifier":"https:\/\/laurentbrusa.hashnode.dev\/","titleInlineContent":[{"type":"text","text":"Blog"}],"type":"link","url":"https:\/\/laurentbrusa.hashnode.dev\/"},"WWDC23-10273-materials":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-materials.jpg"}],"identifier":"WWDC23-10273-materials","alt":"Material properties","type":"image"},"WWDC23-10273-ecs":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-ecs.jpg"}],"identifier":"WWDC23-10273-ecs","alt":"Entity Component System","type":"image"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=720":{"title":"12:00 - User Interface","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=720","titleInlineContent":[{"type":"text","text":"12:00 - User Interface"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=720"},"https://developer.apple.com/videos/play/wwdc2023/10202":{"title":"Explore materials in Reality Composer Pro - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202","titleInlineContent":[{"type":"text","text":"Explore materials in Reality Composer Pro - WWDC23"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10202"},"WWDC23-10273-composer":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-composer.jpg"}],"identifier":"WWDC23-10273-composer","alt":"Reality Composer Pro Icon","type":"image"},"WWDC23-10273-component28":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component28.jpg"}],"identifier":"WWDC23-10273-component28","alt":"Entity Component System","type":"image"},"WWDC23-10273-component13":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component13.jpg"}],"identifier":"WWDC23-10273-component13","alt":"Entity Component System","type":"image"},"WWDC23-10273-component17":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component17.jpg"}],"identifier":"WWDC23-10273-component17","alt":"Entity Component System","type":"image"},"WWDC23-10273-component16":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component16.jpg"}],"identifier":"WWDC23-10273-component16","alt":"Entity Component System","type":"image"},"WWDC23-10273-component22":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component22.jpg"}],"identifier":"WWDC23-10273-component22","alt":"Entity Component System","type":"image"},"https://developer.apple.com/wwdc23/10273":{"checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc23\/10273","type":"download","url":"https:\/\/developer.apple.com\/wwdc23\/10273"},"WWDC23-10273-catalina":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-catalina.jpg"}],"identifier":"WWDC23-10273-catalina","alt":"Catalina Island off the coast of Los Angeles.","type":"image"},"WWDC23-10273-anatomy2":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-anatomy2.jpg"}],"identifier":"WWDC23-10273-anatomy2","alt":"Anatomy of a Reality Composer Pro package","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10074-Dive-into-RealityKit-2":{"url":"\/documentation\/wwdcnotes\/wwdc21-10074-dive-into-realitykit-2","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10074-Dive-into-RealityKit-2","abstract":[{"type":"text","text":"Creating engaging AR experiences has never been easier with RealityKit 2. Explore the latest enhancements to the RealityKit framework and take a deep dive into this underwater sample project. We’ll take you through the improved Entity Component System, streamlined animation pipeline, and the plug-and-play character controller with enhancements to face mesh and audio."}],"type":"topic","role":"sampleCode","kind":"article","title":"Dive into RealityKit 2"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10080-Build-spatial-experiences-with-RealityKit":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10080-Build-spatial-experiences-with-RealityKit","type":"topic","title":"Build spatial experiences with RealityKit","abstract":[{"text":"Discover how RealityKit can bring your apps into a new dimension. Get started with RealityKit entities, components, and systems, and learn how you can add 3D models and effects to your app on visionOS. We’ll also take you through the RealityView API and demonstrate how to add 3D objects to windows, volumes, and spaces to make your apps more immersive. And we’ll explore combining RealityKit with spatial input, animation, and spatial audio.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10080-build-spatial-experiences-with-realitykit","role":"sampleCode"},"WWDC23-10273-component8":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component8.jpg"}],"identifier":"WWDC23-10273-component8","alt":"Entity Component System","type":"image"},"WWDC23-10273-anatomy":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-anatomy.jpg"}],"identifier":"WWDC23-10273-anatomy","alt":"Anatomy of a Reality Composer Pro package","type":"image"},"WWDC23-10273-component29":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component29.jpg"}],"identifier":"WWDC23-10273-component29","alt":"Entity Component System","type":"image"},"WWDC23-10273-component11":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component11.jpg"}],"identifier":"WWDC23-10273-component11","alt":"Entity Component System","type":"image"},"WWDC23-10273-component24":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component24.jpg"}],"identifier":"WWDC23-10273-component24","alt":"Entity Component System","type":"image"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=1671":{"title":"27:51 - Play audio","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1671","titleInlineContent":[{"type":"text","text":"27:51 - Play audio"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1671"},"WWDC23-10273-audio3":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-audio3.jpg"}],"identifier":"WWDC23-10273-audio3","alt":"Material properties","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"title":"WWDC Notes","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","role":"collection","images":[{"identifier":"WWDCNotes.png","type":"icon"}]},"WWDC23-10273-component6":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component6.jpg"}],"identifier":"WWDC23-10273-component6","alt":"Entity Component System","type":"image"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=157":{"title":"2:37 - Load 3D content","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=157","titleInlineContent":[{"type":"text","text":"2:37 - Load 3D content"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=157"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=387":{"title":"6:27 - Components","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=387","titleInlineContent":[{"type":"text","text":"6:27 - Components"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=387"},"WWDC23-10273-component9":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component9.jpg"}],"identifier":"WWDC23-10273-component9","alt":"Entity Component System","type":"image"},"WWDC23-10273-audio":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-audio.jpg"}],"identifier":"WWDC23-10273-audio","alt":"Play audio","type":"image"},"WWDC23-10273-component25":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component25.jpg"}],"identifier":"WWDC23-10273-component25","alt":"Entity Component System","type":"image"},"WWDC23-10273-component21":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component21.jpg"}],"identifier":"WWDC23-10273-component21","alt":"Entity Component System","type":"image"},"multitudes":{"variants":[{"traits":["1x","light"],"url":"\/images\/multitudes.jpeg"}],"identifier":"multitudes","alt":"Profile image of laurent b","type":"image"},"https://x.com/wrmultitudes":{"title":"X\/Twitter","identifier":"https:\/\/x.com\/wrmultitudes","titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"type":"link","url":"https:\/\/x.com\/wrmultitudes"},"WWDC23-10273-component18":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component18.jpg"}],"identifier":"WWDC23-10273-component18","alt":"Entity Component System","type":"image"},"https://developer.apple.com/forums/create/question?&tag1=740030&tag2=266&tag3=796030":{"title":"Have a question? Ask with tag wwdc2023-10273","identifier":"https:\/\/developer.apple.com\/forums\/create\/question?&tag1=740030&tag2=266&tag3=796030","titleInlineContent":[{"type":"text","text":"Have a question? Ask with tag wwdc2023-10273"}],"type":"link","url":"https:\/\/developer.apple.com\/forums\/create\/question?&tag1=740030&tag2=266&tag3=796030"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Contributions are welcome!","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"type":"link","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"WWDC23-10273-component4":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component4.jpg"}],"identifier":"WWDC23-10273-component4","alt":"Entity Component System","type":"image"},"WWDC23-10273-component3":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component3.jpg"}],"identifier":"WWDC23-10273-component3","alt":"Entity Component System","type":"image"},"WWDC23-10273-audio2":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-audio2.jpg"}],"identifier":"WWDC23-10273-audio2","alt":"Play audio","type":"image"},"WWDC23-10273-component7":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component7.jpg"}],"identifier":"WWDC23-10273-component7","alt":"Entity Component System","type":"image"},"https://developer.apple.com/videos/play/wwdc2023/10273/?time=1818":{"title":"30:18 - Material properties","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1818","titleInlineContent":[{"type":"text","text":"30:18 - Material properties"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10273\/?time=1818"},"WWDC23-10273-component5":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component5.jpg"}],"identifier":"WWDC23-10273-component5","alt":"Entity Component System","type":"image"},"WWDC23-10273-component14":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component14.jpg"}],"identifier":"WWDC23-10273-component14","alt":"Entity Component System","type":"image"},"WWDC23-10273-component30":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component30.jpg"}],"identifier":"WWDC23-10273-component30","alt":"Entity Component System","type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/multitudes":{"url":"\/documentation\/wwdcnotes\/multitudes","kind":"article","images":[{"type":"card","identifier":"multitudes.jpeg"},{"type":"icon","identifier":"multitudes.jpeg"}],"type":"topic","abstract":[{"type":"text","text":"student at 42Berlin 🐬 | 🍎 Swift(UI) app dev  | speciality coffee ☕️ & cycling 🚴🏻‍♂️"}],"title":"laurent b (33 notes)","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/multitudes"},"https://developer.apple.com/videos/play/wwdc2023/10081":{"title":"Enhance your spatial computing app with RealityKit - WWDC23","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10081","titleInlineContent":[{"type":"text","text":"Enhance your spatial computing app with RealityKit - WWDC23"}],"type":"link","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10081"},"WWDC23-10273-component19":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component19.jpg"}],"identifier":"WWDC23-10273-component19","alt":"Entity Component System","type":"image"},"WWDC23-10273-component":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component.jpg"}],"identifier":"WWDC23-10273-component","alt":"Entity Component System","type":"image"},"WWDC23-10273-component15":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component15.jpg"}],"identifier":"WWDC23-10273-component15","alt":"Entity Component System","type":"image"},"WWDC23-10273-component10":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-10273-component10.jpg"}],"identifier":"WWDC23-10273-component10","alt":"Entity Component System","type":"image"}}}