{"primaryContentSections":[{"content":[{"level":2,"type":"heading","text":"Overview","anchor":"Overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Apple’s ecosystem of machine learning tools, including "},{"type":"codeVoice","code":"Create ML"},{"type":"text","text":", allows you to build and deploy models in your apps."}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Create ML consists of the Create ML App, "},{"code":"Create ML Framework","type":"codeVoice"},{"type":"text","text":", and underlying Components."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Train models with a click in the ","type":"text"},{"code":"Create ML App","type":"codeVoice"},{"text":".","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Use the frameworks directly for automating model creation or on-device personalization."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Create ML leverages system frameworks like "},{"type":"codeVoice","code":"Vision"},{"type":"text","text":", "},{"type":"codeVoice","code":"Natural Language"},{"type":"text","text":", and "},{"type":"codeVoice","code":"Sound Analysis"},{"type":"text","text":" to customize models with your training data."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"The output of Create ML is a model that you can deploy in your app using these system frameworks.","type":"text"}],"type":"paragraph"}]}]},{"style":"note","type":"aside","name":"If you’re new to machine learning check out","content":[{"type":"paragraph","inlineContent":[{"text":"","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/developer.apple.com\/wwdc24\/10223","isActive":true,"type":"reference"}]}]},{"level":2,"type":"heading","anchor":"App-Enhancements","text":"App Enhancements"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The Create ML App on your Mac is the easiest place to start building custom machine learning models."}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Create models to predict content in images, videos, or tabular data.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Detect objects in images, sounds in audio files, human actions in videos, or activities."}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Ensure your annotations align with your expectations before training. For example, if your app detects both a coffee cup and its surface separately, it indicates annotation issues. Avoid duplicate predictions for a better user experience."}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"In the Create ML App, view your data source distribution with the explore option."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Drill into specific objects or class labels to visualize annotations.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Preview your data source to ensure annotations match expectations, especially for image-based models like image classification and hand pose classification.","type":"text"}],"type":"paragraph"}]}]},{"level":2,"type":"heading","anchor":"Object-Tracking","text":"Object Tracking"},{"type":"paragraph","inlineContent":[{"text":"Create ML simplifies integrating machine learning into your apps across all Apple operating systems.","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Object tracking in Create ML enhances spatial computing experiences, ideal for Apple Vision Pro.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The new Spatial Category in Create ML includes a template for tracking the spatial location and orientation of objects.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Training an object tracker begins with your training data, like all Create ML templates.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The Create ML App streamlines the training process.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Simply provide a 3D asset of your object, and the app handles the rest."}],"type":"paragraph"}]}]},{"style":"note","type":"aside","name":"For a full workflow, of building an object tracking experience and deploying it on Apple Vision Pro check","content":[{"inlineContent":[{"text":"","type":"text"},{"text":" ","type":"text"},{"overridingTitle":"Explore object tracking for visionOS","overridingTitleInlineContent":[{"type":"text","text":"Explore object tracking for visionOS"}],"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc24\/10223"}],"type":"paragraph"}]},{"level":2,"type":"heading","anchor":"Components","text":"Components"},{"level":3,"type":"heading","anchor":"Classification","text":"Classification"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Time-series in Create ML Components consists of uniformly sampled numerical data changing over time, such as:"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Accelerometer","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"GPS location","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Temperature"}]}]}],"type":"unorderedList"}]},{"content":[{"inlineContent":[{"text":"A powerful, general-purpose time series ","type":"text"},{"inlineContent":[{"text":"classifier","type":"text"}],"type":"strong"},{"text":" component now classifies gestures like pinch, snap, or clench using accelerometer data from your Apple Watch.","type":"text"}],"type":"paragraph"}]}]},{"level":3,"type":"heading","anchor":"Forecasting","text":"Forecasting"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Time-series forecasting is a new model type in Create ML.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"It learns from historical data to predict future values over time.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"The "},{"type":"strong","inlineContent":[{"text":"forecaster","type":"text"}]},{"type":"text","text":" is a versatile component, suitable for predicting future values in various contexts, including audio, accelerometer, and sales, by analyzing historical data."}],"type":"paragraph"}]}]},{"level":3,"type":"heading","anchor":"Date-Components","text":"Date Components"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Extract date components to identify trends in the data."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Weekday extraction helps the model learn weekly variations, and month extraction aids in learning annual variations.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Use the ","type":"text"},{"code":"DateFeatureExtractor","type":"codeVoice"},{"text":" component to easily extract features from dates.","type":"text"}],"type":"paragraph"}]}]},{"type":"codeListing","syntax":"swift","code":["let featureExtractor = DateFeatureExtractor<Float>(features: ([.month, .weekday])","\/\/ create a DateFeatureExtractor with month and weekday feature components.","","let preprocessingEstimator = ColumnSelector<_, Date>(.include(columnNames: [\"Date\"]),","transformer: OptionalUnwrapper().appending(featureExtractor))","\/\/compose a ColumnSelector and featureExtractor together into a pipeline.","",".appending(","    ColumnConcatenator<Float>(","        columnSelection: .all, ","        concatenatedColumnName: \"Features\"","        \/\/add a ColumnConcatenator component, to combine all the features into a shaped array.","    )",")","let preprocessor = try await preprocessingEstimator.fitted(to: dataFrame)","\/\/ use pre-processing pipeline to fit data frame","","let featuresDataFrame = try await preprocessor.applied(to: dataFrame)","","let features = featuresDataFrame[\"Features\", MLShapedArray<Float>.self]",".filled(with: MLShappedArray<Float>())","let annotations = dataFrame[\"Quantity\", Float.self]",".filled(with: 0.0)",".map({ MLShapedArray<Float>(scalars: [Float($0)], shape: [1]) })","\/\/ extract the features column and the quantity column, both as columns of MLShapedArray",""]},{"level":3,"type":"heading","anchor":"Features","text":"Features"},{"type":"paragraph","inlineContent":[{"text":"Training a Forecaster model:","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Split the training data into two parts:","type":"text"}],"type":"paragraph"}]}]},{"type":"codeListing","syntax":"swift","code":["\/\/ Training split","let trainingPortion = 0..<10_000","let training = zip(features[trainingPortion], annotations[trainingPortion])","    .map(AnnotatedFeature.init)","","\/\/ Validation split","let validationPortion = 10_000..<12_000","let validation = zip(features[validationPortion], annotations[validationPortion])","    .map(AnnotatedFeature.init)","","\/\/ Train","let configuration = LinearTimeSeriesForecasterConfiguration(","    inputWindowSize: 15,","    forecastWindowSize: 3",")","","let estimator = LinearTimeSeriesForecaster<Float>(configuration: configuration)","let model = try await estimator.fitted(to: training, validatedOn: validation)","","\/\/ Perform predictions","let predictions = try await model.applied(to: validation(\\.feature))",""]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Pick how many days in the future to predict.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Your context should be longer than your prediction window.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Create a series forecaster, configure the "},{"type":"codeVoice","code":"inputWindowSize"},{"type":"text","text":" and "},{"type":"codeVoice","code":"forecastWindowSize"},{"type":"text","text":", and train using the fitted method."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Once training completes, you can make predictions."}]}]}]},{"level":2,"type":"heading","anchor":"Written-By","text":"Written By"},{"type":"row","columns":[{"content":[{"inlineContent":[{"type":"image","identifier":"RamitSharma991"}],"type":"paragraph"}],"size":1},{"content":[{"anchor":"Ramit-Sharma","text":"Ramit Sharma","type":"heading","level":3},{"inlineContent":[{"overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"isActive":true,"overridingTitle":"Contributed Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/RamitSharma991","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/github.com\/RamitSharma991","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/x.com\/iosDev_ramit","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"identifier":"https:\/\/","type":"reference"}],"type":"paragraph"}],"size":4}],"numberOfColumns":5},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"isActive":true,"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"}]},{"level":2,"type":"heading","anchor":"Related-Sessions","text":"Related Sessions"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110332-Whats-new-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10037-Build-dynamic-iOS-apps-with-the-Create-ML-framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10223-Explore-machine-learning-on-Apple-platforms"]},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}]},{"type":"small","inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}],"kind":"content"}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc24-10183-whats-new-in-create-ml"],"traits":[{"interfaceLanguage":"swift"}]}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10183-Whats-new-in-Create-ML"},"sampleCodeDownload":{"action":{"isActive":true,"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc24\/10183","overridingTitle":"Watch Video"},"kind":"sampleDownload"},"schemaVersion":{"patch":0,"minor":3,"major":0},"metadata":{"modules":[{"name":"WWDC Notes"}],"role":"sampleCode","title":"What’s new in Create ML","roleHeading":"WWDC24"},"sections":[],"abstract":[{"text":"Explore updates to Create ML, including interactive data source previews and a new template for building object tracking models for visionOS apps. We’ll also cover important framework improvements, including new time-series forecasting and classification APIs.","type":"text"}],"kind":"article","hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24"]]},"references":{"doc://WWDCNotes/documentation/WWDCNotes/RamitSharma991":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/RamitSharma991","title":"Ramit Sharma (15 notes)","url":"\/documentation\/wwdcnotes\/ramitsharma991","type":"topic","abstract":[{"type":"text","text":"Indie iOS Dev. Swift, SwiftUI, Obj-C, UX and related."}],"role":"sampleCode","images":[{"type":"card","identifier":"RamitSharma991.jpeg"},{"type":"icon","identifier":"RamitSharma991.jpeg"}]},"WWDCNotes.png":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"type":"image","alt":null,"identifier":"WWDCNotes.png"},"RamitSharma991":{"variants":[{"traits":["1x","light"],"url":"\/images\/RamitSharma991.jpeg"}],"type":"image","alt":"Profile image of Ramit Sharma","identifier":"RamitSharma991"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10037-Build-dynamic-iOS-apps-with-the-Create-ML-framework":{"kind":"article","title":"Build dynamic iOS apps with the Create ML framework","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10037-Build-dynamic-iOS-apps-with-the-Create-ML-framework","abstract":[{"type":"text","text":"Discover how your app can train Core ML models fully on device with the Create ML framework, enabling adaptive and customized app experiences, all while preserving data privacy. We’ll explore the types of models that can be created on-the-fly for image-based tasks like Style Transfer and Image Classification, audio tasks like custom Sound Classification, or tasks that build on a rich set of Text Classification, Tabular Data Classification, and Tabular Regressors. And we’ll take you through the many opportunities these models offer to make your app more personal and dynamic."}],"url":"\/documentation\/wwdcnotes\/wwdc21-10037-build-dynamic-ios-apps-with-the-create-ml-framework","role":"sampleCode","type":"topic"},"https://":{"title":"Blog","titleInlineContent":[{"type":"text","text":"Blog"}],"type":"link","url":"https:\/\/","identifier":"https:\/\/"},"https://developer.apple.com/wwdc24/10183":{"type":"download","checksum":null,"url":"https:\/\/developer.apple.com\/wwdc24\/10183","identifier":"https:\/\/developer.apple.com\/wwdc24\/10183"},"https://github.com/RamitSharma991":{"type":"link","identifier":"https:\/\/github.com\/RamitSharma991","url":"https:\/\/github.com\/RamitSharma991","titleInlineContent":[{"text":"GitHub","type":"text"}],"title":"GitHub"},"WWDC24.jpeg":{"type":"image","identifier":"WWDC24.jpeg","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC24.jpeg"}]},"WWDC24-Icon.png":{"type":"image","identifier":"WWDC24-Icon.png","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC24-Icon.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24":{"url":"\/documentation\/wwdcnotes\/wwdc24","abstract":[{"text":"Xcode 16, Swift 6, iOS 18, macOS 15 (Sequoia), tvOS 18, visionOS 2, watchOS 11.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: Swift Testing, ","type":"text"},{"code":"FinanceKit","type":"codeVoice"},{"text":", ","type":"text"},{"code":"TabletopKit","type":"codeVoice"},{"text":", and more.","type":"text"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24","type":"topic","title":"WWDC24","role":"collectionGroup","images":[{"type":"icon","identifier":"WWDC24-Icon.png"},{"type":"card","identifier":"WWDC24.jpeg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10223-Explore-machine-learning-on-Apple-platforms":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10223-Explore-machine-learning-on-Apple-platforms","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc24-10223-explore-machine-learning-on-apple-platforms","kind":"article","role":"sampleCode","title":"Explore machine learning on Apple platforms","abstract":[{"text":"Get started with an overview of machine learning frameworks on Apple platforms. Whether you’re implementing your first ML model, or an ML expert, we’ll offer guidance to help you select the right framework for your app’s needs.","type":"text"}]},"https://x.com/iosDev_ramit":{"type":"link","identifier":"https:\/\/x.com\/iosDev_ramit","url":"https:\/\/x.com\/iosDev_ramit","titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"title":"X\/Twitter"},"doc://WWDCNotes/documentation/WWDCNotes":{"url":"\/documentation\/wwdcnotes","title":"WWDC Notes","role":"collection","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"kind":"symbol","type":"topic","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}]},"RamitSharma991.jpeg":{"type":"image","identifier":"RamitSharma991.jpeg","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/RamitSharma991.jpeg"}]},"https://developer.apple.com/wwdc24/10223":{"type":"link","identifier":"https:\/\/developer.apple.com\/wwdc24\/10223","url":"https:\/\/developer.apple.com\/wwdc24\/10223","titleInlineContent":[{"type":"text","text":"Explore machine learning on Apple platforms"}],"title":"Explore machine learning on Apple platforms"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-110332-Whats-new-in-Create-ML":{"kind":"article","title":"What’s new in Create ML","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-110332-Whats-new-in-Create-ML","type":"topic","role":"sampleCode","abstract":[{"text":"Discover the latest updates to Create ML. We’ll share improvements to Create ML’s evaluation tools that can help you understand how your custom models will perform on real-world data. Learn how you can check model performance on each type of image in your test data and identify problems within individual images to help you troubleshoot mistaken classifications, poorly labeled data, and other errors. We’ll also show you how to test your model with iPhone and iPad in live preview using Continuity Camera, and share how you can take Action Classification even further with the new Repetition Counting capabilities of the Create ML Components framework.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc22-110332-whats-new-in-create-ml"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"type":"link","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"title":"Contributions are welcome!"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10019-Get-to-know-Create-ML-Components":{"url":"\/documentation\/wwdcnotes\/wwdc22-10019-get-to-know-create-ml-components","type":"topic","title":"Get to know Create ML Components","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","role":"sampleCode","abstract":[{"type":"text","text":"Create ML makes it easy to build custom machine learning models for image classification, object detection, sound classification, hand pose classification, action classification, tabular data regression, and more. And with the Create ML Components framework, you can further customize underlying tasks and improve your model. We’ll explore the feature extractors, transformers, and estimators that make up these tasks, and show you how you can combine them with other components and pre-processing steps to build custom tasks for concepts like image regression."}],"kind":"article"}}}