{"metadata":{"role":"sampleCode","modules":[{"name":"WWDC Notes"}],"title":"Training Object Detection Models in Create ML","roleHeading":"WWDC19"},"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-424-Training-Object-Detection-Models-in-Create-ML"},"schemaVersion":{"patch":0,"major":0,"minor":3},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc19-424-training-object-detection-models-in-create-ml"],"traits":[{"interfaceLanguage":"swift"}]}],"kind":"article","primaryContentSections":[{"content":[{"type":"heading","level":2,"anchor":"Image-Object-Detection","text":"Image Object Detection"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Image object detection enables our applications to identify real-world objects captured by your device’s camera, and respond based on their presence, position, and relationship."}]},{"type":"paragraph","inlineContent":[{"type":"emphasis","inlineContent":[{"type":"text","text":"”you can teach your application about the subtle differences between individual animals, hand gestures, street signs, or game tokens.“"}]}]},{"type":"paragraph","inlineContent":[{"inlineContent":[{"type":"text","text":"“Image object detection can identify multiple objects within your photograph, and provide the location, size, and label for each one”"}],"type":"emphasis"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-424-dog","type":"image"}]},{"type":"heading","level":2,"anchor":"Training","text":"Training"},{"type":"paragraph","inlineContent":[{"type":"text","text":"To train an object detector, you need to annotate each image with labeled regions that you would like the model to recognize."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The bounding box for your annotated objects starting at the center of each object of interest and having a size, height, and width, this is measured in pixels from the top left-hand corner of your image to the center of each object of interest."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"You bundle together all of these annotations, label, position, and size, in a JSON file in a given format."}]},{"style":"note","type":"aside","name":"Note","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The more training images, the more the training phase will take."},{"type":"text","text":"\n"},{"type":"text","text":"The example in the session had slightly over 1000 images and the speaker said that  "},{"type":"emphasis","inlineContent":[{"type":"text","text":"“I estimate this is probably going to take an hour or more”"}]},{"type":"text","text":". Therefore be warned before doing such GPU-heavy computation on your MacBook on the go."}]}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-424-training","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"Looking at the screenshot of Create ML above, we have some numbers in the tabs on the top:","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"6","type":"text"}],"type":"strong"},{"text":": number of different classes in our model","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"text":"92%","type":"text"}]},{"type":"text","text":": overall performance based on our input."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"In the training tab we can see our "},{"type":"reference","identifier":"https:\/\/en.wikipedia.org\/wiki\/Loss_function","isActive":true},{"type":"text","text":", "},{"type":"reference","identifier":"https:\/\/developers.google.com\/machine-learning\/crash-course\/descending-into-ml\/training-and-loss","isActive":true},{"type":"text","text":"."}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Below the loss chart, we see the accuracy for each class."}]},{"type":"paragraph","inlineContent":[{"text":"We should strive to have a similar accuracy on all classes.","type":"text"}]},{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"“We also want to have a look and check that we have consistent performance across all of the classes”","type":"text"}],"type":"emphasis"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This is important because it shows that it is performing equally well, for example, in recognizing all six sides of a dice:"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-424-accuracy"}]},{"type":"paragraph","inlineContent":[{"text":"After training, we can test our model by clicking on the output tab:","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-424-output","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We can import an image directly from the iPhone camera, too (for testing)."}]},{"type":"heading","level":2,"anchor":"Training-best-practices","text":"Training best practices"},{"type":"heading","level":3,"anchor":"Balanced","text":"Balanced"},{"type":"paragraph","inlineContent":[{"text":"We should have a balanced number of images with annotations for each class:","type":"text"},{"text":"\n","type":"text"},{"text":"this tells the algorithm that we consider each of these to be equally important, and we can build a model which performs equally well on all of the classes.","type":"text"}]},{"level":3,"anchor":"Quantity","text":"Quantity","type":"heading"},{"inlineContent":[{"text":"We are going to need a bunch of images:","type":"text"},{"text":"\n","type":"text"},{"text":"Apple recommends to start with 30 images with annotations for each class we want the model to recognize, and increase that number if the find performance isn’t good enough, or if the subjects are particularly complicated.","type":"text"}],"type":"paragraph"},{"level":3,"anchor":"Real-World-Inputs","text":"Real World Inputs","type":"heading"},{"inlineContent":[{"text":"Use real-world data:","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"inlineContent":[{"text":"Multiple angles","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Variation of backgrounds","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Different lighting conditions"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Other objects in images","type":"text"}]}]}],"type":"unorderedList"},{"level":2,"anchor":"Written-By","text":"Written By","type":"heading"},{"numberOfColumns":5,"columns":[{"content":[{"inlineContent":[{"type":"image","identifier":"zntfdr"}],"type":"paragraph"}],"size":1},{"content":[{"type":"heading","anchor":"Federico-Zanetello","text":"Federico Zanetello","level":3},{"type":"paragraph","inlineContent":[{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","isActive":true,"overridingTitle":"Contributed Notes","type":"reference","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/github.com\/zntfdr","isActive":true,"type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/zntfdr.dev","isActive":true,"type":"reference"}]}],"size":4}],"type":"row"},{"inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}],"type":"paragraph"},{"level":2,"anchor":"Related-Sessions","text":"Related Sessions","type":"heading"},{"style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10096-Explore-Packages-and-Projects-with-Xcode-Playgrounds","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App"],"type":"links"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"Legal Notice"}],"type":"strong"}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}],"kind":"content"}],"sampleCodeDownload":{"action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc19\/424","isActive":true,"overridingTitle":"Watch Video (15 min)"},"kind":"sampleDownload"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"sections":[],"abstract":[{"text":"Custom Core ML models for Object Detection offer you an opportunity to add some real magic to your app. Learn how the Create ML app in Xcode makes it easy to train and evaluate these models. See how you can test the model performance directly within the app by taking advantage of Continuity Camera. It’s never been easier to build and deploy great Object Detection models for Core ML.","type":"text"}],"references":{"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","type":"link","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification":{"abstract":[{"text":"Create ML enables you to create, evaluate, and test powerful, production-class Core ML models. See how easy it is to create your own Object Detection and Sound Classification models for use in your apps. Learn strategies for balancing your training data to achieve great model accuracy.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc19-406-create-ml-for-object-detection-and-sound-classification","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification","title":"Create ML for Object Detection and Sound Classification","role":"sampleCode","type":"topic","kind":"article"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"type":"image","identifier":"WWDCNotes.png","alt":null},"WWDC19-424-output":{"variants":[{"url":"\/images\/WWDC19-424-output.png","traits":["1x","light"]}],"type":"image","identifier":"WWDC19-424-output","alt":null},"doc://WWDCNotes/documentation/WWDCNotes":{"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"type":"topic","url":"\/documentation\/wwdcnotes","role":"collection","title":"WWDC Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"kind":"symbol"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","images":[{"identifier":"WWDC19-Icon.png","type":"icon"},{"identifier":"WWDC19.jpeg","type":"card"}],"title":"WWDC19","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19","abstract":[{"type":"text","text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"Combine"},{"type":"text","text":", "},{"type":"codeVoice","code":"Core Haptics"},{"type":"text","text":", "},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"role":"collectionGroup"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"type":"topic","title":"Federico Zanetello (332 notes)","url":"\/documentation\/wwdcnotes\/zntfdr","abstract":[{"text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more.","type":"text"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","images":[{"type":"card","identifier":"zntfdr.jpeg"},{"type":"icon","identifier":"zntfdr.jpeg"}],"role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-430-Introducing-the-Create-ML-App":{"abstract":[{"text":"Bringing the power of Core ML to your app begins with one challenge. How do you create your model? The new Create ML app provides an intuitive workflow for model creation. See how to train, evaluate, test, and preview your models quickly in this easy-to-use tool. Get started with one of the many available templates handling a number of powerful machine learning tasks. Learn more about the many features for continuous model improvement and experimentation.","type":"text"}],"role":"sampleCode","type":"topic","title":"Introducing the Create ML App","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","url":"\/documentation\/wwdcnotes\/wwdc19-430-introducing-the-create-ml-app","kind":"article"},"WWDC19-424-accuracy":{"variants":[{"url":"\/images\/WWDC19-424-accuracy.png","traits":["1x","light"]}],"type":"image","identifier":"WWDC19-424-accuracy","alt":null},"zntfdr.jpeg":{"variants":[{"url":"\/images\/zntfdr.jpeg","traits":["1x","light"]}],"type":"image","identifier":"zntfdr.jpeg","alt":null},"https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss":{"url":"https:\/\/developers.google.com\/machine-learning\/crash-course\/descending-into-ml\/training-and-loss","title":"the lower the better","type":"link","identifier":"https:\/\/developers.google.com\/machine-learning\/crash-course\/descending-into-ml\/training-and-loss","titleInlineContent":[{"text":"the lower the better","type":"text"}]},"zntfdr":{"variants":[{"url":"\/images\/zntfdr.jpeg","traits":["1x","light"]}],"type":"image","identifier":"zntfdr","alt":"Profile image of Federico Zanetello"},"WWDC19-Icon.png":{"variants":[{"url":"\/images\/WWDC19-Icon.png","traits":["1x","light"]}],"type":"image","identifier":"WWDC19-Icon.png","alt":null},"https://github.com/zntfdr":{"url":"https:\/\/github.com\/zntfdr","title":"GitHub","type":"link","identifier":"https:\/\/github.com\/zntfdr","titleInlineContent":[{"text":"GitHub","type":"text"}]},"https://developer.apple.com/wwdc19/424":{"url":"https:\/\/developer.apple.com\/wwdc19\/424","checksum":null,"type":"download","identifier":"https:\/\/developer.apple.com\/wwdc19\/424"},"https://zntfdr.dev":{"url":"https:\/\/zntfdr.dev","title":"Blog","type":"link","identifier":"https:\/\/zntfdr.dev","titleInlineContent":[{"text":"Blog","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10096-Explore-Packages-and-Projects-with-Xcode-Playgrounds":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10096-Explore-Packages-and-Projects-with-Xcode-Playgrounds","kind":"article","title":"Explore Packages and Projects with Xcode Playgrounds","abstract":[{"text":"Xcode Playgrounds helps developers explore Swift and framework APIs and provides a scratchpad for rapid experimentation. Learn how Xcode Playgrounds utilizes Xcode’s modern build system, provides improved support for resources, and integrates into your projects, frameworks, and Swift packages to improve your documentation and development workflow.","type":"text"}],"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc20-10096-explore-packages-and-projects-with-xcode-playgrounds","role":"sampleCode"},"WWDC19-424-dog":{"variants":[{"url":"\/images\/WWDC19-424-dog.png","traits":["1x","light"]}],"type":"image","identifier":"WWDC19-424-dog","alt":null},"WWDC19.jpeg":{"variants":[{"url":"\/images\/WWDC19.jpeg","traits":["1x","light"]}],"type":"image","identifier":"WWDC19.jpeg","alt":null},"https://en.wikipedia.org/wiki/Loss_function":{"url":"https:\/\/en.wikipedia.org\/wiki\/Loss_function","title":"loss function","type":"link","identifier":"https:\/\/en.wikipedia.org\/wiki\/Loss_function","titleInlineContent":[{"text":"loss function","type":"text"}]},"WWDC19-424-training":{"variants":[{"url":"\/images\/WWDC19-424-training.png","traits":["1x","light"]}],"type":"image","identifier":"WWDC19-424-training","alt":null}}}