{"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10176-Lift-subjects-from-images-in-your-app","interfaceLanguage":"swift"},"abstract":[{"text":"Discover how you can easily pull the subject of an image from its background in your apps. Learn how to lift the primary subject or to access the subject at a given point with VisionKit. We’ll also share how you can lift subjects using Vision and combine that with lower-level frameworks like Core Image to create fun image effects and more complex compositing pipelines.","type":"text"}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc23-10176-lift-subjects-from-images-in-your-app"],"traits":[{"interfaceLanguage":"swift"}]}],"primaryContentSections":[{"content":[{"level":2,"text":"Overview","type":"heading","anchor":"overview"},{"inlineContent":[{"type":"text","text":"Speakers: Lizzy Board and Saumitro Dasgupta"}],"type":"paragraph"},{"level":2,"text":"What is a subject?","type":"heading","anchor":"What-is-a-subject"},{"type":"aside","content":[{"type":"paragraph","inlineContent":[{"text":"A subject is the foreground object, or objects, of a photo. This is not always a person or a pet. It can be anything from a building, a plate of food, or some pairs of shoes.","type":"text"}]}],"style":"note","name":"Note"},{"level":2,"text":"Supporting frameworks","type":"heading","anchor":"Supporting-frameworks"},{"level":3,"text":"VisionKit","type":"heading","anchor":"VisionKit"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Allows you to very easily adopt system-like subject lifting behavior, right out of the box."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Lets you easily recreate the subject lifting UI that we all know and love, with just a few lines of code."}]}]},{"content":[{"inlineContent":[{"text":"Exposes some basic information about these subjects.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"All happens out-of-process, which has performance benefits but mens the image size is limited."}],"type":"paragraph"}]}]},{"level":3,"text":"Vision","type":"heading","anchor":"Vision"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Lower-level framework that doesn’t have out-of-the-box UI."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Support multiple input sources and higher image resolution.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Good for more advanced image editing pipelines."}]}]}]},{"level":2,"text":"Subject lifting in VisionKit","type":"heading","anchor":"Subject-lifting-in-VisionKit"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"iOS"}],"type":"strong"},{"text":":","type":"text"}],"type":"paragraph"},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"text":"Initialize an ","type":"text"},{"type":"codeVoice","code":"ImageAnalysisInteraction"},{"text":".","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Add the interaction to a view containing the image (this can be an "},{"type":"codeVoice","code":"UIImageView"},{"type":"text","text":", but doesn’t need to be)."}],"type":"paragraph"}]}]},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"macOS"}]},{"type":"text","text":":"}],"type":"paragraph"},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Initialize an "},{"code":"ImageAnalysisOverlayView","type":"codeVoice"},{"type":"text","text":"."}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Add the view as a subview of the ","type":"text"},{"type":"codeVoice","code":"NSView"},{"text":" containing the image.","type":"text"}],"type":"paragraph"}]}]},{"inlineContent":[{"type":"text","text":"A preferred interaction type can be set on the "},{"code":"ImageAnalysisInteraction","type":"codeVoice"},{"type":"text","text":" and "},{"code":"ImageAnalysisOverlayView","type":"codeVoice"},{"type":"text","text":" to choose which interactions should be supported."}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Default is ","type":"text"},{"type":"codeVoice","code":".automatic"},{"text":" which mirrors system behavior. This supports subject lifting, live text and data detectors.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"The new ","type":"text"},{"code":".imageSubject","type":"codeVoice"},{"text":" supports only subject lifting.","type":"text"}],"type":"paragraph"}]}]},{"level":3,"text":"Manually analyzing images","type":"heading","anchor":"Manually-analyzing-images"},{"code":["let analyzer = ImageAnalyzer()","let analysis = try? await analyzer.analyze(image, configuration: configuration)"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"The ","type":"text"},{"code":"ImageAnalysis","type":"codeVoice"},{"text":" struct has a property ","type":"text"},{"code":"subjects","type":"codeVoice"},{"text":" which contains a list of the image’s ","type":"text"},{"code":"Subject","type":"codeVoice"},{"text":"s which contains an image and its bounds. It also has a property ","type":"text"},{"code":"highlightedSubjects","type":"codeVoice"},{"text":" which contains the highlighted subjects. This can be changed programatically.","type":"text"}],"type":"paragraph"},{"level":3,"text":"Looking up a subject","type":"heading","anchor":"Looking-up-a-subject"},{"code":["let subject = try? await interaction.subject(at: point)"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"text":"If there are no subjects at that point, this method will return nil.","type":"text"}],"type":"paragraph"},{"level":3,"text":"Generating subject images","type":"heading","anchor":"Generating-subject-images"},{"code":["\/\/ For an image for a single Subject:","subject.image","\/\/ For an image composed of multiple Subjects:","interaction.image(for: interaction.subjects)"],"syntax":"swift","type":"codeListing"},{"level":2,"text":"Subject lifting in Vision","type":"heading","anchor":"Subject-lifting-in-Vision"},{"level":3,"text":"Different kind of APIs","type":"heading","anchor":"Different-kind-of-APIs"},{"inlineContent":[{"inlineContent":[{"text":"Saliency:","type":"text"}],"type":"strong"}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"VNGenerateAttentionBasedSaliencyImageRequest","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"VNGenerateObjectnessBasedSaliencyImageRequest","type":"text"}],"type":"paragraph"}]}]},{"type":"aside","content":[{"inlineContent":[{"type":"text","text":"Saliency requests, like the ones for attention and objectness, are best used for coarse, region-based analysis."}],"type":"paragraph"}],"style":"note","name":"Note"},{"type":"aside","content":[{"type":"paragraph","inlineContent":[{"text":"The generated saliency maps are at a fairly low resolution and as such, not suitable for segmentation. Instead, you could use the salient regions for tasks like auto-cropping an image.","type":"text"}]}],"style":"note","name":"Note"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"Person segmentation:"}],"type":"strong"}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"VNGeneratePersonSegmentationRequest"}],"type":"paragraph"}]}]},{"type":"aside","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"It shines at producing detailed segmentation masks for people in the scene. Use this if you specifically want to focus on segmenting people."}]}],"style":"note","name":"Note"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"Person instance segmentation:"}],"type":"strong"}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"VNGeneratePersonInstanceMaskRequest ","type":"text"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"NEW"}]}],"type":"emphasis"}]}]}]},{"type":"aside","content":[{"inlineContent":[{"text":"The new person instance segmentation API takes things further by providing a separate mask for each person in the scene.","type":"text"}],"type":"paragraph"}],"style":"note","name":"Note"},{"inlineContent":[{"inlineContent":[{"text":"See more in the session “Explore 3D body pose and person segmentation in Vision”.","type":"text"}],"type":"emphasis"}],"type":"paragraph"},{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Subject lifting:","type":"text"}]}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"VNGenerateForegroundInstanceMaskRequest "},{"type":"emphasis","inlineContent":[{"type":"strong","inlineContent":[{"text":"NEW","type":"text"}]}]}]}]}]},{"type":"aside","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"The newly introduced subject lifting API is “class agnostic”. Any foreground object, regardless of its semantic class, can be potentially segmented."}]}],"style":"note","name":"Note"},{"level":3,"text":"Concepts","type":"heading","anchor":"Concepts"},{"type":"aside","content":[{"inlineContent":[{"text":"You start with an input image. The subject lifting request processes this image and produces a soft segmentation mask at the same resolution. Taking this mask and applying it to the source image results in the masked image.","type":"text"}],"type":"paragraph"}],"style":"note","name":"Note"},{"type":"aside","content":[{"inlineContent":[{"text":"Each distinct segmented object is referred to as an instance.","type":"text"}],"type":"paragraph"}],"style":"note","name":"Note"},{"type":"aside","content":[{"inlineContent":[{"text":"Vision also provides you with pixelwise information about these instances. This instance mask maps pixels in the source image to their instance index. The zero index is reserved for the background, and then each foreground instance is labeled sequentially, starting at 1.","type":"text"}],"type":"paragraph"}],"style":"note","name":"Note"},{"inlineContent":[{"type":"text","text":"The ordering of the IDs are not guaranteed."}],"type":"paragraph"},{"level":3,"text":"Generate a masked image","type":"heading","anchor":"Generate-a-masked-image"},{"code":["let request = VNGenerateForegroundInstanceMaskRequest()","let handler = VNImageRequestHandler(cgImage: image)","try handler.perform([request])","guard let result = request.results?.first else {","  return","}","let output = result.generateMaskedImage(","  ofInstances: result.allInstances,","  from: requestHandler,","  croppedToInstancesExtent: false)"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"type":"text","text":"This is a resource intensive task and best deferred to a background thread so as not to block the UI."}],"type":"paragraph"},{"level":3,"text":"Working with masks","type":"heading","anchor":"Working-with-masks"},{"code":["result.generateScaledMaskForImage(","  forInstances: result.allInstances,","  from: requestHandler)"],"syntax":"swift","type":"codeListing"},{"type":"aside","content":[{"type":"paragraph","inlineContent":[{"text":"The mask I just generated is perfectly suited for use with CoreImage. Vision, much like VisionKit, produces SDR outputs. Performing the masking in CoreImage, however, preserves the high dynamic range of the input.","type":"text"}]}],"style":"note","name":"Note"},{"code":["func apply(mask: CIImage, toImage image: CIImage) -> CIImage {","  let filter = CIFilter.blendWithMask()","  filter.inputImage = image","  filter.maskImage = mask","  filter.backgroundImage = CIImage.empty()","  return filter.outputImage!","}"],"syntax":"swift","type":"codeListing"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"See more in the session “Support HDR images in your app”."}],"type":"emphasis"}],"type":"paragraph"},{"level":3,"text":"Demo app \/ putting it all together","type":"heading","anchor":"Demo-app--putting-it-all-together"},{"inlineContent":[{"text":"Between 13:22 and 18:04, the code snippets above, and some more are put together, to enable a visual effects demo app.","type":"text"}],"type":"paragraph"},{"level":2,"text":"Written By","type":"heading","anchor":"Written-By"},{"numberOfColumns":5,"columns":[{"content":[{"inlineContent":[{"identifier":"MortenGregersen","type":"image"}],"type":"paragraph"}],"size":1},{"content":[{"level":3,"anchor":"Morten-Bjerg-Gregersen","type":"heading","text":"Morten Bjerg Gregersen"},{"type":"paragraph","inlineContent":[{"overridingTitle":"Contributed Notes","type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/MortenGregersen","isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","identifier":"https:\/\/github.com\/MortenGregersen","isActive":true},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","identifier":"http:\/\/atterdagapps.com","isActive":true},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","identifier":"https:\/\/x.com\/mortengregersen","isActive":true}]}],"size":4}],"type":"row"},{"inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true}],"type":"paragraph"},{"level":2,"text":"Related Sessions","type":"heading","anchor":"Related-Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10048-Whats-new-in-VisionKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision"],"style":"list"},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}],"type":"small"}],"kind":"content"}],"sections":[],"schemaVersion":{"major":0,"minor":3,"patch":0},"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc23\/10176","overridingTitle":"Watch Video (18 min)","isActive":true}},"kind":"article","metadata":{"role":"sampleCode","roleHeading":"WWDC23","modules":[{"name":"WWDC Notes"}],"title":"Lift subjects from images in your app"},"references":{"https://github.com/MortenGregersen":{"url":"https:\/\/github.com\/MortenGregersen","identifier":"https:\/\/github.com\/MortenGregersen","titleInlineContent":[{"text":"GitHub","type":"text"}],"title":"GitHub","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision":{"type":"topic","abstract":[{"text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We’ll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image.","type":"text"}],"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","kind":"article","title":"Explore 3D body pose and person segmentation in Vision","url":"\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision"},"WWDCNotes.png":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"identifier":"WWDCNotes.png","alt":null,"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10048-Whats-new-in-VisionKit":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10048-Whats-new-in-VisionKit","abstract":[{"text":"Discover how VisionKit can help people quickly lift subjects from images in your app and learn more about the content of an image with Visual Look Up. We’ll also take a tour of the latest updates to VisionKit for Live Text interaction, data scanning, and expanded support for macOS apps.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc23-10048-whats-new-in-visionkit","title":"What’s new in VisionKit","role":"sampleCode"},"MortenGregersen":{"variants":[{"traits":["1x","light"],"url":"\/images\/MortenGregersen.jpeg"}],"identifier":"MortenGregersen","alt":"Profile image of Morten Bjerg Gregersen","type":"image"},"http://atterdagapps.com":{"url":"http:\/\/atterdagapps.com","identifier":"http:\/\/atterdagapps.com","titleInlineContent":[{"text":"Blog","type":"text"}],"title":"Blog","type":"link"},"https://x.com/mortengregersen":{"url":"https:\/\/x.com\/mortengregersen","identifier":"https:\/\/x.com\/mortengregersen","titleInlineContent":[{"text":"X\/Twitter","type":"text"}],"title":"X\/Twitter","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes":{"kind":"symbol","role":"collection","title":"WWDC Notes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","url":"\/documentation\/wwdcnotes"},"doc://WWDCNotes/documentation/WWDCNotes/MortenGregersen":{"images":[{"identifier":"MortenGregersen.jpeg","type":"card"},{"identifier":"MortenGregersen.jpeg","type":"icon"}],"title":"Morten Bjerg Gregersen (21 notes)","url":"\/documentation\/wwdcnotes\/mortengregersen","kind":"article","role":"sampleCode","type":"topic","abstract":[{"text":"Hi 👋 I am Morten - I live in Denmark 🇩🇰","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/MortenGregersen"},"https://developer.apple.com/wwdc23/10176":{"url":"https:\/\/developer.apple.com\/wwdc23\/10176","identifier":"https:\/\/developer.apple.com\/wwdc23\/10176","checksum":null,"type":"download"},"WWDC23-Icon.png":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23-Icon.png"}],"identifier":"WWDC23-Icon.png","alt":null,"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"images":[{"identifier":"WWDC23-Icon.png","type":"icon"},{"identifier":"WWDC23.jpeg","type":"card"}],"title":"WWDC23","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23","abstract":[{"text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"SwiftData","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Observation","type":"codeVoice"},{"text":", ","type":"text"},{"code":"StoreKit","type":"codeVoice"},{"text":" views, and more.","type":"text"}],"role":"collectionGroup","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"title":"Contributions are welcome!","type":"link"},"WWDC23.jpeg":{"variants":[{"traits":["1x","light"],"url":"\/images\/WWDC23.jpeg"}],"identifier":"WWDC23.jpeg","alt":null,"type":"image"},"MortenGregersen.jpeg":{"variants":[{"traits":["1x","light"],"url":"\/images\/MortenGregersen.jpeg"}],"identifier":"MortenGregersen.jpeg","alt":null,"type":"image"}}}