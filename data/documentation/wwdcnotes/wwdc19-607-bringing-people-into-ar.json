{"primaryContentSections":[{"content":[{"type":"heading","anchor":"Overview","level":2,"text":"Overview"},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"People Occlusion"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Motion Capture","type":"text"}]}]}]},{"type":"heading","anchor":"1-People-Occlusion","level":2,"text":"1. People Occlusion"},{"type":"heading","anchor":"Overview","level":3,"text":"Overview"},{"type":"paragraph","inlineContent":[{"text":"In ARKit today, we can place objects in the real world.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-607-before_occlusion","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We would expect items to be hidden, or occluded, by a person closer to the camera."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-607-after_occlusion","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"ARKit3 enables us to provide this functionality."}]},{"type":"heading","anchor":"Explanation","level":3,"text":"Explanation"},{"type":"paragraph","inlineContent":[{"text":"In ARKit2, the AR object would simply be overlayed on top of the camera scene.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now, we need to have different things in different “depth frames”. This is a “depth ordering problem”."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-depth_frames"}]},{"type":"paragraph","inlineContent":[{"text":"The graphics pipeline already knows where the rendered object will be using the depth buffer, but we need to understand where people are in the scene. ARKit3 introduces the Segmentation and People buffers, which are generated using the A12 chip and machine learning.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-depth_buffers"}]},{"type":"paragraph","inlineContent":[{"text":"These two new buffers are exposed on the ","type":"text"},{"type":"codeVoice","code":"ARFrame"},{"text":" as two new properties:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var segmentationBuffer: CVPixelBuffer? { get }","var estimatedDepthData: CVPixelBuffer? { get }"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"These are generated in real-time at the same cadence as the camera frame, i.e. 60 fps."}]},{"type":"paragraph","inlineContent":[{"text":"We would also like these to be at the same resolution as the camera image, but the neural network only sees a smaller image that is missing a lot of detail.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"To compensate for that missing detail, ARKit3 does some additional processing called ","type":"text"},{"inlineContent":[{"type":"text","text":"matting"}],"type":"strong"},{"text":". Matting uses the ","type":"text"},{"code":"segmentationBuffer","type":"codeVoice"},{"text":" as a guide to look at the camera image and figure out what the missing detail was.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"There’s a lot that goes on behind the scenes, but they’re making it as easy as possible to adopt this. It’s available in RealityKit, but it’s also added in SceneKit’s ","type":"text"},{"code":"ARSCNView","type":"codeVoice"},{"text":". And, in case you have your own renderer or you’re working with third-party rendering, you can incorporate it into your own app using Metal.","type":"text"}]},{"type":"heading","anchor":"Implementation-in-RealityKit","level":3,"text":"Implementation in RealityKit"},{"type":"paragraph","inlineContent":[{"type":"text","text":"RealityKit has a new UI element called the "},{"type":"codeVoice","code":"ARView"},{"type":"text","text":", which provides an easy to use API for AR photorealism. This is the recommended way for new apps because it has a deep renderer integration, meaning that the entire pipeline expects people occlusion and works with transparent objects, so it is built for optimal performance."}]},{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"ARView"},{"text":" also has built-in support for people occlusion:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["override func viewDidLoad() {","  super.viewDidLoad()","","  \/\/ Check if people occlusion is supported.","  guard","    let config = arView.session.config as? ARWorldTrackingConfiguration,","    ARWorldTrackingConfiguration.supportsFrameSemantics(.personSegmentationWithDepth)","  else {","    return","  }","","  \/\/ Enable frame semantics.","  config.frameSemantics = .personSegmentationWithDepth","","  \/\/ You can also use `.personSegmentation` in case you want to enable a \"green-screen\" experience.","}"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Now, when the ARView starts running, people occlusion will automatically happen."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"As an example for what you can build now, see SwiftStrike at timestamp 8:15."}]},{"type":"heading","anchor":"What-if-youve-been-using-SceneKit","level":3,"text":"What if you’ve been using SceneKit?"},{"type":"paragraph","inlineContent":[{"type":"text","text":"All you have to do is enable frame semantics. However, composition is done as a post-process, so it may not work as well with transparent objects."}]},{"type":"heading","anchor":"What-if-you-have-your-ownthird-party-rendering-engine","level":3,"text":"What if you have your own\/third-party rendering engine?"},{"type":"paragraph","inlineContent":[{"type":"text","text":"You can use Metal for complete control of composition. Metal provides a new class for generating matte."}]},{"type":"heading","anchor":"Generating-matte","level":4,"text":"Generating matte"},{"type":"paragraph","inlineContent":[{"text":"To review, matting is the process of finding details that the ML engine missed:","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-matting"}]},{"type":"paragraph","inlineContent":[{"text":"Let’s look at some code.","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["func compositeFrame(_ frame: ARFrame!, commandBuffer: MTLCommandBuffer!) {","  \/\/ Ensure people occlusion is supported.","  guard ARWorldTrackingConfiguration.supportsFrameSemantics(.personSegmentationWithDepth) else {","    return","  }","","  \/\/ Create Metal texture using new API.","  let matte = matteGenerator.generateMatte(from: frame, commandBuffer: commandBuffer)","","  \/\/ Custom composition code using texture...","","  \/\/ Schedule everything to GPU.","  commandBuffer.commit()","}"]},{"type":"paragraph","inlineContent":[{"text":"The new object for ","type":"text"},{"type":"codeVoice","code":"matteGenerator"},{"text":" is here:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["class ARMatteGenerator: NSObject {","  func generateMatte(from: ARFrame, commandBuffer: MTLCommandBuffer) -> MTLTexture","}"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"But we’re not done yet. Just like the "},{"type":"codeVoice","code":"segmentationBuffer"},{"type":"text","text":" was of a lower resolution, the "},{"type":"codeVoice","code":"estimatedDepthData"},{"type":"text","text":" is also lower. If we simply magnify it and overlay it on our matted image, there might be a mismatch. We can have depth values where there’s no alpha value in the matte, and more importantly, we can have alpha values in the matte where there’s no corresponding depth value."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-magnify_depth_data"}]},{"type":"heading","anchor":"Modifying-the-depth-buffer","level":4,"text":"Modifying the depth buffer"},{"type":"paragraph","inlineContent":[{"text":"Because the matte has already recaptured some missing detail, we can’t modify the alpha itself. Instead, we need to modify the depth buffer.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Going back to the previous example:"}]},{"type":"codeListing","syntax":"swift","code":["\/\/ Create Metal texture using new API.","let matte = matteGenerator.generateMatte(from: frame, commandBuffer: commandBuffer)"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Add a function call underneath from the same "},{"type":"codeVoice","code":"ARMatteGenerator"},{"type":"text","text":" class:"}]},{"type":"codeListing","syntax":"swift","code":["\/\/ Create Metal texture using new API.","let matte = matteGenerator.generateMatte(from: frame, commandBuffer: commandBuffer)","let dilatedDepth = matteGenerator.generateDilatedDepth(from: frame, commandBuffer: commandBuffer)"]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This function ensures that for every alpha value we find in the matte, we have a corresponding depth value that we can use when we do our final composition."}]},{"type":"heading","anchor":"Composition","level":4,"text":"Composition"},{"type":"paragraph","inlineContent":[{"text":"Now let’s move on to composition. Composition is usually done on the GPU in the fragment shader.","type":"text"}]},{"type":"codeListing","syntax":"cpp","code":["fragment half4 customComposition(...) {","  \/\/ Sample the camera image and texture.","  \/\/ Also sample rendered depth because we're doing people occlusion.","  half4 camera = cameraTexture.sample(s, in.uv);","  half4 rendered = renderedTexture.sample(s, in.uv);","  float renderedDepth = renderedDepthTexture.sample(s, in.uv);","","  \/\/ Do what we would usually do in AR: simply overlay rendered content on the","  \/\/ real image given the rendered alpha.","  half4 scene = mix(rendered, camera, rendered.a);","","  \/\/ The new part to enable people occlusion: also sample matte and dilated depth.","  half matte = matteTexture.sample(s, in.uv);","  float dilatedDepth = dilatedDepthTexture.sample(s, in.uv);","","  \/\/ If there's people in front of rendered, mix the camera back. Otherwise,","  \/\/ do what we always do and overlay rendered content.","  if (dilatedDepth < renderedDepth) {","    return mix(scene, camera, matte);","  } else {","    return scene;","  }","}"]},{"type":"heading","anchor":"Best-practices-for-people-occlusion","level":4,"text":"Best practices for people occlusion"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Supported on A12 devices and later","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Works best in indoor environments"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"text","text":"Works on people, hands and feet, and multiple people in a scene."}],"type":"paragraph"}]}]},{"type":"heading","anchor":"2-Motion-Capture","level":2,"text":"2. Motion Capture"},{"type":"paragraph","inlineContent":[{"type":"text","text":"A way to animate a virtual character that performs the same set of actions as the person you’re looking at."}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-607-motion_capture_intro","type":"image"}]},{"type":"heading","anchor":"Virtual-Character-Overview","level":3,"text":"Virtual Character Overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Has 2 main components: An outer coating (mesh), and the bony structure inside (skeleton)."}]},{"type":"paragraph","inlineContent":[{"text":"The skeleton is the driving force behind the entire character, so we need to animate the skeleton and the mesh will follow.","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-607-skeleton_capture","type":"image"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Machine Learning estimates the pose of the person in the image, which builds a high-fidelity skeleton. The mesh is overlayed to give a final character and is presented with ARKit3 + RealityKit. This is available on A12 devices and later."}]},{"type":"heading","anchor":"Use-Cases","level":3,"text":"Use Cases"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Animate virtual characters (puppets). This is enabled out-of-the-box.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Action\/Activity recognition.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Sports and fitness analysis.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Interacting with virtual objects."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Since ARKit also provides a 2D version of the skeleton in image space, you can use it to build editing tools or for semantic image understanding.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Remote use cases?","type":"text"}]}]}]},{"type":"heading","anchor":"How-to-Use-It","level":3,"text":"How to Use It"},{"type":"orderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Motion Capture in RealityKit:"}]},{"text":" If you just want to quickly animate a character, this high-level API will help you get there.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Extracting data from skeleton in 3D"}]},{"type":"text","text":": Activity recognition\/analysis, or interaction with 3D objects in a scene."}]}]},{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Extracting data from skeleton in 2D","type":"text"}]},{"text":": For semantic image analysis or editing tools.","type":"text"}],"type":"paragraph"}]}]},{"type":"heading","anchor":"1-Motion-Capture-in-RealityKit","level":3,"text":"1. Motion Capture in RealityKit"},{"type":"heading","anchor":"Overview","level":4,"text":"Overview"},{"type":"paragraph","inlineContent":[{"text":"You can quickly animate characters with a simple API. You can add your custom characters with your own meshes.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The tracked person is easy to access using "},{"code":"AnchorEntity","type":"codeVoice"},{"type":"text","text":" objects that automatically capture the required transforms for motion capture."}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-realitykit_motion_capture"}]},{"type":"paragraph","inlineContent":[{"text":"As an overview, the ","type":"text"},{"code":"bodyTrackedEntity","type":"codeVoice"},{"text":" represents 1 person. It contains the underlying skeleton and its position. It’s tracked in real time and gets updated every frame. It combines the skeleton to give an automatic mesh and the full character in a ","type":"text"},{"code":".usdz","type":"codeVoice"},{"text":" model.","type":"text"}]},{"type":"heading","anchor":"Code","level":4,"text":"Code"},{"type":"codeListing","syntax":"swift","code":["\/\/ Step 1: Load skeleton + mesh for character using \"robot.usdz\".","Entity.loadBodyTrackedAsync(named: \"robot\")","  .sink(","    receiveCompletion: {","      \/\/ Catch any failures\/errors.","    },","    receiveValue: { character in","      guard let character = character as? BodyTrackedEntity else { return }","","      \/\/ Step 2: Get the location where you want to put your character.","      \/\/ You can put it anywhere you want, just provide an anchor at a different location.","      let personAnchor = AnchorEntity(.body)","      arView.scene.addAnchor(personAnchor)","","      \/\/ Step 3: Add the character to that location.","      personAnchor.addChild(character)","    }","  )"]},{"type":"paragraph","inlineContent":[{"text":"You can use your own characters as long as they follow the skeleton naming scheme. The skeleton has 91 joints.","type":"text"}]},{"type":"heading","anchor":"2-Extracting-Data-from-Skeleton-in-3D","level":3,"text":"2. Extracting Data from Skeleton in 3D"},{"type":"paragraph","inlineContent":[{"type":"text","text":"You can access data using a new type of anchor called "},{"type":"codeVoice","code":"ARBodyAnchor"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-ar_body_anchor"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s walk through this structure."}]},{"type":"codeListing","syntax":"swift","code":["class ARBodyAnchor: ARAnchor"]},{"type":"paragraph","inlineContent":[{"type":"codeVoice","code":"ARBodyAnchor"},{"type":"text","text":" is just a regular anchor that contains a geometry (the skeleton). The skeleton consists of nodes and  edges, just like any other geometry."}]},{"type":"codeListing","syntax":"swift","code":["class ARSkeleton"]},{"type":"paragraph","inlineContent":[{"text":"The transform is just the location of the anchor in rotation and translation matrix form.","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var transform: matrix_float4x4"]},{"type":"heading","anchor":"Skeleton-Overview","level":4,"text":"Skeleton Overview"},{"type":"paragraph","inlineContent":[{"text":"We’re mainly interested in accessing the skeleton. The nodes represent the joints, and the edges represent the bones.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"You can access the skeleton using the "},{"type":"codeVoice","code":"skeleton"},{"type":"text","text":" property of "},{"type":"codeVoice","code":"ARBodyAnchor"},{"type":"text","text":". Note that the root node of the skeleton is the right hip joint."}]},{"type":"paragraph","inlineContent":[{"text":"The ","type":"text"},{"type":"codeVoice","code":"definition"},{"text":" property contains 2 components:","type":"text"}]},{"type":"orderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"The names of all the joints in the skeleton, like “left shoulder”.","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"The connections between them, which tell you how to connect those joints together. So, the “right hand” is a child of “right elbow”, which is a child of “right shoulder”, all the way down to the root node, “hip”. Note that fingers, etc. aren’t tracked, they simply follow the general motion of their closest parent.","type":"text"}]}]}]},{"type":"heading","anchor":"Accessing-Joint-Locations","level":4,"text":"Accessing Joint Locations"},{"type":"paragraph","inlineContent":[{"type":"text","text":"There are two ways to access joint locations."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The first one is relative to its parent. If you want the location of the right hand relative to the elbow, you use this these functions on the "},{"type":"codeVoice","code":"ARSkeleton3D"},{"type":"text","text":" object:"}]},{"type":"codeListing","syntax":"swift","code":["func localTransform(for: ARSkeleton.JointName) -> simd_float4x4?","","let rightHandRelativeTransforms = localTransform(for: .rightHand)"]},{"type":"paragraph","inlineContent":[{"text":"If you want the transform relative to the root (hip), you use:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["func modelTransform(for: ARSkeleton.JointName) -> simd_float4x4?","","let rightHandModelTransforms = modelTransform(for: .rightHand)"]},{"type":"paragraph","inlineContent":[{"text":"If you want a list of transforms for all the joints you can get them with the following properties:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["var jointLocalTransforms: [simd_float4x4]","var jointModelTransforms: [simd_float4x4]"]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-607-transforms","type":"image"}]},{"type":"heading","anchor":"Drawing-the-Skeleton","level":4,"text":"Drawing the Skeleton"},{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"Note that there are many code updates with visualizations. If you would like to watch all of them in the video, see 29:43","type":"text"}],"type":"emphasis"}]},{"type":"codeListing","syntax":"swift","code":["\/\/ Look for the bodyAnchor","for anchor in anchors {","  guard let bodyAnchor = anchor as? ARBodyAnchor else { return }","","  \/\/ Access the position of the root node.","  let hipWorldPosition = bodyAnchor.transform","","  \/\/ Access the skeleton geometry.","  let skeleton = bodyAnchor.skeleton","","  \/\/ Access the list of transforms of all joints relative to the root.","  let jointTransforms = skeleton.jointModelTransforms","","  \/\/ Iterate over all the joints.","  for (i, jointTransform) in jointTransforms.enumerated() {","    \/\/ Extract parent index from definition.","    let parentIndex = skeleton.definition.parentIndices[i]","","    \/\/ Check if it's not root (the root doesn't have a parent).","    guard parentIndex != -1 else { continue }","","    \/\/ Find position of parent joint","    let parentJointTransform = jointTransforms[parentIndex.intValue]","","    \/\/ Use this however you want...","  }","}"]},{"type":"heading","anchor":"3-Extracting-Data-from-Skeleton-in-2D","level":2,"text":"3. Extracting Data from Skeleton in 2D"},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-ar_body_2d"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-607-2d_structure","type":"image"}]},{"type":"heading","anchor":"Accessing-the-Skeleton","level":3,"text":"Accessing the Skeleton"},{"type":"paragraph","inlineContent":[{"text":"You can easily access the ","type":"text"},{"code":"ARBody2D","type":"codeVoice"},{"text":" object in 2 different ways.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"If you’re already working in 2D space, you can access it using ","type":"text"},{"code":"ARFrame","type":"codeVoice"},{"text":":","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["func session(_ session: ARSession, didUpdate frame: ARFrame) {","  let person: ARBody2D = frame.detectedBody","}"]},{"type":"paragraph","inlineContent":[{"text":"If you’re working in 3D space and working with a 3D skeleton, but you also want the corresponding 2D skeleton in image space, you can use the following code:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["guard let bodyAnchor = anchor as? ARBodyAnchor else { continue }","","let body2D = bodyAnchor.referenceBody"]},{"type":"heading","anchor":"Skeleton-Image-Space","level":3,"text":"Skeleton Image Space"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Points are in normalized Cartesian coordinate space, as shown below:"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-skeleton_grid"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Note that joints are called joint landmarks because they are pixel locations in an image:"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-607-joint_landmarks"}]},{"type":"paragraph","inlineContent":[{"text":"Just like the 3D version, there is a definition object with joint names and parent-child relationships.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"While the 3D version has 91 joints, the 2D version has only 16 joint landmarks. They also have semantically meaningful names, like “Right Foot” and “Left Shoulder” and “Head”. The root node is still the Hip joint, and parent-child relationships work just as the 3D version.","type":"text"}]},{"type":"heading","anchor":"Drawing-the-2D-Skeleton","level":3,"text":"Drawing the 2D Skeleton"},{"type":"paragraph","inlineContent":[{"text":"The approach is very similar to the 3D version:","type":"text"}]},{"type":"codeListing","syntax":"swift","code":["func session(_ session: ARSession, didUpdate frame: ARFrame) {","  \/\/ Access `ARBody2D` object from `ARFrame`.","  let person = frame.detectedBody","","  \/\/ Use `skeleton` property to access the skeleton.","  let skeleton2D = person.skeleton","","  \/\/ Access `definition` object containing structure.","  let definition = skeleton2D.definition","","  \/\/ Get list of joint landmarks.","  let jointLandmarks = skeleton2D.jointLandmarks","","  \/\/ Iterate over landmarks.","  for (i, joint) in jointLandmarks.enumerated() {","    \/\/ Find index of parent.","    let parentIndex = definition.parentIndices[i]","","    \/\/ Check if current joint landmark is not the root.","    guard parentIndex != -1 else { continue }","","    \/\/ Find position of parent index.","    let parentJointLandmark = jointLandmarks[parentIndex.intValue]","","    \/\/ Use this however you want...","  }","}"]},{"type":"heading","anchor":"Summary","level":2,"text":"Summary"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Access to tracked person"}]}]},{"content":[{"inlineContent":[{"text":"Provides 3D and 2D skeleton","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Enables character animation"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"RealityKit API for quickly animating a character","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"ARKit API for advanced use cases"}],"type":"paragraph"}]}]},{"type":"heading","anchor":"Written-By","level":2,"text":"Written By"},{"type":"row","numberOfColumns":5,"columns":[{"size":1,"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"skhillon"}]}]},{"size":4,"content":[{"level":3,"anchor":"Sarthak-Khillon","text":"Sarthak Khillon","type":"heading"},{"inlineContent":[{"isActive":true,"type":"reference","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/skhillon","overridingTitle":"Contributed Notes"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/skhillon"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/x.com\/SarthakKhillon"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/www.linkedin.com\/in\/sarthakkhillon\/"}],"type":"paragraph"}]}]},{"type":"thematicBreak"},{"type":"paragraph","inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"type":"heading","anchor":"Related-Sessions","level":2,"text":"Related Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10073-Explore-ARKit-5","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-603-Introducing-RealityKit-and-Reality-Composer","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-604-Introducing-ARKit-3","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-605-Building-Apps-with-RealityKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-610-Building-Collaborative-AR-Experiences"],"style":"list"},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}]},{"type":"small","inlineContent":[{"text":"All content copyright © 2012 – 2025 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}],"kind":"content"}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc19-607-bringing-people-into-ar"]}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"overridingTitle":"Watch Video (39 min)","type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc19\/607","isActive":true}},"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-607-Bringing-People-into-AR"},"kind":"article","hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"metadata":{"title":"Bringing People into AR","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC19","role":"sampleCode"},"sections":[],"abstract":[{"type":"text","text":"ARKit 3 enables a revolutionary capability for robust integration of real people into AR scenes. Learn how apps can use live motion capture to animate virtual characters or be applied to 2D and 3D simulation. See how People Occlusion enables even more immersive AR experiences by enabling virtual content to pass behind people in the real world."}],"schemaVersion":{"minor":3,"major":0,"patch":0},"references":{"WWDC19-607-ar_body_2d":{"identifier":"WWDC19-607-ar_body_2d","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-ar_body_2d.png","traits":["1x","light"]}],"type":"image"},"WWDC19-607-ar_body_anchor":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-ar_body_anchor.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC19-607-ar_body_anchor"},"WWDC19-607-depth_buffers":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-depth_buffers.png","traits":["1x","light"]}],"alt":null,"type":"image","identifier":"WWDC19-607-depth_buffers"},"skhillon.jpeg":{"identifier":"skhillon.jpeg","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/skhillon.jpeg","traits":["1x","light"]}],"type":"image"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}]},"WWDC19-607-joint_landmarks":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-joint_landmarks.png","traits":["1x","light"]}],"alt":null,"type":"image","identifier":"WWDC19-607-joint_landmarks"},"https://x.com/SarthakKhillon":{"titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"title":"X\/Twitter","identifier":"https:\/\/x.com\/SarthakKhillon","url":"https:\/\/x.com\/SarthakKhillon","type":"link"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-610-Building-Collaborative-AR-Experiences":{"title":"Building Collaborative AR Experiences","kind":"article","abstract":[{"type":"text","text":"With iOS 13, ARKit and RealityKit enable apps to establish shared AR experiences faster and easier than ever. Understand how collaborative sessions allow multiple devices to build a combined world map and share AR anchors and updates in real-time. Learn how to incorporate collaborative sessions into ARKit-based apps, then roll into SwiftStrike, an engaging and immersive multiplayer AR game built using RealityKit and Swift."}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-610-Building-Collaborative-AR-Experiences","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc19-610-building-collaborative-ar-experiences"},"WWDC19-607-skeleton_capture":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-skeleton_capture.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC19-607-skeleton_capture"},"WWDC19-607-motion_capture_intro":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-motion_capture_intro.png","traits":["1x","light"]}],"alt":null,"type":"image","identifier":"WWDC19-607-motion_capture_intro"},"skhillon":{"identifier":"skhillon","alt":"Profile image of Sarthak Khillon","variants":[{"url":"\/images\/WWDCNotes\/skhillon.jpeg","traits":["1x","light"]}],"type":"image"},"WWDC19-607-after_occlusion":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-after_occlusion.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC19-607-after_occlusion"},"doc://WWDCNotes/documentation/WWDCNotes/skhillon":{"title":"Sarthak Khillon (19 notes)","type":"topic","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/skhillon","url":"\/documentation\/wwdcnotes\/skhillon","images":[{"type":"card","identifier":"skhillon.jpeg"},{"type":"icon","identifier":"skhillon.jpeg"}],"abstract":[{"type":"text","text":"No Bio on GitHub"}],"kind":"article"},"WWDC19-607-skeleton_grid":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-skeleton_grid.png","traits":["1x","light"]}],"alt":null,"type":"image","identifier":"WWDC19-607-skeleton_grid"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-604-Introducing-ARKit-3":{"url":"\/documentation\/wwdcnotes\/wwdc19-604-introducing-arkit-3","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-604-Introducing-ARKit-3","role":"sampleCode","abstract":[{"text":"ARKit is the groundbreaking augmented reality (AR) platform for iOS that can transform how people connect with the world around them. Explore the state-of-the-art capabilities of ARKit 3 and discover the innovative foundation it provides for RealityKit. Learn how ARKit makes AR even more immersive through understanding of body position and movement for motion capture and people occlusion. Check out additions for multiple face tracking, collaborative session building, a coaching UI for on-boarding, and much more.","type":"text"}],"title":"Introducing ARKit 3","type":"topic","kind":"article"},"WWDC19-607-depth_frames":{"identifier":"WWDC19-607-depth_frames","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-depth_frames.png","traits":["1x","light"]}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-603-Introducing-RealityKit-and-Reality-Composer":{"title":"Introducing RealityKit and Reality Composer","kind":"article","abstract":[{"type":"text","text":"Architected for AR, RealityKit provides developers access to world-class capabilities for rendering, animation, physics, and spatial audio. See how RealityKit reimagines the traditional 3D engine to make AR development faster and easier for developers than ever before. Understand the building blocks of developing RealityKit based apps and games, and learn about prototyping and producing content for AR experiences with Reality Composer."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-603-Introducing-RealityKit-and-Reality-Composer","type":"topic","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc19-603-introducing-realitykit-and-reality-composer"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-605-Building-Apps-with-RealityKit":{"role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19-605-building-apps-with-realitykit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-605-Building-Apps-with-RealityKit","type":"topic","title":"Building Apps with RealityKit","abstract":[{"type":"text","text":"Gain a practical understanding of RealityKit capabilities by developing a game using its easy-to-learn API. Learn the recommended approach for loading assets, building a scene, applying animations, and handling game input. See how entities and components express the powerful elements of RealityKit while providing flexibility for customization. Find out how to take advantage of built-in networking and get details about extending the game into an immersive muliti-player experience."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"abstract":[{"type":"text","text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"code":"Combine","type":"codeVoice"},{"type":"text","text":", "},{"code":"Core Haptics","type":"codeVoice"},{"type":"text","text":", "},{"code":"Create ML","type":"codeVoice"},{"type":"text","text":", and more."}],"url":"\/documentation\/wwdcnotes\/wwdc19","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","title":"WWDC19","images":[{"type":"icon","identifier":"WWDC19-Icon.png"},{"type":"card","identifier":"WWDC19.jpeg"}],"role":"collectionGroup","type":"topic","kind":"article"},"https://www.linkedin.com/in/sarthakkhillon/":{"url":"https:\/\/www.linkedin.com\/in\/sarthakkhillon\/","type":"link","identifier":"https:\/\/www.linkedin.com\/in\/sarthakkhillon\/","title":"Blog","titleInlineContent":[{"text":"Blog","type":"text"}]},"WWDC19-607-matting":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-matting.png","traits":["1x","light"]}],"alt":null,"type":"image","identifier":"WWDC19-607-matting"},"WWDC19-607-before_occlusion":{"identifier":"WWDC19-607-before_occlusion","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-before_occlusion.png","traits":["1x","light"]}],"type":"image"},"WWDC19-607-realitykit_motion_capture":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-realitykit_motion_capture.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC19-607-realitykit_motion_capture"},"https://github.com/skhillon":{"titleInlineContent":[{"text":"GitHub","type":"text"}],"url":"https:\/\/github.com\/skhillon","title":"GitHub","type":"link","identifier":"https:\/\/github.com\/skhillon"},"WWDC19-607-magnify_depth_data":{"identifier":"WWDC19-607-magnify_depth_data","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-magnify_depth_data.png","traits":["1x","light"]}],"type":"image"},"WWDC19-Icon.png":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-Icon.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDC19-Icon.png"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10073-Explore-ARKit-5":{"role":"sampleCode","title":"Explore ARKit 5","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10073-Explore-ARKit-5","kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc21-10073-explore-arkit-5","abstract":[{"text":"Build the next generation of augmented reality apps with ARKit 5. Explore how you can use Location Anchors in additional regions and more easily onboard people into your location-based AR experience. Learn more about Face Tracking and Motion Capture. And discover best practices for placing your AR content in the real world. We’ll also show you how you can integrate App Clip Codes into your AR app for easy discovery and precise positioning of your virtual content.","type":"text"}]},"https://developer.apple.com/wwdc19/607":{"checksum":null,"url":"https:\/\/developer.apple.com\/wwdc19\/607","type":"download","identifier":"https:\/\/developer.apple.com\/wwdc19\/607"},"WWDC19.jpeg":{"identifier":"WWDC19.jpeg","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC19.jpeg","traits":["1x","light"]}],"type":"image"},"WWDC19-607-transforms":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-transforms.png","traits":["1x","light"]}],"alt":null,"type":"image","identifier":"WWDC19-607-transforms"},"WWDCNotes.png":{"variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"type":"image","alt":null,"identifier":"WWDCNotes.png"},"WWDC19-607-2d_structure":{"identifier":"WWDC19-607-2d_structure","alt":null,"variants":[{"url":"\/images\/WWDCNotes\/WWDC19-607-2d_structure.png","traits":["1x","light"]}],"type":"image"},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","title":"WWDC Notes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"role":"collection","kind":"symbol","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}]}}}