{"kind":"article","schemaVersion":{"patch":0,"major":0,"minor":3},"abstract":[{"type":"text","text":"Find out how you can improve on-device speech recognition in your app by customizing the underlying model with additional vocabulary. We’ll share how speech recognition works on device and show you how to boost specific words and phrases for more predictable transcription. Learn how you can provide specific pronunciations for words and use template support to quickly generate a full set of custom phrases — all at runtime."}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","identifier":"https:\/\/developer.apple.com\/wwdc23\/10101","isActive":true,"overridingTitle":"Watch Video (7 min)"}},"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10101-Customize-ondevice-speech-recognition"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23"]]},"metadata":{"roleHeading":"WWDC23","modules":[{"name":"WWDC Notes"}],"role":"sampleCode","title":"Customize on-device speech recognition"},"primaryContentSections":[{"content":[{"anchor":"Previous-Iteration-of-Speech-Recognizer","type":"heading","level":3,"text":"Previous Iteration of Speech Recognizer"},{"inlineContent":[{"text":"By default, when you embed Apple’s Speech framework into your app it uses a general language model to reject transcription candidates that it feels are less likely. This doesn’t work well if your app is geared toward less common verbiage.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10101-default-speech-recognition-workflow","type":"image"}],"type":"paragraph"},{"inlineContent":[{"text":"For example, in a chess app, you may want to tell the app “Play the Albin counter gambit” but this verbiage is so rare in the general language model that it incorrectly interprets this as “Play the album Counter Gambit”.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"identifier":"WWDC23-10101-customized-speech-recognition-workflow","type":"image"}],"type":"paragraph"},{"anchor":"Language-Model-Customization","type":"heading","level":3,"text":"Language Model Customization"},{"inlineContent":[{"type":"text","text":"New in iOS 17, you’ll be able to customize the behavior of the SFSpeechRecognizer’s language model, tailor it to your application, and improve its accuracy."}],"type":"paragraph"},{"inlineContent":[{"text":"Steps:","type":"text"}],"type":"paragraph"},{"type":"orderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"Create a collection of training data (during development process)"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Prepare the training data","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Configure a recognition request","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Run it","type":"text"}]}]}]},{"anchor":"Data-Generation","type":"heading","level":3,"text":"Data Generation"},{"inlineContent":[{"text":"Training data will consist of bits of text that represent phrases your app’s users are likely to speak.","type":"text"}],"type":"paragraph"},{"code":["  import Speech","","  let data = SFCustomLanguageModelData(","      locale: Locale(identifier: \"en_US\"),","      identifier: \"com.apple.SampleApp\",","      version: \"1.0\"","  ) {","      SFCustomLanguageModelData.PhraseCount(","          phrase: \"Play the Albin counter gambit\",","          count: 10","      )","  }"],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"type":"text","text":"In the above example, we feed our custom phrase into the model 10 times. Experiment often, you could be surpised at how quickly the model learns your phrases."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Only so much data can be accepted by the system, so balance your need to boost phrases against your overall training data budget."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Furthermore, you can declare classes of words and put them into a pattern to represent every possible combination."}],"type":"paragraph"},{"code":["  SFCustomLanguageModelData.PhraseCountsFromTemplates(","      classes: [","          \"piece\" : [\"pawn\", \"rook\", \"knight\", \"bishop\", \"queen\", \"king\"],","          \"royal\" : [\"queen\", \"king\"],","          \"rank\" : Array(1...8).map({String($0)})","      ]","  ) {","      SFCustomLanguageModelData.TemplatePhraseCountGenerator.Template(","          \"‹piece> to <royal> <piece> <rank>\",","          count: 10000","      )","  }"],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"text":"When you are done building up the data object, export it to a file and deploy into your app like any other asset.","type":"text"}],"type":"paragraph"},{"code":["  try await data.export(to: URL(filePath: \"\/var\/tmp\/SampleApp.bin\"))"],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"type":"text","text":"If your app makes use of specialized terminology, for example, a medical app that includes the names of pharmaceuticals, you can define both the spelling and pronunciations of those terms, and provide phrase counts that demonstrates their usage."}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Pronunciations are accepted in the form of X-SAMPA strings","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Each locale supports a unique subset of pronunciation symbols","type":"text"}]}]}]},{"code":["  SFCustomLanguageModelData.CustomPronunciation(","      grapheme: \"Winawer\",","      phonemes: [\"w I n aU @r\"]","  )","  SFCustomLanguageModelData.PhraseCount(","      phrase: \"Play the Winawer variation\",","      count: 10","  )"],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"type":"text","text":"The model can also be trained at runtime, for example, if you want to train it on commonly used names from the user’s contacts."}],"type":"paragraph"},{"code":["  func scrapeDataForLmCustomization() {","      Task.detached {","          let data = SFCustomLanguageModelData(","              locale: Locale(identifier: \"en_US\"),","              identifier: \"SampleApp\", ","              version: \"1.0\"","          ) {","              for (name, timesCalled) in getCallHistory() {","                  SFCustomLanguageModelData.PhraseCount (","                      phrase: \"Call \\(name)\",","                      count: timesCalled","                  )","              }","              \/\/...","          }","      }","  }"],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"text":"Once the training data is generated, it is bound to a single locale. If you want to support multiple locales within a single script, you can use standard localization facilities like NSLocalizedString to do so.","type":"text"}],"type":"paragraph"},{"anchor":"Deploying-Your-Model","type":"heading","level":3,"text":"Deploying Your Model"},{"code":["  public func prepCustomlm() {","      self.customLmTask = Task.detached {","          self.hasBuiltLm = false","          try await SFSpeechLanguageModel.prepareCustomLanguageModel(","              for: self.assetPath,","              clientIdentifier: \"com.apple.SampleApp\",","              configuration: self.ImConfiguration","          )","          self.hasBuiltLm = true","      ｝","  }"],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"text":"This method call can have a large amount of associated latency, so it’s best to call it off the main thread, and hide the latency behind some UI, such as a loading screen.","type":"text"}],"type":"paragraph"},{"code":["  public func startRecording(updateRecognitionText: @escaping (String) -> Void) throws {","      recognitionRequest = SFSpeechAudioBufferRecognitionRequest ()","      \/\/ keep recognition data on device","      recognitionRequest.requires0nDeviceRecognition = true","      recognitionRequest. customizedLanguageModel = self.ImConfiguration","      \/\/...","  }"],"type":"codeListing","syntax":"swift"},{"inlineContent":[{"text":"When your app constructs the speech recognition request, you first enforce that the recognition is run on device. Failing to do so will cause requests to be serviced without customization.","type":"text"}],"type":"paragraph"},{"anchor":"Written-By","type":"heading","level":2,"text":"Written By"},{"columns":[{"content":[{"type":"paragraph","inlineContent":[{"identifier":"trav-ma","type":"image"}]}],"size":1},{"content":[{"anchor":"Travis-Ma","level":3,"type":"heading","text":"Travis Ma"},{"inlineContent":[{"type":"reference","isActive":true,"overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}],"overridingTitle":"Contributed Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/trav-ma"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"type":"reference","isActive":true,"identifier":"https:\/\/github.com\/trav-ma"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"type":"reference","isActive":true,"identifier":"https:\/\/www.travisma.me"}],"type":"paragraph"}],"size":4}],"type":"row","numberOfColumns":5},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"}],"type":"paragraph"},{"anchor":"Related-Sessions","type":"heading","level":2,"text":"Related Sessions"},{"type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-256-Advances-in-Speech-Recognition"],"style":"list"},{"inlineContent":[{"inlineContent":[{"type":"text","text":"Legal Notice"}],"type":"strong"}],"type":"small"},{"inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}],"kind":"content"}],"sections":[],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc23-10101-customize-ondevice-speech-recognition"]}],"references":{"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"type":"link","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-256-Advances-in-Speech-Recognition":{"title":"Advances in Speech Recognition","kind":"article","role":"sampleCode","abstract":[{"text":"Speech Recognizer can now be used locally on iOS or macOS devices with no network connection. Learn how you can bring text-to-speech support to your app while maintaining privacy and eliminating the limitations of server-based processing. Speech recognition API has also been enhanced to provide richer analytics including speaking rate, pause duration, and voice quality.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-256-Advances-in-Speech-Recognition","url":"\/documentation\/wwdcnotes\/wwdc19-256-advances-in-speech-recognition"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23","images":[{"identifier":"WWDC23-Icon.png","type":"icon"},{"identifier":"WWDC23.jpeg","type":"card"}],"url":"\/documentation\/wwdcnotes\/wwdc23","abstract":[{"type":"text","text":"Xcode 15, Swift 5.9, iOS 17, macOS 14 (Sonoma), tvOS 17, visionOS 1, watchOS 10."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"SwiftData"},{"type":"text","text":", "},{"type":"codeVoice","code":"Observation"},{"text":", ","type":"text"},{"type":"codeVoice","code":"StoreKit"},{"type":"text","text":" views, and more."}],"kind":"article","type":"topic","title":"WWDC23","role":"collectionGroup"},"https://github.com/trav-ma":{"type":"link","titleInlineContent":[{"type":"text","text":"GitHub"}],"url":"https:\/\/github.com\/trav-ma","identifier":"https:\/\/github.com\/trav-ma","title":"GitHub"},"WWDCNotes.png":{"type":"image","identifier":"WWDCNotes.png","alt":null,"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}]},"https://www.travisma.me":{"type":"link","titleInlineContent":[{"type":"text","text":"Blog"}],"url":"https:\/\/www.travisma.me","identifier":"https:\/\/www.travisma.me","title":"Blog"},"trav-ma.jpeg":{"type":"image","identifier":"trav-ma.jpeg","alt":null,"variants":[{"url":"\/images\/trav-ma.jpeg","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/trav-ma":{"title":"Travis Ma (5 notes)","abstract":[{"text":"Software Development Manager for IQVIA’s Mobile team in Arizona.","type":"text"}],"images":[{"identifier":"trav-ma.jpeg","type":"card"},{"identifier":"trav-ma.jpeg","type":"icon"}],"role":"sampleCode","url":"\/documentation\/wwdcnotes\/trav-ma","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/trav-ma","kind":"article","type":"topic"},"https://developer.apple.com/wwdc23/10101":{"type":"download","url":"https:\/\/developer.apple.com\/wwdc23\/10101","identifier":"https:\/\/developer.apple.com\/wwdc23\/10101","checksum":null},"WWDC23-10101-customized-speech-recognition-workflow":{"type":"image","identifier":"WWDC23-10101-customized-speech-recognition-workflow","alt":"Custom Speech Recognition Workflow","variants":[{"url":"\/images\/WWDC23-10101-customized-speech-recognition-workflow.png","traits":["1x","light"]}]},"WWDC23-10101-default-speech-recognition-workflow":{"type":"image","identifier":"WWDC23-10101-default-speech-recognition-workflow","alt":"Default Speech Recognition Workflow","variants":[{"url":"\/images\/WWDC23-10101-default-speech-recognition-workflow.png","traits":["1x","light"]}]},"WWDC23.jpeg":{"type":"image","identifier":"WWDC23.jpeg","alt":null,"variants":[{"url":"\/images\/WWDC23.jpeg","traits":["1x","light"]}]},"trav-ma":{"type":"image","identifier":"trav-ma","alt":"Profile image of Travis Ma","variants":[{"url":"\/images\/trav-ma.jpeg","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes":{"title":"WWDC Notes","url":"\/documentation\/wwdcnotes","type":"topic","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","kind":"symbol"},"WWDC23-Icon.png":{"type":"image","identifier":"WWDC23-Icon.png","alt":null,"variants":[{"url":"\/images\/WWDC23-Icon.png","traits":["1x","light"]}]}}}