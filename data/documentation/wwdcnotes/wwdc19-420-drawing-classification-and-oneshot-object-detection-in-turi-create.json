{"schemaVersion":{"patch":0,"major":0,"minor":3},"sections":[],"metadata":{"title":"Drawing Classification and One-Shot Object Detection in Turi Create","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC19","role":"sampleCode"},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-420-Drawing-Classification-and-OneShot-Object-Detection-in-Turi-Create","interfaceLanguage":"swift"},"sampleCodeDownload":{"action":{"identifier":"https:\/\/developer.apple.com\/wwdc19\/420","overridingTitle":"Watch Video (42 min)","isActive":true,"type":"reference"},"kind":"sampleDownload"},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"kind":"article","variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc19-420-drawing-classification-and-oneshot-object-detection-in-turi-create"]}],"abstract":[{"text":"Apple’s open source toolset, Turi Create, recently added tasks for Core ML model creation including Drawing Classification and One-Shot Object Detection. Learn how to quickly use these capabilities in your apps as well as new techniques for visualizing and evaluating the performance of your custom models.","type":"text"}],"primaryContentSections":[{"content":[{"anchor":"Whats-Turi-Create","type":"heading","level":2,"text":"What’s Turi Create?"},{"inlineContent":[{"type":"text","text":"Turi Create is a Python library used for creating Core ML models. It has a simple, easy to use task-oriented API."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Tasks are our term for a collection of complex, machine learning algorithms abstracted away into a simple, easy to use solution for you to build your models."}],"type":"paragraph"},{"inlineContent":[{"type":"reference","isActive":true,"identifier":"https:\/\/github.com\/apple\/turicreate"},{"type":"text","text":"."}],"type":"paragraph"},{"text":"How to create a CoreML model","anchor":"How-to-create-a-CoreML-model","type":"heading","level":2},{"inlineContent":[{"type":"text","text":"Five steps:"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"Identify the task, aka what problem we’re trying to solve","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Collect data"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Train a model"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Test\/evaluate the model"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Deploy the model","type":"text"}]}]}],"type":"orderedList"},{"type":"paragraph","inlineContent":[{"text":"Here’s the code (in python):","type":"text"}]},{"code":["import turicreate ","","\/\/ Load data ","data = turicreate.SFrame(\"ActivityData.sframe\")","train, test = data.random_split (0.8) ","","\/\/ Create a model ","model = turicreate.activity_classifier.create(train, ","","\/\/ Evaluate the model","metrics = model.evaluate(test) ","","\/\/ Export for deployment ","model.export_coreml(\"MyActivityClassifier.mlmodel\") "],"type":"codeListing","syntax":"python"},{"level":2,"anchor":"Task-types","type":"heading","text":"Task types"},{"type":"paragraph","inlineContent":[{"text":"| ","type":"text"},{"identifier":"WWDC19-420-oldTasks","type":"image"},{"text":" | ","type":"text"},{"identifier":"WWDC19-420-newTasks","type":"image"},{"text":" |","type":"text"},{"text":" ","type":"text"},{"text":"| Current | New |","type":"text"}]},{"level":2,"anchor":"One-Shot-Object-Detection","type":"heading","text":"One-Shot Object Detection"},{"type":"paragraph","inlineContent":[{"text":"The one-shot object detection is like object detection, but requires much less train data.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Let’s say that we want to detect objects like cards and logos with our camera. These features are 2D features:"},{"type":"text","text":"\n"},{"type":"text","text":"One-Shot Object Detection is able to take advantage of these characteristics of regularity and consistency to dramatically reduce the amount of data needed to build an accurate model."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Object Detection normal approach:"},{"type":"text","text":"\n"},{"type":"text","text":"Take plenty of pictures, annotate all the pics with the object location and tag said object:"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-420-cards"}]},{"type":"paragraph","inlineContent":[{"text":"One-shot approach:","type":"text"},{"text":" ","type":"text"},{"text":"One clear picture per class","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"image","identifier":"WWDC19-420-cardsClear"}]},{"type":"paragraph","inlineContent":[{"text":"This works with a process called Synthetic Data Augmentation. Turi Create ships with a collection of images of the real world. The script takes the provided image and overlay onto these real-world images along with various color perturbations and distortions to simulate real world effects. The script also takes care of annotating these images.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"On the session demo the Turi script has created 2000 images per class (and annotate them etc)."}]},{"type":"paragraph","inlineContent":[{"text":"One-shot vs traditional approach:","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-420-oneVStraditional","type":"image"}]},{"level":2,"anchor":"Drawing-Classification","type":"heading","text":"Drawing Classification"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Drawing classification lets us use PencilKit input to train our model and recognize gestures."},{"type":"text","text":" "},{"type":"text","text":"In short PencilKit strokes recognition."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The Drawing Classification task can accept bitmap-based drawings, and stroke-based drawings."}]},{"type":"paragraph","inlineContent":[{"text":"CoreML has been optimized so that we need as few as 30 drawings per class.","type":"text"}]},{"type":"paragraph","inlineContent":[{"text":"Turi also offers a way to explore all the tests and see the results:","type":"text"}]},{"type":"paragraph","inlineContent":[{"identifier":"WWDC19-420-results","type":"image"}]},{"type":"paragraph","inlineContent":[{"text":"This window has three sections:","type":"text"}]},{"items":[{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Overview"}]},{"text":": gives us the overall accuracy of the model as well as the number of iterations in the model that we’ve trained, the Drawing Classifier model.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"text":"Per Class breakdown","type":"text"}]},{"text":": breakdown of the metrics as well as a couple of example images that have been correctly been predicted by the model.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Error section"}]},{"type":"text","text":": shows us all of the errors that the model has made."}],"type":"paragraph"}]}],"type":"unorderedList"},{"level":2,"anchor":"Written-By","type":"heading","text":"Written By"},{"columns":[{"size":1,"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"zntfdr"}]}]},{"size":4,"content":[{"text":"Federico Zanetello","type":"heading","level":3,"anchor":"Federico-Zanetello"},{"type":"paragraph","inlineContent":[{"overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","isActive":true,"overridingTitle":"Contributed Notes","type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/github.com\/zntfdr","isActive":true,"type":"reference"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/zntfdr.dev","isActive":true,"type":"reference"}]}]}],"type":"row","numberOfColumns":5},{"type":"paragraph","inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"}]},{"level":2,"anchor":"Related-Sessions","type":"heading","text":"Related Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-221-Introducing-PencilKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-424-Training-Object-Detection-Models-in-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework"],"style":"list","type":"links"},{"type":"small","inlineContent":[{"inlineContent":[{"text":"Legal Notice","type":"text"}],"type":"strong"}]},{"type":"small","inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}],"kind":"content"}],"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-209-Whats-New-in-Machine-Learning":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-209-whats-new-in-machine-learning","role":"sampleCode","title":"What’s New in Machine Learning","abstract":[{"text":"Core ML 3 has been greatly expanded to enable even more amazing, on-device machine learning capabilities in your app. Learn about the new Create ML app which makes it easy to build Core ML models for many tasks. Get an overview of model personalization; exciting updates in Vision, Natural Language, Sound, and Speech; and added support for cutting-edge model types.","type":"text"}]},"WWDC19-420-oneVStraditional":{"identifier":"WWDC19-420-oneVStraditional","variants":[{"url":"\/images\/WWDC19-420-oneVStraditional.png","traits":["1x","light"]}],"type":"image","alt":null},"zntfdr.jpeg":{"identifier":"zntfdr.jpeg","variants":[{"url":"\/images\/zntfdr.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"WWDC19-420-cards":{"identifier":"WWDC19-420-cards","variants":[{"url":"\/images\/WWDC19-420-cards.png","traits":["1x","light"]}],"type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"type":"icon","identifier":"WWDCNotes.png"}],"kind":"symbol","title":"WWDC Notes","role":"collection","url":"\/documentation\/wwdcnotes"},"https://github.com/zntfdr":{"url":"https:\/\/github.com\/zntfdr","identifier":"https:\/\/github.com\/zntfdr","titleInlineContent":[{"text":"GitHub","type":"text"}],"type":"link","title":"GitHub"},"WWDCNotes.png":{"identifier":"WWDCNotes.png","variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"type":"image","alt":null},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"type":"link","title":"Contributions are welcome!"},"WWDC19.jpeg":{"identifier":"WWDC19.jpeg","variants":[{"url":"\/images\/WWDC19.jpeg","traits":["1x","light"]}],"type":"image","alt":null},"WWDC19-420-results":{"identifier":"WWDC19-420-results","variants":[{"url":"\/images\/WWDC19-420-results.png","traits":["1x","light"]}],"type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"type":"topic","title":"Federico Zanetello (332 notes)","role":"sampleCode","url":"\/documentation\/wwdcnotes\/zntfdr","abstract":[{"type":"text","text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more."}],"images":[{"identifier":"zntfdr.jpeg","type":"card"},{"identifier":"zntfdr.jpeg","type":"icon"}],"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-424-Training-Object-Detection-Models-in-Create-ML":{"type":"topic","title":"Training Object Detection Models in Create ML","url":"\/documentation\/wwdcnotes\/wwdc19-424-training-object-detection-models-in-create-ml","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-424-Training-Object-Detection-Models-in-Create-ML","role":"sampleCode","abstract":[{"text":"Custom Core ML models for Object Detection offer you an opportunity to add some real magic to your app. Learn how the Create ML app in Xcode makes it easy to train and evaluate these models. See how you can test the model performance directly within the app by taking advantage of Continuity Camera. It’s never been easier to build and deploy great Object Detection models for Core ML.","type":"text"}]},"zntfdr":{"identifier":"zntfdr","variants":[{"url":"\/images\/zntfdr.jpeg","traits":["1x","light"]}],"type":"image","alt":"Profile image of Federico Zanetello"},"WWDC19-Icon.png":{"identifier":"WWDC19-Icon.png","variants":[{"url":"\/images\/WWDC19-Icon.png","traits":["1x","light"]}],"type":"image","alt":null},"WWDC19-420-oldTasks":{"identifier":"WWDC19-420-oldTasks","variants":[{"url":"\/images\/WWDC19-420-oldTasks.png","traits":["1x","light"]}],"type":"image","alt":null},"WWDC19-420-newTasks":{"identifier":"WWDC19-420-newTasks","variants":[{"url":"\/images\/WWDC19-420-newTasks.png","traits":["1x","light"]}],"type":"image","alt":null},"https://github.com/apple/turicreate":{"url":"https:\/\/github.com\/apple\/turicreate","identifier":"https:\/\/github.com\/apple\/turicreate","titleInlineContent":[{"text":"Open source","type":"text"}],"type":"link","title":"Open source"},"https://zntfdr.dev":{"url":"https:\/\/zntfdr.dev","identifier":"https:\/\/zntfdr.dev","titleInlineContent":[{"text":"Blog","type":"text"}],"type":"link","title":"Blog"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-704-Core-ML-3-Framework":{"kind":"article","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-704-core-ml-3-framework","abstract":[{"text":"Core ML 3 now enables support for advanced model types that were never before available in on-device machine learning. Learn how model personalization brings amazing personalization opportunities to your app. Gain a deeper understanding of strategies for linking models and improvements to Core ML tools used for conversion of existing models.","type":"text"}],"title":"Core ML 3 Framework","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"abstract":[{"text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6.","type":"text"},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"type":"codeVoice","code":"Core Haptics"},{"text":", ","type":"text"},{"type":"codeVoice","code":"Create ML"},{"text":", and more.","type":"text"}],"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19","role":"collectionGroup","title":"WWDC19","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","images":[{"identifier":"WWDC19-Icon.png","type":"icon"},{"identifier":"WWDC19.jpeg","type":"card"}],"kind":"article"},"WWDC19-420-cardsClear":{"identifier":"WWDC19-420-cardsClear","variants":[{"url":"\/images\/WWDC19-420-cardsClear.png","traits":["1x","light"]}],"type":"image","alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-221-Introducing-PencilKit":{"title":"Introducing PencilKit","kind":"article","abstract":[{"text":"Meet PencilKit, Apple’s feature-rich drawing and annotation framework. With just a few lines of code, you can add a full drawing experience to your app — with access to a canvas, responsive inks, rich tool palette and drawing model. Hear the technical details that make a great Apple Pencil experience. Learn about the new screenshot editor and how you can adopt just a few small APIs to enable your full content to be captured beyond the size of the screen, with or without your app’s user interface.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-221-Introducing-PencilKit","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-221-introducing-pencilkit"},"https://developer.apple.com/wwdc19/420":{"checksum":null,"url":"https:\/\/developer.apple.com\/wwdc19\/420","identifier":"https:\/\/developer.apple.com\/wwdc19\/420","type":"download"}}}