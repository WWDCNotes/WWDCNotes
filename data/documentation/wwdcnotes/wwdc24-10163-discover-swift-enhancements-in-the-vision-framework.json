{"metadata":{"role":"sampleCode","roleHeading":"WWDC24","modules":[{"name":"WWDC Notes"}],"title":"Discover Swift enhancements in the Vision framework"},"kind":"article","primaryContentSections":[{"content":[{"anchor":"overview","level":2,"type":"heading","text":"Overview"},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:"},{"type":"text","text":" "},{"type":"reference","isActive":true,"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"}]},{"anchor":"Related-Sessions","level":2,"type":"heading","text":"Related Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision"],"type":"links","style":"list"}],"kind":"content"}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc24-10163-discover-swift-enhancements-in-the-vision-framework"],"traits":[{"interfaceLanguage":"swift"}]}],"schemaVersion":{"minor":3,"patch":0,"major":0},"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","overridingTitle":"Watch Video (16 min)","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2024\/10163","isActive":true}},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24"]]},"sections":[],"abstract":[{"type":"text","text":"The Vision Framework API has been redesigned to leverage modern Swift features like concurrency, making it easier and faster to integrate a wide array of Vision algorithms into your app. We‚Äôll tour the updated API and share sample code, along with best practices, to help you get the benefits of this framework with less coding effort. We‚Äôll also demonstrate two new features: image aesthetics and holistic body pose."}],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10163-Discover-Swift-enhancements-in-the-Vision-framework","interfaceLanguage":"swift"},"references":{"https://developer.apple.com/videos/play/wwdc2024/10163":{"url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2024\/10163","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2024\/10163","type":"download","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10045-Detect-animal-poses-in-Vision":{"role":"sampleCode","kind":"article","abstract":[{"type":"text","text":"Go beyond detecting cats and dogs in images. We‚Äôll show you how to use Vision to detect the individual joints and poses of these animals as well ‚Äî all in real time ‚Äî and share how you can enable exciting features like animal tracking for a camera app, creative embellishment on an animal photo, and more. We‚Äôll also explore other important enhancements to Vision and share best practices."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","title":"Detect animal poses in Vision","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc23-10045-detect-animal-poses-in-vision"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision":{"role":"sampleCode","abstract":[{"type":"text","text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We‚Äôll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image."}],"url":"\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision","kind":"article","title":"Explore 3D body pose and person segmentation in Vision","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","type":"topic"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"text":"Learn More‚Ä¶","type":"text"}],"type":"link","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"WWDCNotes.png":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDCNotes.png"}],"type":"image","identifier":"WWDCNotes.png"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC24":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24","type":"topic","abstract":[{"type":"text","text":"Xcode 16, Swift 6, iOS 18, macOS 15 (Sequoia), tvOS 18, visionOS 2, watchOS 11."},{"type":"text","text":" "},{"type":"text","text":"New APIs: Swift Testing, "},{"type":"codeVoice","code":"FinanceKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"TabletopKit"},{"type":"text","text":", and more."}],"role":"collectionGroup","images":[{"type":"icon","identifier":"WWDC24-Icon.png"},{"type":"card","identifier":"WWDC24.jpeg"}],"title":"WWDC24","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24"},"doc://WWDCNotes/documentation/WWDCNotes":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","type":"topic","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"title":"WWDC Notes","url":"\/documentation\/wwdcnotes","kind":"symbol"},"WWDC24-Icon.png":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC24-Icon.png","traits":["1x","light"]}],"identifier":"WWDC24-Icon.png","type":"image","alt":null},"WWDC24.jpeg":{"type":"image","identifier":"WWDC24.jpeg","alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC24.jpeg"}]}}}