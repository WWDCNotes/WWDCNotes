{"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"primaryContentSections":[{"kind":"content","content":[{"type":"heading","level":2,"anchor":"overview","text":"Overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Sound classification is the task of taking a sound, and placing it into one of many categories."}]},{"type":"paragraph","inlineContent":[{"text":"Different ways to categorize the sound:","type":"text"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Instrument\/object that made the sound (guitar\/drums)"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Location\/texture of the sound (Nature\/City), even when there’s no particular sound that necessarily stands out"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"Attributes\/property of the sound (Laugh\/cry)"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"text":"When we tell Create ML to train a new model, the first thing Create ML is going to be doing when training this model is walking through each of the sound files we provided, and extracting audio features across the entire file.","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"When testing, we can pass a sound with multiple classes: CreateML will separate each recognized class by time. We can even do microphone recording and see CreateML recognizing things live! How cool is that?"}]},{"type":"paragraph","inlineContent":[{"text":"New framework for sound recognition: ","type":"text"},{"type":"reference","identifier":"https:\/\/developer.apple.com\/documentation\/soundanalysis","isActive":true},{"text":".","type":"text"}]},{"level":2,"type":"heading","text":"Written By","anchor":"Written-By"},{"columns":[{"size":1,"content":[{"inlineContent":[{"identifier":"zntfdr","type":"image"}],"type":"paragraph"}]},{"size":4,"content":[{"text":"Federico Zanetello","level":3,"type":"heading","anchor":"Federico-Zanetello"},{"inlineContent":[{"overridingTitle":"Contributed Notes","isActive":true,"type":"reference","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/zntfdr"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"isActive":true,"type":"reference","identifier":"https:\/\/zntfdr.dev"}],"type":"paragraph"}]}],"type":"row","numberOfColumns":5},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"level":2,"type":"heading","text":"Related Sessions","anchor":"Related-Sessions"},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10020-Compose-advanced-models-with-Create-ML-Components","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10036-Discover-builtin-sound-classification-in-SoundAnalysis","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework"]},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"type":"text","text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries."},{"type":"text","text":" "},{"type":"text","text":"This website is not made by, affiliated with, nor endorsed by Apple."}]}]}],"sampleCodeDownload":{"kind":"sampleDownload","action":{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc19\/425","overridingTitle":"Watch Video (20 min)"}},"abstract":[{"type":"text","text":"Learn how to quickly and easily create Core ML models capable of classifying the sounds heard in audio files and live audio streams. In addition to providing you the ability to train and evaluate these models, the Create ML app allows you to test the model performance in real-time using the microphone on your Mac. Leverage these on-device models in your app using the new Sound Analysis framework."}],"kind":"article","metadata":{"role":"sampleCode","title":"Training Sound Classification Models in Create ML","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC19"},"schemaVersion":{"patch":0,"minor":3,"major":0},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc19-425-training-sound-classification-models-in-create-ml"],"traits":[{"interfaceLanguage":"swift"}]}],"sections":[],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-425-Training-Sound-Classification-Models-in-Create-ML","interfaceLanguage":"swift"},"references":{"https://github.com/zntfdr":{"title":"GitHub","url":"https:\/\/github.com\/zntfdr","type":"link","titleInlineContent":[{"text":"GitHub","type":"text"}],"identifier":"https:\/\/github.com\/zntfdr"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10020-Compose-advanced-models-with-Create-ML-Components":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc22-10020-compose-advanced-models-with-create-ml-components","abstract":[{"type":"text","text":"Take your custom machine learning models to the next level with Create ML Components. We’ll show you how to work with temporal data like video or audio and compose models that can count repetitive human actions or provide advanced sound classification. We’ll also share best practices on using incremental fitting to speed up model training with new data."}],"role":"sampleCode","title":"Compose advanced models with Create ML Components","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10020-Compose-advanced-models-with-Create-ML-Components"},"https://developer.apple.com/documentation/soundanalysis":{"title":"SoundAnalysis","url":"https:\/\/developer.apple.com\/documentation\/soundanalysis","type":"link","titleInlineContent":[{"code":"SoundAnalysis","type":"codeVoice"}],"identifier":"https:\/\/developer.apple.com\/documentation\/soundanalysis"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Contributions are welcome!","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link","titleInlineContent":[{"text":"Contributions are welcome!","type":"text"}],"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"doc://WWDCNotes/documentation/WWDCNotes":{"role":"collection","type":"topic","url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"images":[{"identifier":"WWDCNotes.png","type":"icon"}],"kind":"symbol","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","title":"WWDC Notes"},"zntfdr.jpeg":{"alt":null,"variants":[{"url":"\/images\/zntfdr.jpeg","traits":["1x","light"]}],"type":"image","identifier":"zntfdr.jpeg"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"type":"topic","url":"\/documentation\/wwdcnotes\/zntfdr","images":[{"identifier":"zntfdr.jpeg","type":"card"},{"identifier":"zntfdr.jpeg","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","abstract":[{"text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more.","type":"text"}],"title":"Federico Zanetello (332 notes)","kind":"article","role":"sampleCode"},"https://developer.apple.com/wwdc19/425":{"checksum":null,"url":"https:\/\/developer.apple.com\/wwdc19\/425","type":"download","identifier":"https:\/\/developer.apple.com\/wwdc19\/425"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10036-Discover-builtin-sound-classification-in-SoundAnalysis":{"title":"Discover built-in sound classification in SoundAnalysis","type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc21-10036-discover-builtin-sound-classification-in-soundanalysis","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10036-Discover-builtin-sound-classification-in-SoundAnalysis","abstract":[{"type":"text","text":"Explore how you can use the Sound Analysis framework in your app to detect and classify discrete sounds from any audio source — including live sounds from a microphone or from a video or audio file — and identify precisely in a moment where that sound occurs. Learn how the built-in sound classifier makes it easy for you to identify over 300 different types of sounds without the need for a custom trained model. This includes a variety of noises, ranging from human sounds, musical instruments, animals, and various items."}]},"WWDC19.jpeg":{"alt":null,"variants":[{"url":"\/images\/WWDC19.jpeg","traits":["1x","light"]}],"type":"image","identifier":"WWDC19.jpeg"},"zntfdr":{"alt":"Profile image of Federico Zanetello","variants":[{"url":"\/images\/zntfdr.jpeg","traits":["1x","light"]}],"type":"image","identifier":"zntfdr"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-704-Core-ML-3-Framework":{"abstract":[{"text":"Core ML 3 now enables support for advanced model types that were never before available in on-device machine learning. Learn how model personalization brings amazing personalization opportunities to your app. Gain a deeper understanding of strategies for linking models and improvements to Core ML tools used for conversion of existing models.","type":"text"}],"title":"Core ML 3 Framework","kind":"article","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-704-Core-ML-3-Framework","url":"\/documentation\/wwdcnotes\/wwdc19-704-core-ml-3-framework","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-209-Whats-New-in-Machine-Learning":{"kind":"article","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","title":"What’s New in Machine Learning","url":"\/documentation\/wwdcnotes\/wwdc19-209-whats-new-in-machine-learning","role":"sampleCode","abstract":[{"text":"Core ML 3 has been greatly expanded to enable even more amazing, on-device machine learning capabilities in your app. Learn about the new Create ML app which makes it easy to build Core ML models for many tasks. Get an overview of model personalization; exciting updates in Vision, Natural Language, Sound, and Speech; and added support for cutting-edge model types.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification":{"title":"Create ML for Object Detection and Sound Classification","role":"sampleCode","abstract":[{"text":"Create ML enables you to create, evaluate, and test powerful, production-class Core ML models. See how easy it is to create your own Object Detection and Sound Classification models for use in your apps. Learn strategies for balancing your training data to achieve great model accuracy.","type":"text"}],"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19-406-create-ml-for-object-detection-and-sound-classification","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-406-Create-ML-for-Object-Detection-and-Sound-Classification"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19","images":[{"identifier":"WWDC19-Icon.png","type":"icon"},{"identifier":"WWDC19.jpeg","type":"card"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","abstract":[{"text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Combine","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Core Haptics","type":"codeVoice"},{"text":", ","type":"text"},{"code":"Create ML","type":"codeVoice"},{"text":", and more.","type":"text"}],"title":"WWDC19","kind":"article","role":"collectionGroup"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-430-Introducing-the-Create-ML-App":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-430-Introducing-the-Create-ML-App","abstract":[{"type":"text","text":"Bringing the power of Core ML to your app begins with one challenge. How do you create your model? The new Create ML app provides an intuitive workflow for model creation. See how to train, evaluate, test, and preview your models quickly in this easy-to-use tool. Get started with one of the many available templates handling a number of powerful machine learning tasks. Learn more about the many features for continuous model improvement and experimentation."}],"title":"Introducing the Create ML App","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc19-430-introducing-the-create-ml-app","role":"sampleCode","kind":"article"},"WWDC19-Icon.png":{"alt":null,"variants":[{"url":"\/images\/WWDC19-Icon.png","traits":["1x","light"]}],"type":"image","identifier":"WWDC19-Icon.png"},"WWDCNotes.png":{"alt":null,"variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"type":"image","identifier":"WWDCNotes.png"},"https://zntfdr.dev":{"title":"Blog","url":"https:\/\/zntfdr.dev","type":"link","titleInlineContent":[{"text":"Blog","type":"text"}],"identifier":"https:\/\/zntfdr.dev"}}}