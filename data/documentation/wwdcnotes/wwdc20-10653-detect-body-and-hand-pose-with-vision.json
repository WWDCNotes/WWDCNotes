{"kind":"article","variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc20-10653-detect-body-and-hand-pose-with-vision"]}],"schemaVersion":{"major":0,"minor":3,"patch":0},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10653-Detect-Body-and-Hand-Pose-with-Vision","interfaceLanguage":"swift"},"sections":[],"sampleCodeDownload":{"action":{"type":"reference","isActive":true,"identifier":"https:\/\/developer.apple.com\/wwdc20\/10653","overridingTitle":"Watch Video (24 min)"},"kind":"sampleDownload"},"metadata":{"modules":[{"name":"WWDC Notes"}],"title":"Detect Body and Hand Pose with Vision","role":"sampleCode","roleHeading":"WWDC20"},"primaryContentSections":[{"kind":"content","content":[{"level":2,"anchor":"overview","type":"heading","text":"Overview"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"21 points on the hand are recognized"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"4 points per finger plus one for the wrist","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Use ","type":"text"},{"type":"codeVoice","code":"VNDetectHumanHandPoseRequest"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Default maximum hand count is 2","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Multiple bodies supported, too","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"5 points for the face, nose, eyes and ears"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"3 points per arm","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"6 for torso (overlapping shoulders with arm)"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"3 points per leg (overlapping hip with torso)","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"Could also be used for offline analysis of e.g. a photo collection","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Can be combined with ","type":"text"},{"code":"CreateML","type":"codeVoice"},{"text":" classification","type":"text"}],"type":"paragraph"}]}]},{"level":2,"anchor":"Written-By","type":"heading","text":"Written By"},{"columns":[{"content":[{"inlineContent":[{"identifier":"Jeehut","type":"image"}],"type":"paragraph"}],"size":1},{"content":[{"type":"heading","anchor":"Cihat-Gündüz","text":"Cihat Gündüz","level":3},{"type":"paragraph","inlineContent":[{"type":"reference","isActive":true,"overridingTitle":"Contributed Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/Jeehut","overridingTitleInlineContent":[{"type":"text","text":"Contributed Notes"}]},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","isActive":true,"identifier":"https:\/\/github.com\/Jeehut"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","isActive":true,"identifier":"https:\/\/fline.dev"},{"text":" ","type":"text"},{"text":"|","type":"text"},{"text":" ","type":"text"},{"type":"reference","isActive":true,"identifier":"https:\/\/x.com\/Jeehut"}]}],"size":4}],"type":"row","numberOfColumns":5},{"type":"paragraph","inlineContent":[{"type":"text","text":"Missing anything? Corrections? "},{"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true}]},{"level":2,"anchor":"Related-Sessions","type":"heading","text":"Related Sessions"},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10099-Explore-the-Action-and-Vision-app","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10673-Explore-Computer-Vision-APIs"]},{"type":"small","inlineContent":[{"type":"strong","inlineContent":[{"text":"Legal Notice","type":"text"}]}]},{"type":"small","inlineContent":[{"type":"text","text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved."},{"type":"text","text":" "},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}]}]}],"abstract":[{"type":"text","text":"Explore how the Vision framework can help your app detect body and hand poses in photos and video. With pose detection, your app can analyze the poses, movements, and gestures of people to offer new video editing possibilities, or to perform action classification when paired with an action classifier built in Create ML. And we’ll show you how you can bring gesture recognition into your app through hand pose, delivering a whole new form of interaction."}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20"]]},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit":{"type":"topic","abstract":[{"type":"text","text":"Discover how you can create incredible photo and video experiences in your camera app when integrating with DockKit-compatible motorized stands. We’ll show how your app can automatically track subjects in live video across a 360-degree field of view, take direct control of the stand to customize framing, directly control the motors, and provide your own inference model for tracking other objects. Finally, we’ll demonstrate how to create a sense of emotion through dynamic device animations."}],"kind":"article","role":"sampleCode","title":"Integrate with motorized iPhone stands using DockKit","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10304-Integrate-with-motorized-iPhone-stands-using-DockKit","url":"\/documentation\/wwdcnotes\/wwdc23-10304-integrate-with-motorized-iphone-stands-using-dockkit"},"Jeehut":{"type":"image","alt":"Profile image of Cihat Gündüz","identifier":"Jeehut","variants":[{"traits":["1x","light"],"url":"\/images\/Jeehut.jpeg"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML":{"url":"\/documentation\/wwdcnotes\/wwdc21-10039-classify-hand-poses-and-actions-with-create-ml","title":"Classify hand poses and actions with Create ML","abstract":[{"text":"With Create ML, your app’s ability to understand the expressiveness of the human hand has never been easier. Discover how you can build off the support for Hand Pose Detection in Vision and train custom Hand Pose and Hand Action classifiers using the Create ML app and framework. Learn how simple it is to collect data, train a model, and integrate it with Vision, Camera, and ARKit to create a fun, entertaining app experience.","type":"text"}],"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10039-Classify-hand-poses-and-actions-with-Create-ML","type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20":{"url":"\/documentation\/wwdcnotes\/wwdc20","images":[{"identifier":"WWDC20-Icon.png","type":"icon"},{"identifier":"WWDC20.jpeg","type":"card"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20","abstract":[{"text":"Xcode 12, Swift 5.3, iOS 14, macOS 11 (Big Sur), tvOS 14, watchOS 7.","type":"text"},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"WidgetKit"},{"type":"text","text":", "},{"code":"StoreKit Testing","type":"codeVoice"},{"text":", and more.","type":"text"}],"title":"WWDC20","role":"collectionGroup","type":"topic","kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/Jeehut":{"type":"topic","abstract":[{"type":"text","text":"Spatial-first Indie Developer for  Platforms. Actively contributing to Open Source since 2011!"}],"kind":"article","role":"sampleCode","images":[{"type":"card","identifier":"Jeehut.jpeg"},{"type":"icon","identifier":"Jeehut.jpeg"}],"title":"Cihat Gündüz (60 notes)","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/Jeehut","url":"\/documentation\/wwdcnotes\/jeehut"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10040-Detect-people-faces-and-poses-using-Vision":{"abstract":[{"text":"Discover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app.","type":"text"}],"title":"Detect people, faces, and poses using Vision","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc21-10040-detect-people-faces-and-poses-using-vision","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","role":"sampleCode"},"Jeehut.jpeg":{"type":"image","alt":null,"identifier":"Jeehut.jpeg","variants":[{"traits":["1x","light"],"url":"\/images\/Jeehut.jpeg"}]},"WWDC20-Icon.png":{"type":"image","alt":null,"identifier":"WWDC20-Icon.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC20-Icon.png"}]},"https://fline.dev":{"url":"https:\/\/fline.dev","type":"link","title":"Blog","titleInlineContent":[{"type":"text","text":"Blog"}],"identifier":"https:\/\/fline.dev"},"https://github.com/Jeehut":{"url":"https:\/\/github.com\/Jeehut","type":"link","title":"GitHub","titleInlineContent":[{"type":"text","text":"GitHub"}],"identifier":"https:\/\/github.com\/Jeehut"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10099-Explore-the-Action-and-Vision-app":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10099-Explore-the-Action-and-Vision-app","role":"sampleCode","title":"Explore the Action & Vision app","url":"\/documentation\/wwdcnotes\/wwdc20-10099-explore-the-action-and-vision-app","type":"topic","kind":"article","abstract":[{"type":"text","text":"It’s now easy to create an app for fitness or sports coaching that takes advantage of machine learning — and to prove it, we built our own. Learn how we designed the Action & Vision app using Object Detection and Action Classification in Create ML along with the new Body Pose Estimation, Trajectory Detection, and Contour Detection features in the Vision framework. Explore how you can create an immersive application for gameplay or training from setup to analysis and feedback. And follow along in Xcode with a full sample project."}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10673-Explore-Computer-Vision-APIs":{"type":"topic","role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10673-Explore-Computer-Vision-APIs","url":"\/documentation\/wwdcnotes\/wwdc20-10673-explore-computer-vision-apis","title":"Explore Computer Vision APIs","kind":"article","abstract":[{"text":"Learn how to bring Computer Vision intelligence to your app when you combine the power of Core Image, Vision, and Core ML. Go beyond machine learning alone and gain a deeper understanding of images and video. Discover new APIs in Core Image and Vision to bring Computer Vision to your application like new thresholding filters as well as Contour Detection and Optical Flow. And consider ways to use Core Image for preprocessing and visualization of these results.","type":"text"}]},"https://developer.apple.com/wwdc20/10653":{"url":"https:\/\/developer.apple.com\/wwdc20\/10653","type":"download","checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc20\/10653"},"WWDC20.jpeg":{"type":"image","alt":null,"identifier":"WWDC20.jpeg","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC20.jpeg"}]},"WWDCNotes.png":{"type":"image","alt":null,"identifier":"WWDCNotes.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision":{"title":"Explore 3D body pose and person segmentation in Vision","abstract":[{"text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We’ll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image.","type":"text"}],"type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","url":"\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision","role":"sampleCode"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link","title":"Contributions are welcome!","titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"https://x.com/Jeehut":{"url":"https:\/\/x.com\/Jeehut","type":"link","title":"X\/Twitter","titleInlineContent":[{"type":"text","text":"X\/Twitter"}],"identifier":"https:\/\/x.com\/Jeehut"},"doc://WWDCNotes/documentation/WWDCNotes":{"role":"collection","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"title":"WWDC Notes","type":"topic","abstract":[{"text":"Session notes shared by the community for the community.","type":"text"}],"kind":"symbol","url":"\/documentation\/wwdcnotes"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML":{"type":"topic","kind":"article","role":"sampleCode","abstract":[{"text":"Discover how to build Action Classification models in Create ML. With a custom action classifier, your app can recognize and understand body movements in real-time from videos or through a camera. We’ll show you how to use samples to easily train a Core ML model to identify human actions like jumping jacks, squats, and dance moves. Learn how this is powered by the Body Pose estimation features of the Vision Framework. Get inspired to create apps that can provide coaching for fitness routines, deliver feedback on athletic form, and more.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10043-Build-an-Action-Classifier-with-Create-ML","url":"\/documentation\/wwdcnotes\/wwdc20-10043-build-an-action-classifier-with-create-ml","title":"Build an Action Classifier with Create ML"}}}