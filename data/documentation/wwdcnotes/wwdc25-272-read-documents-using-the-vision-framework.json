{"kind":"article","sections":[],"metadata":{"role":"sampleCode","title":"Read documents using the Vision framework","roleHeading":"WWDC25","modules":[{"name":"WWDC Notes"}]},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc25-272-read-documents-using-the-vision-framework"]}],"abstract":[{"type":"text","text":"Learn about the latest advancements in the Vision framework. We‚Äôll introduce RecognizeDocumentsRequest, and how you can use it to read lines of text and group them into paragraphs, read tables, etc. And we‚Äôll also dive into camera lens smudge detection, and how to identify potentially smudged images in photo libraries or your own camera capture pipeline."}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-272-Read-documents-using-the-Vision-framework"},"primaryContentSections":[{"kind":"content","content":[{"anchor":"overview","level":2,"type":"heading","text":"Overview"},{"inlineContent":[{"type":"text","text":"üò± ‚ÄúNo Overview Available!‚Äù"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:"},{"type":"text","text":" "},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}],"type":"paragraph"},{"anchor":"Related-Sessions","level":2,"type":"heading","text":"Related Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10163-Discover-Swift-enhancements-in-the-Vision-framework","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-360-Discover-machine-learning-and-AI-frameworks-on-Apple-platforms"],"style":"list","type":"links"}]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25"]]},"schemaVersion":{"major":0,"patch":0,"minor":3},"sampleCodeDownload":{"kind":"sampleDownload","action":{"isActive":true,"identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2025\/272","type":"reference","overridingTitle":"Watch Video (20 min)"}},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC24-10163-Discover-Swift-enhancements-in-the-Vision-framework":{"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC24-10163-Discover-Swift-enhancements-in-the-Vision-framework","title":"Discover Swift enhancements in the Vision framework","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc24-10163-discover-swift-enhancements-in-the-vision-framework","role":"sampleCode","abstract":[{"type":"text","text":"The Vision Framework API has been redesigned to leverage modern Swift features like concurrency, making it easier and faster to integrate a wide array of Vision algorithms into your app. We‚Äôll tour the updated API and share sample code, along with best practices, to help you get the benefits of this framework with less coding effort. We‚Äôll also demonstrate two new features: image aesthetics and holistic body pose."}]},"WWDC25-Icon.png":{"type":"image","identifier":"WWDC25-Icon.png","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDC25-Icon.png"}],"alt":null},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"type":"link","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"text":"Learn More‚Ä¶","type":"text"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶"},"WWDCNotes.png":{"alt":null,"identifier":"WWDCNotes.png","type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}]},"https://developer.apple.com/videos/play/wwdc2025/272":{"type":"download","identifier":"https:\/\/developer.apple.com\/videos\/play\/wwdc2025\/272","url":"https:\/\/developer.apple.com\/videos\/play\/wwdc2025\/272","checksum":null},"doc://WWDCNotes/documentation/WWDCNotes":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","type":"topic","url":"\/documentation\/wwdcnotes","role":"collection","kind":"symbol","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"title":"WWDC Notes","images":[{"type":"icon","identifier":"WWDCNotes.png"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC25-360-Discover-machine-learning-and-AI-frameworks-on-Apple-platforms":{"kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25-360-Discover-machine-learning-and-AI-frameworks-on-Apple-platforms","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc25-360-discover-machine-learning-and-ai-frameworks-on-apple-platforms","abstract":[{"type":"text","text":"Tour the latest updates to machine learning and AI frameworks available on Apple platforms. Whether you are an app developer ready to tap into Apple Intelligence, an ML engineer optimizing models for on-device deployment, or an AI enthusiast exploring the frontier of what is possible, we‚Äôll offer guidance to help select the right tools for your needs."}],"title":"Discover machine learning & AI frameworks on Apple platforms","type":"topic"},"WWDC25.jpg":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC25.jpg","traits":["1x","light"]}],"alt":null,"identifier":"WWDC25.jpg"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-10045-Detect-animal-poses-in-Vision":{"title":"Detect animal poses in Vision","type":"topic","abstract":[{"type":"text","text":"Go beyond detecting cats and dogs in images. We‚Äôll show you how to use Vision to detect the individual joints and poses of these animals as well ‚Äî all in real time ‚Äî and share how you can enable exciting features like animal tracking for a camera app, creative embellishment on an animal photo, and more. We‚Äôll also explore other important enhancements to Vision and share best practices."}],"kind":"article","role":"sampleCode","url":"\/documentation\/wwdcnotes\/wwdc23-10045-detect-animal-poses-in-vision","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-10045-Detect-animal-poses-in-Vision"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision":{"role":"sampleCode","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC23-111241-Explore-3D-body-pose-and-person-segmentation-in-Vision","url":"\/documentation\/wwdcnotes\/wwdc23-111241-explore-3d-body-pose-and-person-segmentation-in-vision","title":"Explore 3D body pose and person segmentation in Vision","type":"topic","abstract":[{"text":"Discover how to build person-centric features with Vision. Learn how to detect human body poses and measure individual joint locations in 3D space. We‚Äôll also show you how to take advantage of person segmentation APIs to distinguish and segment up to four individuals in an image.","type":"text"}],"kind":"article"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC25":{"kind":"article","role":"collectionGroup","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC25","abstract":[{"text":"Xcode 26, Swift 6.2, iOS\/macOS\/tvOS\/visionOS 26.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"code":"Foundation Models","type":"codeVoice"},{"text":", ","type":"text"},{"type":"codeVoice","code":"AlarmKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"PermissionKit"},{"type":"text","text":", and more."}],"images":[{"identifier":"WWDC25-Icon.png","type":"icon"},{"identifier":"WWDC25.jpg","type":"card"}],"url":"\/documentation\/wwdcnotes\/wwdc25","type":"topic","title":"WWDC25"}}}