{"primaryContentSections":[{"kind":"content","content":[{"level":2,"anchor":"Image-Saliency","type":"heading","text":"Image Saliency"},{"inlineContent":[{"type":"emphasis","inlineContent":[{"type":"text","text":"Saliency = Most noticeable or important"}]}],"type":"paragraph"},{"inlineContent":[{"text":"Vision offers two types of Saliency:","type":"text"}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Attention Based: Based on humans, creating a heat map on the image guessing where we, humans, will look first."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Objectness Based: Creates a heatmap with the goal to highlight the foreground objects or the subjects of an image."}]}]}]},{"inlineContent":[{"type":"image","identifier":"WWDC19-222-penguins"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"| Original Image | Attention Based Saliency | Objectness Based Saliency |"}],"type":"paragraph"},{"inlineContent":[{"text":"Attention Based Saliency is the hardest one, because it involves many factors including: contrast, faces, subjects, horizons, light, perceived motion, …","type":"text"}],"type":"paragraph"},{"level":3,"anchor":"Hows-it-done","type":"heading","text":"How’s it done?"},{"inlineContent":[{"type":"text","text":"The heat map is requested via a "},{"code":"VNImageRequestHandler","type":"codeVoice"},{"type":"text","text":" where we will perform a attention\/objectness based request, which will return an optional heat map made of floats from 0 (non salient) to 1 (max saliency), this map is smaller than our original pic so we will need to scale in order to obtain the effect in the images above."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Beside the heatmap, vision gives us also a bounding box which encapsulates the saliency area:"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC19-222-saliencyArea"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Attention based saliency will always have up to 1 bounding box, while objectless based saliency will have up to 3 bounding boxes."},{"type":"text","text":"\n"},{"type":"text","text":"The bounding boxes are normalized and respect the image dimensions."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"(This is awesome when showing a thumbnail of an image.)"}],"type":"paragraph"},{"level":2,"anchor":"Image-Classification","type":"heading","text":"Image Classification"},{"inlineContent":[{"text":"Over one thousand of objects recognizable.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"The classification is based on Taxonomy, which is a hierarchical structure:","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC19-222-tree"}],"type":"paragraph"},{"level":3,"anchor":"Hows-it-done","type":"heading","text":"How’s it done?"},{"inlineContent":[{"text":"Like before, in order to get a classification we need a ","type":"text"},{"code":"VNImageRequestHandler","type":"codeVoice"},{"text":" and submit a ","type":"text"},{"code":"VNClassifyRequest","type":"codeVoice"},{"text":".","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"This request will give us back an array of observations, each observation will have a label and a confidence."},{"type":"text","text":" "},{"type":"text","text":"E.g. (animal, 0.8), (cat, 0.75), (hat, 0.8)"}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Beside confidence, each class has a recall and precision value, both of which differ between different objects types (classes)."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"We can use these values, along with the confidence value, to more securely filter the results of our classification."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"These values are very powerful as they’re custom for each object kind."}],"type":"paragraph"},{"inlineContent":[{"text":"Based on our images, we can tweak these values in order to not return too few matches, or not too many false positives.","type":"text"}],"type":"paragraph"},{"level":2,"anchor":"Image-Similarity","type":"heading","text":"Image Similarity"},{"inlineContent":[{"type":"text","text":"In short, each image has a FeaturePrint (computed via saliency and more), which is an image descriptor similar to a word vector (obtained via classification)."}],"type":"paragraph"},{"inlineContent":[{"text":"Similarity is a number (actually, a vector distance):","type":"text"},{"text":" ","type":"text"},{"text":"the smaller it is, the more similar the images are.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"type":"image","identifier":"WWDC19-222-cat"}],"type":"paragraph"},{"inlineContent":[{"text":"Cats pics are more similar than a crocodile pics.","type":"text"}],"type":"paragraph"},{"level":2,"anchor":"Face-Technology","type":"heading","text":"Face Technology"},{"inlineContent":[{"type":"text","text":"Previously we only had one confidence score, now we have one for each face feature. Speaking of features, we now have 76 points\/feature, while previously we only had 65. This means that we can now recognize more precisely different faces (think of twins etc)."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Pupil detection is much better."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"New people and pet detector."},{"type":"text","text":" "},{"type":"text","text":"returns bounding boxes of the human (torso area), or the pet, along with a label describing which animal."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Improved Vision tracking."}],"type":"paragraph"},{"inlineContent":[{"text":"In all Machine Learning sessions Apple is making available to developers tools that Apple has been using in the previous years. If you think about it, you will find an use of each Machine Learning feature in Apple’s stock apps.","type":"text"}],"type":"paragraph"},{"inlineContent":[{"text":"For example:","type":"text"},{"text":" ","type":"text"},{"text":"Saliency is one of the most important pillars to make the photos.app smart:","type":"text"}],"type":"paragraph"},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Given an image","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Detect the important areas via saliency bounding boxes","type":"text"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"Crop bounding boxes and run them on vision to categorize the original pic.","type":"text"}]}]}]},{"inlineContent":[{"identifier":"WWDC19-222-food","type":"image"}],"type":"paragraph"},{"level":2,"anchor":"Written-By","type":"heading","text":"Written By"},{"type":"row","numberOfColumns":5,"columns":[{"content":[{"type":"paragraph","inlineContent":[{"type":"image","identifier":"zntfdr"}]}],"size":1},{"content":[{"text":"Federico Zanetello","level":3,"type":"heading","anchor":"Federico-Zanetello"},{"inlineContent":[{"isActive":true,"type":"reference","overridingTitle":"Contributed Notes","overridingTitleInlineContent":[{"text":"Contributed Notes","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/github.com\/zntfdr"},{"type":"text","text":" "},{"type":"text","text":"|"},{"type":"text","text":" "},{"isActive":true,"type":"reference","identifier":"https:\/\/zntfdr.dev"}],"type":"paragraph"}],"size":4}]},{"type":"thematicBreak"},{"inlineContent":[{"text":"Missing anything? Corrections? ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true,"type":"reference"}],"type":"paragraph"},{"level":2,"anchor":"Related-Sessions","type":"heading","text":"Related Sessions"},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10673-Explore-Computer-Vision-APIs","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-234-Text-Recognition-in-Vision-Framework"]},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"text":"All content copyright © 2012 – 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19"]]},"sampleCodeDownload":{"action":{"isActive":true,"overridingTitle":"Watch Video (39 min)","identifier":"https:\/\/developer.apple.com\/wwdc19\/222","type":"reference"},"kind":"sampleDownload"},"kind":"article","sections":[],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-222-Understanding-Images-in-Vision-Framework","interfaceLanguage":"swift"},"metadata":{"title":"Understanding Images in Vision Framework","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC19","role":"sampleCode"},"schemaVersion":{"major":0,"minor":3,"patch":0},"abstract":[{"type":"text","text":"Learn all about the many advances in the Vision Framework including effortless image classification, image saliency, determining image similarity, and improvements in facial feature detection, and face capture quality scoring. This packed session will show you how easy it is to bring powerful computer vision techniques to your apps."}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/wwdcnotes\/wwdc19-222-understanding-images-in-vision-framework"]}],"references":{"WWDC19-222-food":{"type":"image","identifier":"WWDC19-222-food","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-222-food.png","traits":["1x","light"]}],"alt":null},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"type":"text","text":"Contributions are welcome!"}],"url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","title":"Contributions are welcome!","type":"link","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"zntfdr.jpeg":{"identifier":"zntfdr.jpeg","alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes\/zntfdr.jpeg","traits":["1x","light"]}]},"https://developer.apple.com/wwdc19/222":{"type":"download","identifier":"https:\/\/developer.apple.com\/wwdc19\/222","checksum":null,"url":"https:\/\/developer.apple.com\/wwdc19\/222"},"WWDC19-222-saliencyArea":{"type":"image","identifier":"WWDC19-222-saliencyArea","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-222-saliencyArea.png","traits":["1x","light"]}],"alt":null},"WWDC19-222-cat":{"type":"image","identifier":"WWDC19-222-cat","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-222-cat.png","traits":["1x","light"]}],"alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19":{"role":"collectionGroup","images":[{"type":"icon","identifier":"WWDC19-Icon.png"},{"type":"card","identifier":"WWDC19.jpeg"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19","title":"WWDC19","type":"topic","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19","abstract":[{"text":"Xcode 11, Swift 5.1, iOS 13, macOS 10.15 (Catalina), tvOS 13, watchOS 6.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"type":"codeVoice","code":"Combine"},{"text":", ","type":"text"},{"type":"codeVoice","code":"Core Haptics"},{"text":", ","type":"text"},{"type":"codeVoice","code":"Create ML"},{"text":", and more.","type":"text"}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-209-Whats-New-in-Machine-Learning":{"kind":"article","abstract":[{"text":"Core ML 3 has been greatly expanded to enable even more amazing, on-device machine learning capabilities in your app. Learn about the new Create ML app which makes it easy to build Core ML models for many tasks. Get an overview of model personalization; exciting updates in Vision, Natural Language, Sound, and Speech; and added support for cutting-edge model types.","type":"text"}],"url":"\/documentation\/wwdcnotes\/wwdc19-209-whats-new-in-machine-learning","title":"What’s New in Machine Learning","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-209-Whats-New-in-Machine-Learning","role":"sampleCode"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC20-10673-Explore-Computer-Vision-APIs":{"url":"\/documentation\/wwdcnotes\/wwdc20-10673-explore-computer-vision-apis","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC20-10673-Explore-Computer-Vision-APIs","title":"Explore Computer Vision APIs","abstract":[{"type":"text","text":"Learn how to bring Computer Vision intelligence to your app when you combine the power of Core Image, Vision, and Core ML. Go beyond machine learning alone and gain a deeper understanding of images and video. Discover new APIs in Core Image and Vision to bring Computer Vision to your application like new thresholding filters as well as Contour Detection and Optical Flow. And consider ways to use Core Image for preprocessing and visualization of these results."}],"role":"sampleCode","type":"topic"},"WWDC19-Icon.png":{"identifier":"WWDC19-Icon.png","alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-Icon.png","traits":["1x","light"]}]},"https://zntfdr.dev":{"titleInlineContent":[{"type":"text","text":"Blog"}],"url":"https:\/\/zntfdr.dev","title":"Blog","type":"link","identifier":"https:\/\/zntfdr.dev"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC19-234-Text-Recognition-in-Vision-Framework":{"abstract":[{"text":"Document Camera and Text Recognition features in Vision Framework enable you to extract text data from images. Learn how to leverage this built-in machine learning technology in your app. Gain a deeper understanding of the differences between fast versus accurate processing as well as character-based versus language-based recognition.","type":"text"}],"kind":"article","url":"\/documentation\/wwdcnotes\/wwdc19-234-text-recognition-in-vision-framework","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC19-234-Text-Recognition-in-Vision-Framework","role":"sampleCode","title":"Text Recognition in Vision Framework"},"doc://WWDCNotes/documentation/WWDCNotes/zntfdr":{"title":"Federico Zanetello (332 notes)","type":"topic","role":"sampleCode","abstract":[{"type":"text","text":"Software engineer with a strong passion for well-written code, thought-out composable architectures, automation, tests, and more."}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/zntfdr","kind":"article","images":[{"type":"card","identifier":"zntfdr.jpeg"},{"type":"icon","identifier":"zntfdr.jpeg"}],"url":"\/documentation\/wwdcnotes\/zntfdr"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10019-Get-to-know-Create-ML-Components":{"abstract":[{"type":"text","text":"Create ML makes it easy to build custom machine learning models for image classification, object detection, sound classification, hand pose classification, action classification, tabular data regression, and more. And with the Create ML Components framework, you can further customize underlying tasks and improve your model. We’ll explore the feature extractors, transformers, and estimators that make up these tasks, and show you how you can combine them with other components and pre-processing steps to build custom tasks for concepts like image regression."}],"url":"\/documentation\/wwdcnotes\/wwdc22-10019-get-to-know-create-ml-components","title":"Get to know Create ML Components","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10019-Get-to-know-Create-ML-Components","role":"sampleCode","kind":"article"},"WWDC19.jpeg":{"type":"image","identifier":"WWDC19.jpeg","variants":[{"url":"\/images\/WWDCNotes\/WWDC19.jpeg","traits":["1x","light"]}],"alt":null},"WWDC19-222-penguins":{"identifier":"WWDC19-222-penguins","alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-222-penguins.png","traits":["1x","light"]}]},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10040-Detect-people-faces-and-poses-using-Vision":{"title":"Detect people, faces, and poses using Vision","url":"\/documentation\/wwdcnotes\/wwdc21-10040-detect-people-faces-and-poses-using-vision","role":"sampleCode","abstract":[{"text":"Discover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app.","type":"text"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","kind":"article","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes":{"role":"collection","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","title":"WWDC Notes","type":"topic","kind":"symbol","url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}]},"zntfdr":{"type":"image","identifier":"zntfdr","variants":[{"url":"\/images\/WWDCNotes\/zntfdr.jpeg","traits":["1x","light"]}],"alt":"Profile image of Federico Zanetello"},"WWDCNotes.png":{"type":"image","identifier":"WWDCNotes.png","variants":[{"url":"\/images\/WWDCNotes\/WWDCNotes.png","traits":["1x","light"]}],"alt":null},"WWDC19-222-tree":{"identifier":"WWDC19-222-tree","alt":null,"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC19-222-tree.png","traits":["1x","light"]}]},"https://github.com/zntfdr":{"titleInlineContent":[{"type":"text","text":"GitHub"}],"url":"https:\/\/github.com\/zntfdr","title":"GitHub","type":"link","identifier":"https:\/\/github.com\/zntfdr"}}}