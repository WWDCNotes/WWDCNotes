{"primaryContentSections":[{"kind":"content","content":[{"text":"Overview","level":2,"type":"heading","anchor":"overview"},{"inlineContent":[{"type":"text","text":"üò± ‚ÄúNo Overview Available!‚Äù"}],"type":"paragraph"},{"inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}],"type":"paragraph"},{"text":"Related Sessions","level":2,"type":"heading","anchor":"Related-Sessions"},{"style":"list","type":"links","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10025-Capture-machinereadable-codes-and-text-with-VisionKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision"]},{"inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Legal Notice"}]}],"type":"small"},{"inlineContent":[{"text":"All content copyright ¬© 2012 ‚Äì 2024 Apple Inc. All rights reserved.","type":"text"},{"text":" ","type":"text"},{"text":"Swift, the Swift logo, Swift Playgrounds, Xcode, Instruments, Cocoa Touch, Touch ID, FaceID, iPhone, iPad, Safari, Apple Vision, Apple Watch, App Store, iPadOS, watchOS, visionOS, tvOS, Mac, and macOS are trademarks of Apple Inc., registered in the U.S. and other countries.","type":"text"},{"text":" ","type":"text"},{"text":"This website is not made by, affiliated with, nor endorsed by Apple.","type":"text"}],"type":"small"}]}],"schemaVersion":{"patch":0,"major":0,"minor":3},"kind":"article","metadata":{"role":"sampleCode","title":"What‚Äôs new in Vision","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC22"},"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc22-10024-whats-new-in-vision"],"traits":[{"interfaceLanguage":"swift"}]}],"abstract":[{"text":"Learn about the latest updates to Vision APIs that help your apps recognize text, detect faces and face landmarks, and implement optical flow. We‚Äôll take you through the capabilities of optical flow for video-based apps, show you how to update your apps with revisions to the machine learning models that drive these APIs, and explore how you can visualize your Vision tasks with Quick Look Preview support in Xcode.","type":"text"}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10024-Whats-new-in-Vision"},"sections":[],"sampleCodeDownload":{"kind":"sampleDownload","action":{"identifier":"https:\/\/developer.apple.com\/wwdc22\/10024","type":"reference","isActive":true,"overridingTitle":"Watch Video (19 min)"}},"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22"]]},"references":{"doc://WWDCNotes/documentation/WWDCNotes/WWDC22":{"role":"collectionGroup","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22","url":"\/documentation\/wwdcnotes\/wwdc22","title":"WWDC22","kind":"article","abstract":[{"type":"text","text":"Xcode 14, Swift 5.7, iOS 16, macOS 13 (Ventura), tvOS 16, watchOS 9."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"WeatherKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"ScreenCaptureKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"Swift Regex"},{"type":"text","text":", and more."}],"type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10025-Capture-machinereadable-codes-and-text-with-VisionKit":{"role":"sampleCode","abstract":[{"text":"Meet the Data Scanner in VisionKit: This framework combines AVCapture and Vision to enable live capture of machine-readable codes and text through a simple Swift API. We‚Äôll show you how to control the types of content your app can capture by specifying barcode symbologies and language selection. We‚Äôll also explore how you can enable guidance in your app, customize item highlighting or regions of interest, and handle interactions after your app detects an item.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10025-Capture-machinereadable-codes-and-text-with-VisionKit","url":"\/documentation\/wwdcnotes\/wwdc22-10025-capture-machinereadable-codes-and-text-with-visionkit","kind":"article","title":"Capture machine-readable codes and text with VisionKit"},"doc://WWDCNotes/documentation/WWDCNotes":{"type":"topic","title":"WWDC Notes","kind":"symbol","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"url":"\/documentation\/wwdcnotes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10040-Detect-people-faces-and-poses-using-Vision":{"role":"sampleCode","kind":"article","url":"\/documentation\/wwdcnotes\/wwdc21-10040-detect-people-faces-and-poses-using-vision","abstract":[{"text":"Discover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app.","type":"text"}],"type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","title":"Detect people, faces, and poses using Vision"},"https://developer.apple.com/wwdc22/10024":{"url":"https:\/\/developer.apple.com\/wwdc22\/10024","type":"download","identifier":"https:\/\/developer.apple.com\/wwdc22\/10024","checksum":null},"https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"text":"Learn More‚Ä¶","type":"text"}],"url":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","type":"link","identifier":"https:\/\/wwdcnotes.github.io\/WWDCNotes\/documentation\/wwdcnotes\/contributing","title":"Learn More‚Ä¶"},"WWDCNotes.png":{"alt":null,"variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes.png"}],"type":"image","identifier":"WWDCNotes.png"}}}