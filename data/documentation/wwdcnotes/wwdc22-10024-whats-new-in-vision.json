{"metadata":{"role":"sampleCode","modules":[{"name":"WWDC Notes"}],"roleHeading":"WWDC22","title":"What‚Äôs new in Vision"},"sampleCodeDownload":{"action":{"overridingTitle":"Watch Video (19 min)","identifier":"https:\/\/developer.apple.com\/wwdc22\/10024","isActive":true,"type":"reference"},"kind":"sampleDownload"},"sections":[],"kind":"article","variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc22-10024-whats-new-in-vision"],"traits":[{"interfaceLanguage":"swift"}]}],"abstract":[{"text":"Learn about the latest updates to Vision APIs that help your apps recognize text, detect faces and face landmarks, and implement optical flow. We‚Äôll take you through the capabilities of optical flow for video-based apps, show you how to update your apps with revisions to the machine learning models that drive these APIs, and explore how you can visualize your Vision tasks with Quick Look Preview support in Xcode.","type":"text"}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22"]]},"schemaVersion":{"minor":3,"patch":0,"major":0},"primaryContentSections":[{"content":[{"text":"Overview","level":2,"type":"heading","anchor":"overview"},{"type":"paragraph","inlineContent":[{"text":"üò± ‚ÄúNo Overview Available!‚Äù","type":"text"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:"},{"type":"text","text":" "},{"type":"reference","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","isActive":true}]},{"text":"Related Sessions","level":2,"type":"heading","anchor":"Related-Sessions"},{"items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10025-Capture-machinereadable-codes-and-text-with-VisionKit","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision"],"style":"list","type":"links"}],"kind":"content"}],"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10024-Whats-new-in-Vision","interfaceLanguage":"swift"},"references":{"doc://WWDCNotes/documentation/WWDCNotes":{"title":"WWDC Notes","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","type":"topic","images":[{"type":"icon","identifier":"WWDCNotes.png"}],"url":"\/documentation\/wwdcnotes","role":"collection","kind":"symbol","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}]},"WWDC22-Icon.png":{"identifier":"WWDC22-Icon.png","type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC22-Icon.png"}],"alt":null},"WWDCNotes.png":{"identifier":"WWDCNotes.png","type":"image","variants":[{"url":"\/images\/WWDCNotes.png","traits":["1x","light"]}],"alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22":{"kind":"article","title":"WWDC22","abstract":[{"text":"Xcode 14, Swift 5.7, iOS 16, macOS 13 (Ventura), tvOS 16, watchOS 9.","type":"text"},{"text":" ","type":"text"},{"text":"New APIs: ","type":"text"},{"type":"codeVoice","code":"WeatherKit"},{"text":", ","type":"text"},{"type":"codeVoice","code":"ScreenCaptureKit"},{"text":", ","type":"text"},{"type":"codeVoice","code":"Swift Regex"},{"text":", and more.","type":"text"}],"images":[{"identifier":"WWDC22-Icon.png","type":"icon"},{"type":"card","identifier":"WWDC22.jpeg"}],"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc22","role":"collectionGroup"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10025-Capture-machinereadable-codes-and-text-with-VisionKit":{"abstract":[{"type":"text","text":"Meet the Data Scanner in VisionKit: This framework combines AVCapture and Vision to enable live capture of machine-readable codes and text through a simple Swift API. We‚Äôll show you how to control the types of content your app can capture by specifying barcode symbologies and language selection. We‚Äôll also explore how you can enable guidance in your app, customize item highlighting or regions of interest, and handle interactions after your app detects an item."}],"url":"\/documentation\/wwdcnotes\/wwdc22-10025-capture-machinereadable-codes-and-text-with-visionkit","role":"sampleCode","type":"topic","kind":"article","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10025-Capture-machinereadable-codes-and-text-with-VisionKit","title":"Capture machine-readable codes and text with VisionKit"},"WWDC22.jpeg":{"identifier":"WWDC22.jpeg","type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDC22.jpeg"}],"alt":null},"doc://WWDCNotes/documentation/WWDCNotes/WWDC21-10040-Detect-people-faces-and-poses-using-Vision":{"type":"topic","url":"\/documentation\/wwdcnotes\/wwdc21-10040-detect-people-faces-and-poses-using-vision","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC21-10040-Detect-people-faces-and-poses-using-Vision","abstract":[{"text":"Discover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app.","type":"text"}],"title":"Detect people, faces, and poses using Vision","role":"sampleCode","kind":"article"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}],"title":"Learn More‚Ä¶","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"link","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing"},"https://developer.apple.com/wwdc22/10024":{"identifier":"https:\/\/developer.apple.com\/wwdc22\/10024","type":"download","url":"https:\/\/developer.apple.com\/wwdc22\/10024","checksum":null}}}