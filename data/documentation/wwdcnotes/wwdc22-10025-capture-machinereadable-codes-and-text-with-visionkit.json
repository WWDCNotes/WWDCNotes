{"metadata":{"title":"Capture machine-readable codes and text with VisionKit","roleHeading":"WWDC22","role":"sampleCode","modules":[{"name":"WWDC Notes"}]},"identifier":{"url":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10025-Capture-machinereadable-codes-and-text-with-VisionKit","interfaceLanguage":"swift"},"sampleCodeDownload":{"action":{"type":"reference","isActive":true,"overridingTitle":"Watch Video (12 min)","identifier":"https:\/\/developer.apple.com\/wwdc22\/10025"},"kind":"sampleDownload"},"sections":[],"schemaVersion":{"patch":0,"major":0,"minor":3},"primaryContentSections":[{"content":[{"type":"heading","anchor":"overview","level":2,"text":"Overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"üò± ‚ÄúNo Overview Available!‚Äù"}]},{"type":"paragraph","inlineContent":[{"text":"Be the hero to change that by watching the video and providing notes! It‚Äôs super easy:","type":"text"},{"text":" ","type":"text"},{"identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","type":"reference","isActive":true}]},{"type":"heading","anchor":"Related-Sessions","level":2,"text":"Related Sessions"},{"type":"links","style":"list","items":["doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10024-Whats-new-in-Vision","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10026-Add-Live-Text-interaction-to-your-app"]}],"kind":"content"}],"variants":[{"paths":["\/documentation\/wwdcnotes\/wwdc22-10025-capture-machinereadable-codes-and-text-with-visionkit"],"traits":[{"interfaceLanguage":"swift"}]}],"hierarchy":{"paths":[["doc:\/\/WWDCNotes\/documentation\/WWDCNotes","doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22"]]},"kind":"article","abstract":[{"text":"Meet the Data Scanner in VisionKit: This framework combines AVCapture and Vision to enable live capture of machine-readable codes and text through a simple Swift API. We‚Äôll show you how to control the types of content your app can capture by specifying barcode symbologies and language selection. We‚Äôll also explore how you can enable guidance in your app, customize item highlighting or regions of interest, and handle interactions after your app detects an item.","type":"text"}],"references":{"WWDCNotes.png":{"type":"image","variants":[{"traits":["1x","light"],"url":"\/images\/WWDCNotes\/WWDCNotes.png"}],"identifier":"WWDCNotes.png","alt":null},"https://developer.apple.com/wwdc22/10025":{"type":"download","checksum":null,"identifier":"https:\/\/developer.apple.com\/wwdc22\/10025","url":"https:\/\/developer.apple.com\/wwdc22\/10025"},"doc://WWDCNotes/documentation/WWDCNotes":{"url":"\/documentation\/wwdcnotes","abstract":[{"type":"text","text":"Session notes shared by the community for the community."}],"role":"collection","kind":"symbol","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes","images":[{"identifier":"WWDCNotes.png","type":"icon"}],"title":"WWDC Notes","type":"topic"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10024-Whats-new-in-Vision":{"identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10024-Whats-new-in-Vision","kind":"article","abstract":[{"type":"text","text":"Learn about the latest updates to Vision APIs that help your apps recognize text, detect faces and face landmarks, and implement optical flow. We‚Äôll take you through the capabilities of optical flow for video-based apps, show you how to update your apps with revisions to the machine learning models that drive these APIs, and explore how you can visualize your Vision tasks with Quick Look Preview support in Xcode."}],"title":"What‚Äôs new in Vision","role":"sampleCode","type":"topic","url":"\/documentation\/wwdcnotes\/wwdc22-10024-whats-new-in-vision"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22":{"abstract":[{"type":"text","text":"Xcode 14, Swift 5.7, iOS 16, macOS 13 (Ventura), tvOS 16, watchOS 9."},{"type":"text","text":" "},{"type":"text","text":"New APIs: "},{"type":"codeVoice","code":"WeatherKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"ScreenCaptureKit"},{"type":"text","text":", "},{"type":"codeVoice","code":"Swift Regex"},{"type":"text","text":", and more."}],"type":"topic","images":[{"type":"icon","identifier":"WWDC22-Icon.png"},{"type":"card","identifier":"WWDC22.jpeg"}],"kind":"article","title":"WWDC22","url":"\/documentation\/wwdcnotes\/wwdc22","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22","role":"collectionGroup"},"doc://WWDCNotes/documentation/WWDCNotes/WWDC22-10026-Add-Live-Text-interaction-to-your-app":{"url":"\/documentation\/wwdcnotes\/wwdc22-10026-add-live-text-interaction-to-your-app","title":"Add Live Text interaction to your app","kind":"article","abstract":[{"text":"Learn how you can bring Live Text support for still photos or paused video frames to your app. We‚Äôll share how you can easily enable text interactions, translation, data detection, and QR code scanning within any image view on iOS, iPadOS, or macOS. We‚Äôll also go over how to control interaction types, manage the supplementary interface, and resolve potential gesture conflicts.","type":"text"}],"role":"sampleCode","type":"topic","identifier":"doc:\/\/WWDCNotes\/documentation\/WWDCNotes\/WWDC22-10026-Add-Live-Text-interaction-to-your-app"},"https://wwdcnotes.com/documentation/wwdcnotes/contributing":{"title":"Learn More‚Ä¶","url":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","identifier":"https:\/\/wwdcnotes.com\/documentation\/wwdcnotes\/contributing","titleInlineContent":[{"type":"text","text":"Learn More‚Ä¶"}],"type":"link"},"WWDC22-Icon.png":{"variants":[{"url":"\/images\/WWDCNotes\/WWDC22-Icon.png","traits":["1x","light"]}],"type":"image","identifier":"WWDC22-Icon.png","alt":null},"WWDC22.jpeg":{"type":"image","variants":[{"url":"\/images\/WWDCNotes\/WWDC22.jpeg","traits":["1x","light"]}],"identifier":"WWDC22.jpeg","alt":null}}}