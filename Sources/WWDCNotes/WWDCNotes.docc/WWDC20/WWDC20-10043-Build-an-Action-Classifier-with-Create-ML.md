# Build an Action Classifier with Create ML

Discover how to build Action Classification models in Create ML. With a custom action classifier, your app can recognize and understand body movements in real-time from videos or through a camera. We‚Äôll show you how to use samples to easily train a Core ML model to identify human actions like jumping jacks, squats, and dance moves. Learn how this is powered by the Body Pose estimation features of the Vision Framework. Get inspired to create apps that can provide coaching for fitness routines, deliver feedback on athletic form, and more.

To get the most out of this session, you should be familiar with Create ML. For an overview, watch ‚ÄúIntroducing the Create ML app.‚Äù You can also brush up on differences between Action Classification and sensor-based Activity Classification by watching ‚ÄúBuilding Activity Classification Models in Create ML.‚Äù

To learn more about the powerful technology that enables Action Classification features, be sure to check out ‚ÄúDetect Body and Hand Pose with Vision.‚Äù And you can see how we combined this classification capability together with other technologies to create our own sample application in ‚ÄúExplore the Action & Vision App.‚Äù

@Metadata {
   @TitleHeading("WWDC20")
   @PageKind(sampleCode)
   @CallToAction(url: "https://developer.apple.com/wwdc20/10043", purpose: link, label: "Watch Video (26 min)")

   @Contributors {
      @GitHubUser(<replace this with your GitHub handle>)
   }
}

üò± "No Overview Available!"

Be the hero to change that by watching the video and providing notes! It's super easy:
 [Learn More‚Ä¶](https://wwdcnotes.github.io/WWDCNotes/documentation/wwdcnotes/contributing)
